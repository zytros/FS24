{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the template for the submission. You can develop your algorithm in a regular Python script and copy the code here for submission.\n",
    "\n",
    "# TEAM NAME ON KAGGLE\n",
    "# \"EXAMPLE_GROUP\"\n",
    "\n",
    "# GROUP NUMBER\n",
    "# \"group_XX\"\n",
    "\n",
    "# TEAM MEMBERS (E-MAIL, LEGI, KAGGLE USERNAME):\n",
    "# \"examplestudent1@ethz.ch\", \"12-345-678\", \"eXampl3stdNtone\" \n",
    "# \"examplestudent2@ethz.ch\", \"12-345-679\", \"xXexamplestudent2Xx\"\n",
    "# \"examplestudent3@ethz.ch\", \"12-345-670\", \"mhealth_student_98\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# You may change the mhealth_activity module but your algorithm must support the original version\n",
    "from mhealth_activity import Recording\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# For interactive graphs\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the path for all test traces\n",
    "'''\n",
    "dir_traces_test = 'data/test'\n",
    "filenames_test = [join(dir_traces_test, f) for f in listdir(dir_traces_test) if isfile(join(dir_traces_test, f))]\n",
    "filenames_test.sort()\n",
    "recordings_test = []\n",
    "for fn in filenames_test:\n",
    "    rec = Recording(fn)\n",
    "    match = re.search(r'(\\d{3})\\.pkl$', fn)\n",
    "    if match:\n",
    "        id = int(match.group(1))\n",
    "        rec.id = id\n",
    "    else:\n",
    "        raise ValueError(f'Filename {fn} does not match expected format')\n",
    "    recordings_test.append(rec)\n",
    "'''\n",
    "    \n",
    "    \n",
    "dir_traces_train = 'data/train'\n",
    "filenames_train = [join(dir_traces_train, f) for f in listdir(dir_traces_train) if isfile(join(dir_traces_train, f))]\n",
    "filenames_train.sort()\n",
    "\n",
    "alts = []\n",
    "activities = []\n",
    "axs = []\n",
    "ays = []\n",
    "azs = []\n",
    "for fn in filenames_train:\n",
    "    rec = Recording(fn)\n",
    "    alts.append(rec.data['altitude'].values)\n",
    "    activities.append(rec.labels['activities'])\n",
    "    axs.append(rec.data['ax'].values)\n",
    "    ays.append(rec.data['ay'].values)\n",
    "    azs.append(rec.data['az'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import argrelextrema, find_peaks\n",
    "def centered_moving_average(data, window_size):\n",
    "    ret = np.cumsum(data, dtype=float)\n",
    "    ret[window_size:] = ret[window_size:] - ret[:-window_size]\n",
    "    return ret[window_size - 1:] / window_size\n",
    "def plot_array(arr):\n",
    "    plt.figure(figsize=(25, 3))\n",
    "    plt.plot(arr)\n",
    "    plt.show()\n",
    "\n",
    "def windowed_peak_detection(data, window_size):\n",
    "    # Initialize an empty list to store the peaks\n",
    "    peaks = []\n",
    "\n",
    "    # Divide the data into windows\n",
    "    for i in range(0, len(data), window_size):\n",
    "        window = data[i:i + window_size]\n",
    "\n",
    "        # Compute the relative maxima of the window\n",
    "        window_peaks = argrelextrema(window, np.greater)\n",
    "\n",
    "        # Add the indices of the peaks to the list\n",
    "        peaks.extend(window_peaks[0] + i)\n",
    "\n",
    "    return np.array(peaks)\n",
    "\n",
    "def get_steps_from_peaks(data, peaks, threshold=1.25):\n",
    "    steps=0\n",
    "    for peak in peaks:\n",
    "        if data[peak]>threshold:\n",
    "            steps+=1\n",
    "    return steps\n",
    "\n",
    "def get_steps(data, window_size=80, threshold=1.25):\n",
    "    peaks, _ = find_peaks(data, height=1.25, distance=80)\n",
    "    steps = get_steps_from_peaks(data, peaks, threshold)\n",
    "    return steps\n",
    "\n",
    "with open('indices.txt', 'r') as f:\n",
    "    indices = [line.rstrip() for line in f]\n",
    "    \n",
    "\n",
    "def pad_arrays(arr_list):\n",
    "    max_len = max(len(arr) for arr in arr_list)\n",
    "    return [np.pad(arr, (0, max_len - len(arr)), 'constant') for arr in arr_list]\n",
    "\n",
    "def pad_arrays_len(arr_list, pad_length):\n",
    "    return [np.pad(arr, (0, pad_length - len(arr)), 'constant') if len(arr) < pad_length else arr for arr in arr_list]\n",
    "\n",
    "def create_vector(lst):\n",
    "    return [1 if i in lst else 0 for i in range(4)]\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_dataloaders(X, y, batch_size=32, test_size=0.2):\n",
    "    # Convert X and y into PyTorch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=test_size)\n",
    "\n",
    "    # Create TensorDatasets for the training and testing sets\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # Create DataLoaders for the training and testing sets\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#162096\n",
    "padded_alts = pad_arrays_len(alts, 162096)\n",
    "padded_axs = pad_arrays_len(axs, 162096)\n",
    "padded_ays = pad_arrays_len(ays, 162096)\n",
    "padded_azs = pad_arrays_len(azs, 162096)\n",
    "\n",
    "X = []\n",
    "\n",
    "for i in range(len(padded_alts)):\n",
    "    X.append([padded_alts[i], padded_axs[i], padded_ays[i], padded_azs[i]])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = []\n",
    "for i in range(len(activities)):\n",
    "    y.append(create_vector(activities[i]))\n",
    "y = np.array(y)\n",
    "\n",
    "train_Dataloader, test_Dataloader = create_dataloaders(X, y, batch_size=32, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(4, 16, 200)\n",
    "        self.pool = nn.MaxPool1d(50)\n",
    "        self.conv2 = nn.Conv1d(16, 32, 50)\n",
    "        self.fc1 = nn.Linear(32 * 40444, 1024)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 40444)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all filenames to process recordings\n",
    "filenames_test = []\n",
    "submission = []\n",
    "for filename in filenames_test:\n",
    "    recording = Recording(filename)\n",
    "    \n",
    "    # Assumes filename format ends with a three-digit ID before \".pkl\"\n",
    "    match = re.search(r'(\\d{3})\\.pkl$', filename)\n",
    "    if match:\n",
    "        id = int(match.group(1))\n",
    "        recording.id = id\n",
    "    else:\n",
    "        raise ValueError(f'Filename {filename} does not match expected format')\n",
    "\n",
    "    # Placeholder for the algorithm to process the recording\n",
    "    # Implement the logic to infer watch location, path index, step count,\n",
    "    # and activities (standing, walking, running, cycling) here.\n",
    "    # Ensure your algorithm is tolerant to missing data and does not crash\n",
    "    # when optional smartphone data traces are missing.\n",
    "\n",
    "    path_idx = 0  # Integer, path in {0, 1, 2, 3, 4}\n",
    "    watch_loc = 0  # Integer, 0: left wrist, 1: belt, 2: right ankle\n",
    "    standing = False  # Boolean, True if participant was standing still throughout the recording\n",
    "    walking = False  # Boolean, True if participant was walking throughout the recording\n",
    "    running = False  # Boolean, True if participant was running throughout the recording\n",
    "    cycling = False  # Boolean, True if participant was cycling throughout the recording\n",
    "    step_count = 0  # Integer, number of steps, must be provided for each recording\n",
    "\n",
    "    predictions = {\n",
    "        'Id': id, \n",
    "        'watch_loc': watch_loc, \n",
    "        'path_idx': path_idx,\n",
    "        'standing': standing,\n",
    "        'walking': walking,\n",
    "        'running': running,\n",
    "        'cycling': cycling,\n",
    "        'step_count': step_count\n",
    "        }\n",
    "\n",
    "    submission.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the predicted values into a .csv file to then upload the .csv file to Kaggle\n",
    "# When cross-checking the .csv file on your computer, we recommend using a text editor and NOT excel so that the results are displayed correctly\n",
    "# IMPORTANT: Do NOT change the name of the columns of the .csv file (\"Id\", \"watch_loc\", \"path_idx\", \"standing\", \"walking\", \"running\", \"cycling\", \"step_count\")\n",
    "submission_df = pd.DataFrame(submission, columns=['Id', 'watch_loc', 'path_idx', 'standing', 'walking', 'running', 'cycling', 'step_count'])\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_idxs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(pad_arrays(alts))\n\u001b[1;32m----> 2\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mpath_idxs\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'path_idxs' is not defined"
     ]
    }
   ],
   "source": [
    "X = np.array(pad_arrays(alts))\n",
    "y = np.array(path_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 1 4 4 1 2 3 4 4 3 2 3 2 0 1 4 3 1 3 2 4 0 2 3 4 0 2 3 2 3 1 4 2 3 0 2\n",
      " 2 2 4 3 4 3 2 1 3 2 4 3 1 1 4 1 0 1 0 4 3 1 2 0 4 4 3 1 4 0 4 4 0 0 2 2 1\n",
      " 1 4 4 0 4 2]\n",
      "B_Accuracy: 66.13%\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "model = XGBClassifier(objective='multi:softmax', num_class=5)\n",
    "\n",
    "# Fit the model with the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "print(y_pred)\n",
    "print(\"B_Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhealth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
