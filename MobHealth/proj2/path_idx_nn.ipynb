{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the template for the submission. You can develop your algorithm in a regular Python script and copy the code here for submission.\n",
    "\n",
    "# TEAM NAME ON KAGGLE\n",
    "# \"EXAMPLE_GROUP\"\n",
    "\n",
    "# GROUP NUMBER\n",
    "# \"group_XX\"\n",
    "\n",
    "# TEAM MEMBERS (E-MAIL, LEGI, KAGGLE USERNAME):\n",
    "# \"examplestudent1@ethz.ch\", \"12-345-678\", \"eXampl3stdNtone\" \n",
    "# \"examplestudent2@ethz.ch\", \"12-345-679\", \"xXexamplestudent2Xx\"\n",
    "# \"examplestudent3@ethz.ch\", \"12-345-670\", \"mhealth_student_98\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# You may change the mhealth_activity module but your algorithm must support the original version\n",
    "from mhealth_activity import Recording\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\n",
    "\n",
    "# For interactive graphs\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the path for all test traces\n",
    "'''\n",
    "dir_traces_test = 'data/test'\n",
    "filenames_test = [join(dir_traces_test, f) for f in listdir(dir_traces_test) if isfile(join(dir_traces_test, f))]\n",
    "filenames_test.sort()\n",
    "recordings_test = []\n",
    "for fn in filenames_test:\n",
    "    rec = Recording(fn)\n",
    "    match = re.search(r'(\\d{3})\\.pkl$', fn)\n",
    "    if match:\n",
    "        id = int(match.group(1))\n",
    "        rec.id = id\n",
    "    else:\n",
    "        raise ValueError(f'Filename {fn} does not match expected format')\n",
    "    recordings_test.append(rec)\n",
    "'''\n",
    "    \n",
    "    \n",
    "dir_traces_train = 'data/train'\n",
    "filenames_train = [join(dir_traces_train, f) for f in listdir(dir_traces_train) if isfile(join(dir_traces_train, f))]\n",
    "filenames_train.sort()\n",
    "\n",
    "alts_train = []\n",
    "path_idxs = []\n",
    "activities = []\n",
    "mxs = []\n",
    "mys = []\n",
    "mzs = []\n",
    "axs = []\n",
    "ays = []\n",
    "azs = []\n",
    "for fn in filenames_train:\n",
    "    rec = Recording(fn)\n",
    "    alts_train.append(rec.data['altitude'].values)\n",
    "    path_idxs.append(rec.labels['path_idx'])\n",
    "    #activities.append(rec.labels['activities'])\n",
    "    mxs.append(rec.data['mx'].values)\n",
    "    mys.append(rec.data['my'].values)\n",
    "    mzs.append(rec.data['mz'].values)\n",
    "    axs.append(rec.data['ax'].values)\n",
    "    ays.append(rec.data['ay'].values)\n",
    "    azs.append(rec.data['az'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_traces_test = 'data/test'\n",
    "filenames_test = [join(dir_traces_test, f) for f in listdir(dir_traces_test) if isfile(join(dir_traces_test, f))]\n",
    "filenames_test.sort()\n",
    "\n",
    "alts_test = []\n",
    "\n",
    "for fn in filenames_test:\n",
    "    rec = Recording(fn)\n",
    "    alts_test.append(rec.data['altitude'].values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import argrelextrema, find_peaks\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def centered_moving_average(data, window_size):\n",
    "    ret = np.cumsum(data, dtype=float)\n",
    "    ret[window_size:] = ret[window_size:] - ret[:-window_size]\n",
    "    return ret[window_size - 1:] / window_size\n",
    "def plot_array(arr):\n",
    "    plt.figure(figsize=(25, 3))\n",
    "    plt.plot(arr)\n",
    "    plt.show()\n",
    "\n",
    "def windowed_peak_detection(data, window_size):\n",
    "    # Initialize an empty list to store the peaks\n",
    "    peaks = []\n",
    "\n",
    "    # Divide the data into windows\n",
    "    for i in range(0, len(data), window_size):\n",
    "        window = data[i:i + window_size]\n",
    "\n",
    "        # Compute the relative maxima of the window\n",
    "        window_peaks = argrelextrema(window, np.greater)\n",
    "\n",
    "        # Add the indices of the peaks to the list\n",
    "        peaks.extend(window_peaks[0] + i)\n",
    "\n",
    "    return np.array(peaks)\n",
    "\n",
    "def get_steps_from_peaks(data, peaks, threshold=1.25):\n",
    "    steps=0\n",
    "    for peak in peaks:\n",
    "        if data[peak]>threshold:\n",
    "            steps+=1\n",
    "    return steps\n",
    "\n",
    "def get_steps(data, window_size=80, threshold=1.25):\n",
    "    peaks, _ = find_peaks(data, height=1.25, distance=80)\n",
    "    steps = get_steps_from_peaks(data, peaks, threshold)\n",
    "    return steps\n",
    "\n",
    "    \n",
    "\n",
    "def pad_arrays(arr_list):\n",
    "    max_len = max(len(arr) for arr in arr_list)\n",
    "    return [np.pad(arr, (0, max_len - len(arr)), 'constant') for arr in arr_list]\n",
    "\n",
    "def pad_arrays_len(arr_list, pad_length=10032):\n",
    "    #10032\n",
    "    return [np.pad(arr, (0, pad_length - len(arr)), 'constant') if len(arr) < pad_length else arr[:pad_length] for arr in arr_list]\n",
    "\n",
    "def create_one_hot_vector(num):\n",
    "    return [1 if i == num else 0 for i in range(5)]\n",
    "\n",
    "def create_dataloaders(X, y, batch_size=16, test_size=0.2):\n",
    "    # Convert X and y into PyTorch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=test_size)\n",
    "\n",
    "    # Create TensorDatasets for the training and testing sets\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # Create DataLoaders for the training and testing sets\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def train_model(dataloader, model, epochs=5):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for i in range(epochs):\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Compute prediction error\n",
    "            pred = model(X)\n",
    "            #print('pred:', pred, 'y:', y)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss, current = loss.item(), batch * len(X)\n",
    "        #print(f\"loss: {loss:>7f} \")\n",
    "        \n",
    "def make_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    y_true = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            X = X.to(device)\n",
    "            output = model(X)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            y = torch.max(y, 1)[1]\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "    return predictions, y_true\n",
    "\n",
    "def plot_arrays(arrays):\n",
    "    for array in arrays:\n",
    "        plt.plot(array)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def clean_data(data:np.array):\n",
    "    min = np.min(data[:500])\n",
    "    max = np.max(data[:500])\n",
    "    diff = max-min\n",
    "    if diff > 30:\n",
    "        n_data = data[500:]\n",
    "    else:\n",
    "        n_data = data\n",
    "        \n",
    "    n_data = centered_moving_average(n_data, 100)\n",
    "    return n_data\n",
    "\n",
    "def downsample_time_series(time_series):\n",
    "    # Calculate the downsampling factor\n",
    "    downsample_factor = int(200 / 12.5)\n",
    "\n",
    "    # Reshape the time series into chunks of size downsample_factor\n",
    "    reshaped_time_series = time_series[:len(time_series) // downsample_factor * downsample_factor].reshape(-1, downsample_factor)\n",
    "\n",
    "    # Average each chunk to get the downsampled time series\n",
    "    downsampled_time_series = reshaped_time_series.mean(axis=1)\n",
    "\n",
    "    return downsampled_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.5189357023906"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec = Recording(filenames_train[0])\n",
    "rec.data['mx'].samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for d in mxs:\\n    data_mx.append(centered_moving_average(d, 30))\\ndata_mx = pad_arrays(data_mx)\\nfor d in mys:\\n    data_my.append(centered_moving_average(d, 30))\\ndata_my = pad_arrays(data_my)\\nfor d in mzs:\\n    data_mz.append(centered_moving_average(d, 30))\\ndata_mz = pad_arrays(data_mz)\\nfor d in axs:\\n    d = downsample_time_series(d)\\n    data_ax.append(centered_moving_average(d, 30))\\ndata_ax = pad_arrays(data_ax)\\nfor d in ays:\\n    d = downsample_time_series(d)\\n    data_ay.append(centered_moving_average(d, 30))\\ndata_ay = pad_arrays(data_ay)\\nfor d in azs:\\n    d = downsample_time_series(d)\\n    data_az.append(centered_moving_average(d, 30))\\ndata_az = pad_arrays(data_az)\\ndata_tot = np.concatenate((data_alt_train, data_mx, data_my, data_mz, data_ax, data_ay, data_az), axis=1)\\nprint(data_tot.shape)\\nplot_arrays([data_ax[0], data_ay[0], data_az[0]])'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_alt_train = []\n",
    "for d in alts_train:\n",
    "    data_alt_train.append(clean_data(d))\n",
    "data_alt_train = pad_arrays_len(data_alt_train)\n",
    "\n",
    "data_alt_test = []\n",
    "for d in alts_test:\n",
    "    data_alt_test.append(clean_data(d))\n",
    "data_alt_test = pad_arrays_len(data_alt_test)\n",
    "'''for d in mxs:\n",
    "    data_mx.append(centered_moving_average(d, 30))\n",
    "data_mx = pad_arrays(data_mx)\n",
    "for d in mys:\n",
    "    data_my.append(centered_moving_average(d, 30))\n",
    "data_my = pad_arrays(data_my)\n",
    "for d in mzs:\n",
    "    data_mz.append(centered_moving_average(d, 30))\n",
    "data_mz = pad_arrays(data_mz)\n",
    "for d in axs:\n",
    "    d = downsample_time_series(d)\n",
    "    data_ax.append(centered_moving_average(d, 30))\n",
    "data_ax = pad_arrays(data_ax)\n",
    "for d in ays:\n",
    "    d = downsample_time_series(d)\n",
    "    data_ay.append(centered_moving_average(d, 30))\n",
    "data_ay = pad_arrays(data_ay)\n",
    "for d in azs:\n",
    "    d = downsample_time_series(d)\n",
    "    data_az.append(centered_moving_average(d, 30))\n",
    "data_az = pad_arrays(data_az)\n",
    "data_tot = np.concatenate((data_alt_train, data_mx, data_my, data_mz, data_ax, data_ay, data_az), axis=1)\n",
    "print(data_tot.shape)\n",
    "plot_arrays([data_ax[0], data_ay[0], data_az[0]])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data = np.array(pad_arrays(data_alt_train))\n",
    "y_train_data = np.array(path_idxs)\n",
    "X_test_data = np.array(pad_arrays(data_alt_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B Accuracy: 0.6822, n_est: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_data, y_train_data, test_size=0.2)\n",
    "\n",
    "for dep in [5]:\n",
    "    model = XGBClassifier(objective='multi:softmax', num_class=5, subsample=1, reg_lambda=1, n_estimators=100, max_depth=6)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = balanced_accuracy_score(y_test, y_pred, adjusted=True)\n",
    "    #print(y_pred)\n",
    "    print(f\"B Accuracy: {accuracy:.4f}, n_est: {dep}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "balanced accuracy with altitude(preprocessed): 66.93% (no hyperparameter tuning)\n",
    "\n",
    "balanced accuracy with altitute and magnetometer: 59.37% (no hyperparameter tuning)\n",
    "\n",
    "balanced accuracy with altitude, magnetometer and acceleration: 53.37%(no hyperparameter tuning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  4,  4,  0,  0],\n",
       "       [ 5,  9,  3,  0,  0],\n",
       "       [ 0,  0,  7,  0,  0],\n",
       "       [ 0,  0,  0, 18,  3],\n",
       "       [ 0,  0,  0,  3, 16]], dtype=int64)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 40, 53, 57, 66)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_solution = model.predict(X_test_data)\n",
    "np.count_nonzero(y_solution == 0), np.count_nonzero(y_solution == 1), np.count_nonzero(y_solution == 2), np.count_nonzero(y_solution == 3), np.count_nonzero(y_solution == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the predicted values into a .csv file to then upload the .csv file to Kaggle\n",
    "# When cross-checking the .csv file on your computer, we recommend using a text editor and NOT excel so that the results are displayed correctly\n",
    "# IMPORTANT: Do NOT change the name of the columns of the .csv file (\"Id\", \"watch_loc\", \"path_idx\", \"standing\", \"walking\", \"running\", \"cycling\", \"step_count\")\n",
    "submission = []\n",
    "for i in range(len(y_solution)):\n",
    "    predictions = {\n",
    "        'Id': i, \n",
    "        'watch_loc': 0, \n",
    "        'path_idx': y_solution[i],\n",
    "        'standing': False,\n",
    "        'walking': False,\n",
    "        'running': False,\n",
    "        'cycling': False,\n",
    "        'step_count': 0\n",
    "    }\n",
    "    submission.append(predictions)\n",
    "\n",
    "submission_df = pd.DataFrame(submission, columns=['Id', 'watch_loc', 'path_idx', 'standing', 'walking', 'running', 'cycling', 'step_count'])\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhealth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
