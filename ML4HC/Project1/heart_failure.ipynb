{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>140</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>130</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>ST</td>\n",
       "      <td>98</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>F</td>\n",
       "      <td>ASY</td>\n",
       "      <td>138</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>108</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>M</td>\n",
       "      <td>NAP</td>\n",
       "      <td>150</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>122</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39</td>\n",
       "      <td>M</td>\n",
       "      <td>NAP</td>\n",
       "      <td>120</td>\n",
       "      <td>339</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>170</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>41</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>120</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>182</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>57</td>\n",
       "      <td>F</td>\n",
       "      <td>ASY</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>123</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>45</td>\n",
       "      <td>M</td>\n",
       "      <td>TA</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>132</td>\n",
       "      <td>N</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>68</td>\n",
       "      <td>M</td>\n",
       "      <td>ASY</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>141</td>\n",
       "      <td>N</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>57</td>\n",
       "      <td>M</td>\n",
       "      <td>ASY</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>115</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>734 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  \\\n",
       "0     40   M           ATA        140          289          0     Normal   \n",
       "1     37   M           ATA        130          283          0         ST   \n",
       "2     48   F           ASY        138          214          0     Normal   \n",
       "3     54   M           NAP        150          195          0     Normal   \n",
       "4     39   M           NAP        120          339          0     Normal   \n",
       "..   ...  ..           ...        ...          ...        ...        ...   \n",
       "729   41   M           ATA        120          157          0     Normal   \n",
       "730   57   F           ASY        140          241          0     Normal   \n",
       "731   45   M            TA        110          264          0     Normal   \n",
       "732   68   M           ASY        144          193          1     Normal   \n",
       "733   57   M           ASY        130          131          0     Normal   \n",
       "\n",
       "     MaxHR ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
       "0      172              N      0.0       Up             0  \n",
       "1       98              N      0.0       Up             0  \n",
       "2      108              Y      1.5     Flat             1  \n",
       "3      122              N      0.0       Up             0  \n",
       "4      170              N      0.0       Up             0  \n",
       "..     ...            ...      ...      ...           ...  \n",
       "729    182              N      0.0       Up             0  \n",
       "730    123              Y      0.2     Flat             1  \n",
       "731    132              N      1.2     Flat             1  \n",
       "732    141              N      3.4     Flat             1  \n",
       "733    115              Y      1.2     Flat             1  \n",
       "\n",
       "[734 rows x 12 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('heart_failure/train_val_split.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_binning(series, bins=10, name='data'):\n",
    "    plt.hist(series, bins=bins, edgecolor='black')\n",
    "    plt.title('Histogram of ' + name)\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "def create_bar_plot(names, values):\n",
    "    plt.bar(names, values)\n",
    "    plt.xlabel('Names')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title('Bar Plot')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    \n",
    "def standardize_data(data):\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "    \n",
    "categorical = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope', 'HeartDisease']\n",
    "nummeric = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders_from_arrays(X_train, y_train, X_test, y_test, batch_size=64):\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Create TensorDatasets\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def train_model(model, train_loader, test_loader, device='cpu', epochs=10):\n",
    "    # Move model to the device\n",
    "    \n",
    "    if not (device == 'cpu'):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(lr=0.01, params=model.parameters())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            # Move data and target to the device\n",
    "            target = target.view(-1, 1)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "\n",
    "        # Print training statistics \n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        \n",
    "def make_predictions(model, array):\n",
    "    # Convert the array to a PyTorch tensor\n",
    "    tensor = torch.from_numpy(array).float()\n",
    "\n",
    "    # Use the model to make predictions\n",
    "    output = model(tensor)\n",
    "\n",
    "    # Convert the output to probabilities using the sigmoid function\n",
    "    probabilities = torch.sigmoid(output).detach().numpy()\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualization and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex'] = pd.factorize(df['Sex'])[0]\n",
    "df['ChestPainType'] = pd.factorize(df['ChestPainType'])[0]\n",
    "df['RestingECG'] = pd.factorize(df['RestingECG'])[0]\n",
    "df['ExerciseAngina'] = pd.factorize(df['ExerciseAngina'])[0]\n",
    "df['ST_Slope'] = pd.factorize(df['ST_Slope'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>138</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>339</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>734 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  Sex  ChestPainType  RestingBP  Cholesterol  FastingBS  RestingECG  \\\n",
       "0     40    0              0        140          289          0           0   \n",
       "1     37    0              0        130          283          0           1   \n",
       "2     48    1              1        138          214          0           0   \n",
       "3     54    0              2        150          195          0           0   \n",
       "4     39    0              2        120          339          0           0   \n",
       "..   ...  ...            ...        ...          ...        ...         ...   \n",
       "729   41    0              0        120          157          0           0   \n",
       "730   57    1              1        140          241          0           0   \n",
       "731   45    0              3        110          264          0           0   \n",
       "732   68    0              1        144          193          1           0   \n",
       "733   57    0              1        130          131          0           0   \n",
       "\n",
       "     MaxHR  ExerciseAngina  Oldpeak  ST_Slope  HeartDisease  \n",
       "0      172               0      0.0         0             0  \n",
       "1       98               0      0.0         0             0  \n",
       "2      108               1      1.5         1             1  \n",
       "3      122               0      0.0         0             0  \n",
       "4      170               0      0.0         0             0  \n",
       "..     ...             ...      ...       ...           ...  \n",
       "729    182               0      0.0         0             0  \n",
       "730    123               1      0.2         1             1  \n",
       "731    132               0      1.2         1             1  \n",
       "732    141               0      3.4         1             1  \n",
       "733    115               1      1.2         1             1  \n",
       "\n",
       "[734 rows x 12 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHFCAYAAAAUpjivAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABAuklEQVR4nO3de1xVVf7/8fdR4AiIqJggikiJ90te0tImMVMzsdIaLfOWzi/LNE3tQk0jlknpaPbVUasptfyaTpM6jjOZmPdRSyUzL6NWeJfQMvAKCOv3hz/Oz8NNOB48h83r+Xicx6Oz9tr7fM4S9N3aa+9tM8YYAQAAWFQFTxcAAABQmgg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7AADA0gg7gJvMnz9fNptNO3bsKHB7bGys6tWr59RWr149DRkypESfs2XLFsXHx+u3335zrdByaMmSJWratKn8/f1ls9m0a9euIvv/9NNPGjlypBo0aCB/f38FBASoadOm+uMf/6gTJ044+sXExKhZs2ZurdVmsyk+Pt6tx8y1b98+xcfH6/Dhw6Vy/OKKiYlRTEyMR2tA+eLj6QKA8mzZsmWqUqVKifbZsmWLJk6cqCFDhqhq1aqlU5iFnD59WgMHDtT999+v2bNny263q0GDBoX2X7lypR577DHVqFFDI0eOVKtWrWSz2fT999/ro48+0r/+9S99++23N/EbuM++ffs0ceJExcTE5AvegJURdgAPatWqladLKLGsrCzZbDb5+JSNvz4OHjyorKwsDRgwQJ06dSqyb3Jysh577DE1aNBA69atU3BwsGPbvffeq+eee07Lli0r7ZLLnIsXLyogIMDTZQCF4jQW4EF5T2Pl5ORo0qRJatiwofz9/VW1alW1aNFC7777riQpPj5eL7zwgiQpKipKNptNNptN69evd+w/ZcoUNWrUSHa7XTVr1tSgQYN0/Phxp881xmjy5MmKjIxUpUqV1LZtWyUmJuY7vbB+/XrZbDZ98sknGjdunGrXri273a4ffvhBp0+f1ogRI9SkSRNVrlxZNWvW1L333qtNmzY5fdbhw4dls9k0depUvf3226pXr578/f0VExPjCCIvv/yywsPDFRwcrN69eys1NbVY47dixQrdddddCggIUFBQkLp27aqtW7c6tg8ZMkR33323JKlfv36y2WxFnj6ZPn26Lly4oNmzZzsFnVw2m019+vTJ1759+3b97ne/U0BAgG699Va99dZbysnJcepz9OhRDRgwQDVr1pTdblfjxo01bdq0fP0KkpKSouHDh6tOnTry8/NTVFSUJk6cqCtXrjj1mzNnjlq2bKnKlSsrKChIjRo10iuvvCLp6mnW3//+95Kkzp07O3525s+f79h/zZo16tKli6pUqaKAgAB17NhRX331ldNnxMfHy2azKSkpSY8++qiqVaum2267TZJ0+fJlxcXFKSoqSn5+fqpdu7aeffZZTrnC8wwAt5g3b56RZLZt22aysrLyvR544AETGRnptE9kZKQZPHiw431CQoKpWLGimTBhgvnqq6/MqlWrzIwZM0x8fLwxxphjx46ZUaNGGUlm6dKlZuvWrWbr1q0mLS3NGGPMU089ZSSZkSNHmlWrVpm5c+eaW265xURERJjTp087PicuLs5IMk899ZRZtWqV+eCDD0zdunVNrVq1TKdOnRz91q1bZySZ2rVrm0cffdSsWLHCrFy50vzyyy/mv//9r3nmmWfM4sWLzfr1683KlSvNsGHDTIUKFcy6descx0hOTjaSTGRkpOnVq5dZuXKlWbhwoQkNDTUNGjQwAwcONEOHDjVffPGFmTt3rqlcubLp1avXdcf7f//3f40k061bN7N8+XKzZMkS06ZNG+Pn52c2bdpkjDHmhx9+MH/5y1+MJDN58mSzdetWs3fv3kKP2aBBAxMaGnrdz87VqVMnExISYqKjo83cuXNNYmKiGTFihJFkFixY4OiXmppqateubW655RYzd+5cs2rVKjNy5EgjyTzzzDNOx5RkJkyY4Hh/6tQpExERYSIjI817771n1qxZY9544w1jt9vNkCFDHP0+/fRTI8mMGjXKrF692qxZs8bMnTvXPPfcc44aJk+ebCSZv/zlL46fndTUVGOMMZ988omx2Wzm4YcfNkuXLjX//Oc/TWxsrKlYsaJZs2aN43MmTJjg+PN86aWXTGJiolm+fLnJyckx3bt3Nz4+Pua1114zq1evNn/+859NYGCgadWqlbl8+bLTuF37cwaUNsIO4Ca5Yaeo1/XCTmxsrLn99tuL/JypU6caSSY5Odmpff/+/UaSGTFihFP7119/bSSZV155xRhjzK+//mrsdrvp16+fU7+tW7caSQWGnXvuuee63//KlSsmKyvLdOnSxfTu3dvRnht2WrZsabKzsx3tM2bMMJLMgw8+6HScMWPGGEmOAFeQ7OxsEx4ebpo3b+50zHPnzpmaNWuaDh065PsOn3322XW/Q6VKlcydd9553X65OnXqZCSZr7/+2qm9SZMmpnv37o73L7/8coH9nnnmGWOz2cyBAwccbXnDzvDhw03lypXNkSNHnPb985//bCQ5wtvIkSNN1apVi6z3s88+M5Kcwqgxxly4cMFUr149X8jMzs42LVu2NO3atXO05YadP/3pT059V61aZSSZKVOmOLUvWbLESDLvv/++o42wg5uN01iAm3388cfavn17vlfu6ZSitGvXTt99951GjBihL7/8Uunp6cX+3HXr1klSvqu72rVrp8aNGztOR2zbtk0ZGRnq27evU78777yz0EWrjzzySIHtc+fOVevWrVWpUiX5+PjI19dXX331lfbv35+v7wMPPKAKFf7/XzmNGzeWJPXs2dOpX2770aNHC/mm0oEDB3Ty5EkNHDjQ6ZiVK1fWI488om3btunixYuF7u9OYWFhateunVNbixYtdOTIEcf7tWvXqkmTJvn6DRkyRMYYrV27ttDjr1y5Up07d1Z4eLiuXLniePXo0UOStGHDBklX/5x/++03Pf744/rHP/6hM2fOFPs7bNmyRb/++qsGDx7s9Bk5OTm6//77tX37dl24cMFpn7w/E7nfIe/P3+9//3sFBgbmOx0G3ExlY4UhUIY0btxYbdu2zdceHBysY8eOFblvXFycAgMDtXDhQs2dO1cVK1bUPffco7fffrvAY17rl19+kSTVqlUr37bw8HDHP765/UJDQ/P1K6itsGNOnz5d48aN09NPP6033nhDNWrUUMWKFfXaa68VGHaqV6/u9N7Pz6/I9suXLxdYy7XfobDvmpOTo7Nnz5Z40WzdunWVnJxcon1CQkLytdntdl26dMmp3oKCZHh4uGN7YX7++Wf985//lK+vb4Hbc0PNwIEDdeXKFX3wwQd65JFHlJOTozvuuEOTJk1S165di/wOP//8syTp0UcfLbTPr7/+qsDAQMf7vGP/yy+/yMfHR7fccotTu81mU1hYWJHfEShthB3Ai/j4+Gjs2LEaO3asfvvtN61Zs0avvPKKunfvrmPHjhX5j3fuP7qnTp1SnTp1nLadPHlSNWrUcOqX+w/ctVJSUgr8R9lms+VrW7hwoWJiYjRnzhyn9nPnzhX9Jd3g2u+a18mTJ1WhQgVVq1atxMft3r27Zs6cqW3btunOO++84TpzhYSEFFqrJMefTUFq1KihFi1a6M033yxwe25gkqQnn3xSTz75pC5cuKCNGzdqwoQJio2N1cGDBxUZGVnkZ0jSzJkzC/3eeYNw3p+JkJAQXblyRadPn3YKPMYYpaSk6I477ij084HSxmkswEtVrVpVjz76qJ599ln9+uuvjhvB2e12SXKaOZCuXhotXQ0h19q+fbv279+vLl26SJLat28vu92uJUuWOPXbtm2b06mX67HZbI5acu3evdvpaqjS0rBhQ9WuXVuLFi2SMcbRfuHCBX3++eeOK7RK6vnnn1dgYKBGjBihtLS0fNuNMS5det6lSxft27dPSUlJTu0ff/yxbDabOnfuXOi+sbGx2rNnj2677Ta1bds23+vasJMrMDBQPXr00KuvvqrMzEzt3btXUuE/Ox07dlTVqlW1b9++Aj+jbdu2jhm3or6jlP/n7/PPP9eFCxcc2wFPYGYH8CK9evVSs2bN1LZtW91yyy06cuSIZsyYocjISEVHR0uSmjdvLkl69913NXjwYPn6+qphw4Zq2LChnnrqKc2cOVMVKlRQjx49dPjwYb322muKiIjQ888/L+nqaaOxY8cqISFB1apVU+/evXX8+HFNnDhRtWrVcloDU5TY2Fi98cYbmjBhgjp16qQDBw7o9ddfV1RUVL5Lot2tQoUKmjJlip544gnFxsZq+PDhysjI0NSpU/Xbb7/prbfecum4UVFRWrx4sfr166fbb7/dcVNB6eoN+T766CMZY9S7d+8SHff555/Xxx9/rJ49e+r1119XZGSk/vWvf2n27Nl65plnirzJ4euvv67ExER16NBBzz33nBo2bKjLly/r8OHD+ve//625c+eqTp06+j//5//I399fHTt2VK1atZSSkqKEhAQFBwc7ZlVy7/b8/vvvKygoSJUqVVJUVJRCQkI0c+ZMDR48WL/++qseffRR1axZU6dPn9Z3332n06dP55vBy6tr167q3r27XnrpJaWnp6tjx47avXu3JkyYoFatWmngwIElGjPArTy6PBqwkNyrsbZv317g9p49e173aqxp06aZDh06mBo1ahg/Pz9Tt25dM2zYMHP48GGn/eLi4kx4eLipUKGC09U12dnZ5u233zYNGjQwvr6+pkaNGmbAgAHm2LFjTvvn5OSYSZMmmTp16hg/Pz/TokULs3LlStOyZUunK6mKupIpIyPDjB8/3tSuXdtUqlTJtG7d2ixfvtwMHjzY6XvmXo01depUp/0LO/b1xvFay5cvN+3btzeVKlUygYGBpkuXLuY///lPsT6nKD/++KMZMWKEqV+/vrHb7cbf3980adLEjB071ukquE6dOpmmTZvm2z/vGBhjzJEjR0z//v1NSEiI8fX1NQ0bNjRTp051uprMmPxXYxljzOnTp81zzz1noqKijK+vr6levbpp06aNefXVV8358+eNMcYsWLDAdO7c2YSGhho/Pz8THh5u+vbta3bv3u10rBkzZpioqChTsWJFI8nMmzfPsW3Dhg2mZ8+epnr16sbX19fUrl3b9OzZ02nscq/GuvZWBrkuXbpkXnrpJRMZGWl8fX1NrVq1zDPPPGPOnj3r1I+rsXCz2Yy5Zg4YQLmVnJysRo0aacKECY4b0QGAFRB2gHLou+++06effqoOHTqoSpUqOnDggKZMmaL09HTt2bOn0KuyAKAsYs0OUA4FBgZqx44d+vDDD/Xbb78pODhYMTExevPNNwk6ACyHmR0AAGBpXHoOAAAsjbADAAAsjbADAAAsjQXKknJycnTy5EkFBQUVeFt8AADgfYwxOnfunMLDw4u8ISphR1efTxMREeHpMgAAgAuOHTuW75mA1yLsSAoKCpJ0dbCqVKni4WoAAEBxpKenKyIiwvHveGEIO/r/T++tUqUKYQcAgDLmektQWKAMAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAszaNhZ+PGjerVq5fCw8Nls9m0fPnyQvsOHz5cNptNM2bMcGrPyMjQqFGjVKNGDQUGBurBBx/U8ePHS7dwAABQZng07Fy4cEEtW7bUrFmziuy3fPlyff311woPD8+3bcyYMVq2bJkWL16szZs36/z584qNjVV2dnZplV0iR48eVVJSktPr6NGjni4LAIByw8eTH96jRw/16NGjyD4nTpzQyJEj9eWXX6pnz55O29LS0vThhx/qk08+0X333SdJWrhwoSIiIrRmzRp179691GovjqNHj6pho8a6fOmiU3sl/wAd+O9+1a1b10OVAQBQfng07FxPTk6OBg4cqBdeeEFNmzbNt33nzp3KyspSt27dHG3h4eFq1qyZtmzZUmjYycjIUEZGhuN9enq6+4uXdObMGV2+dFEhsePkGxIhScr65Zh+WTlNZ86cIewAAHATePUC5bfffls+Pj567rnnCtyekpIiPz8/VatWzak9NDRUKSkphR43ISFBwcHBjldERIRb687LNyRC9rD6sofVd4QeAABwc3ht2Nm5c6feffddzZ8/XzabrUT7GmOK3CcuLk5paWmO17Fjx260XAAA4KW8Nuxs2rRJqampqlu3rnx8fOTj46MjR45o3LhxqlevniQpLCxMmZmZOnv2rNO+qampCg0NLfTYdrtdVapUcXoBAABr8tqwM3DgQO3evVu7du1yvMLDw/XCCy/oyy+/lCS1adNGvr6+SkxMdOx36tQp7dmzRx06dPBU6QAAwIt4dIHy+fPn9cMPPzjeJycna9euXapevbrq1q2rkJAQp/6+vr4KCwtTw4YNJUnBwcEaNmyYxo0bp5CQEFWvXl3jx49X8+bNHVdnAQCA8s2jYWfHjh3q3Lmz4/3YsWMlSYMHD9b8+fOLdYx33nlHPj4+6tu3ry5duqQuXbpo/vz5qlixYmmUDAAAyhiPhp2YmBgZY4rd//Dhw/naKlWqpJkzZ2rmzJlurAwAAFiF167ZAQAAcAfCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDSPhp2NGzeqV69eCg8Pl81m0/Llyx3bsrKy9NJLL6l58+YKDAxUeHi4Bg0apJMnTzodIyMjQ6NGjVKNGjUUGBioBx98UMePH7/J3wQAAHgrj4adCxcuqGXLlpo1a1a+bRcvXlRSUpJee+01JSUlaenSpTp48KAefPBBp35jxozRsmXLtHjxYm3evFnnz59XbGyssrOzb9bXAAAAXszHkx/eo0cP9ejRo8BtwcHBSkxMdGqbOXOm2rVrp6NHj6pu3bpKS0vThx9+qE8++UT33XefJGnhwoWKiIjQmjVr1L1791L/DgAAwLuVqTU7aWlpstlsqlq1qiRp586dysrKUrdu3Rx9wsPD1axZM23ZsqXQ42RkZCg9Pd3pBQAArKnMhJ3Lly/r5ZdfVv/+/VWlShVJUkpKivz8/FStWjWnvqGhoUpJSSn0WAkJCQoODna8IiIiSrV2AADgOWUi7GRlZemxxx5TTk6OZs+efd3+xhjZbLZCt8fFxSktLc3xOnbsmDvLBQAAXsTrw05WVpb69u2r5ORkJSYmOmZ1JCksLEyZmZk6e/as0z6pqakKDQ0t9Jh2u11VqlRxegEAAGvy6rCTG3QOHTqkNWvWKCQkxGl7mzZt5Ovr67SQ+dSpU9qzZ486dOhws8sFAABeyKNXY50/f14//PCD431ycrJ27dql6tWrKzw8XI8++qiSkpK0cuVKZWdnO9bhVK9eXX5+fgoODtawYcM0btw4hYSEqHr16ho/fryaN2/uuDoLAACUbx4NOzt27FDnzp0d78eOHStJGjx4sOLj47VixQpJ0u233+6037p16xQTEyNJeuedd+Tj46O+ffvq0qVL6tKli+bPn6+KFSvelO8AAAC8m0fDTkxMjIwxhW4valuuSpUqaebMmZo5c6Y7SwMAABbh1Wt2AAAAbhRhBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWJpHw87GjRvVq1cvhYeHy2azafny5U7bjTGKj49XeHi4/P39FRMTo7179zr1ycjI0KhRo1SjRg0FBgbqwQcf1PHjx2/itwAAAN7Mo2HnwoULatmypWbNmlXg9ilTpmj69OmaNWuWtm/frrCwMHXt2lXnzp1z9BkzZoyWLVumxYsXa/PmzTp//rxiY2OVnZ19s74GAADwYj6e/PAePXqoR48eBW4zxmjGjBl69dVX1adPH0nSggULFBoaqkWLFmn48OFKS0vThx9+qE8++UT33XefJGnhwoWKiIjQmjVr1L1795v2XQAAgHfy2jU7ycnJSklJUbdu3RxtdrtdnTp10pYtWyRJO3fuVFZWllOf8PBwNWvWzNGnIBkZGUpPT3d6AQAAa/LasJOSkiJJCg0NdWoPDQ11bEtJSZGfn5+qVatWaJ+CJCQkKDg42PGKiIhwc/UAAMBbeG3YyWWz2ZzeG2PyteV1vT5xcXFKS0tzvI4dO+aWWgEAgPfx2rATFhYmSflmaFJTUx2zPWFhYcrMzNTZs2cL7VMQu92uKlWqOL0AAIA1eW3YiYqKUlhYmBITEx1tmZmZ2rBhgzp06CBJatOmjXx9fZ36nDp1Snv27HH0AQAA5ZtHr8Y6f/68fvjhB8f75ORk7dq1S9WrV1fdunU1ZswYTZ48WdHR0YqOjtbkyZMVEBCg/v37S5KCg4M1bNgwjRs3TiEhIapevbrGjx+v5s2bO67OAgAA5ZtHw86OHTvUuXNnx/uxY8dKkgYPHqz58+frxRdf1KVLlzRixAidPXtW7du31+rVqxUUFOTY55133pGPj4/69u2rS5cuqUuXLpo/f74qVqx4078PAADwPjZjjPF0EZ6Wnp6u4OBgpaWluXX9TlJSktq0aaOwwTNkD6svScpI+UEpC8Zo586dat26tds+CwCA8qa4/3577ZodAAAAdyDsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAASyPsAAAAS3Mp7CQnJ7u7DgAAgFLhUtipX7++OnfurIULF+ry5cvurgkAAMBtXAo73333nVq1aqVx48YpLCxMw4cP1zfffOPu2gAAAG6YS2GnWbNmmj59uk6cOKF58+YpJSVFd999t5o2barp06fr9OnT7q4TAADAJTe0QNnHx0e9e/fW3/72N7399tv68ccfNX78eNWpU0eDBg3SqVOn3FUnAACAS24o7OzYsUMjRoxQrVq1NH36dI0fP14//vij1q5dqxMnTuihhx5yV50AAAAu8XFlp+nTp2vevHk6cOCAHnjgAX388cd64IEHVKHC1ewUFRWl9957T40aNXJrsQAAACXlUtiZM2eOhg4dqieffFJhYWEF9qlbt64+/PDDGyoOAADgRrkUdg4dOnTdPn5+fho8eLArhwcAAHAbl9bszJs3T5999lm+9s8++0wLFiy44aIAAADcxaWw89Zbb6lGjRr52mvWrKnJkyffcFEAAADu4lLYOXLkiKKiovK1R0ZG6ujRozdcFAAAgLu4FHZq1qyp3bt352v/7rvvFBIScsNFAQAAuItLYeexxx7Tc889p3Xr1ik7O1vZ2dlau3atRo8erccee8zdNQIAALjMpbAzadIktW/fXl26dJG/v7/8/f3VrVs33XvvvW5ds3PlyhX98Y9/VFRUlPz9/XXrrbfq9ddfV05OjqOPMUbx8fEKDw+Xv7+/YmJitHfvXrfVAAAAyjaXLj338/PTkiVL9MYbb+i7776Tv7+/mjdvrsjISLcW9/bbb2vu3LlasGCBmjZtqh07dujJJ59UcHCwRo8eLUmaMmWKpk+frvnz56tBgwaaNGmSunbtqgMHDigoKMit9QAAgLLHpbCTq0GDBmrQoIG7asln69ateuihh9SzZ09JUr169fTpp59qx44dkq7O6syYMUOvvvqq+vTpI0lasGCBQkNDtWjRIg0fPrzUagMAAGWDS2EnOztb8+fP11dffaXU1FSn00qStHbtWrcUd/fdd2vu3Lk6ePCgGjRooO+++06bN2/WjBkzJEnJyclKSUlRt27dHPvY7XZ16tRJW7ZsKTTsZGRkKCMjw/E+PT3dLfUCAADv41LYGT16tObPn6+ePXuqWbNmstls7q5LkvTSSy8pLS1NjRo1UsWKFZWdna0333xTjz/+uCQpJSVFkhQaGuq0X2hoqI4cOVLocRMSEjRx4sRSqRkAAHgXl8LO4sWL9be//U0PPPCAu+txsmTJEi1cuFCLFi1S06ZNtWvXLo0ZM0bh4eFOj6LIG7aMMUUGsLi4OI0dO9bxPj09XREREe7/AgAAwONcXqBcv359d9eSzwsvvKCXX37ZcTl78+bNdeTIESUkJGjw4MGOh5CmpKSoVq1ajv1SU1PzzfZcy263y263l27xAADAK7h06fm4ceP07rvvyhjj7nqcXLx4URUqOJdYsWJFxxqhqKgohYWFKTEx0bE9MzNTGzZsUIcOHUq1NgAAUDa4NLOzefNmrVu3Tl988YWaNm0qX19fp+1Lly51S3G9evXSm2++qbp166pp06b69ttvNX36dA0dOlTS1dNXY8aM0eTJkxUdHa3o6GhNnjxZAQEB6t+/v1tqAAAAZZtLYadq1arq3bu3u2vJZ+bMmXrttdc0YsQIpaamKjw8XMOHD9ef/vQnR58XX3xRly5d0ogRI3T27Fm1b99eq1ev5h47AABAkmQzpX0uqgxIT09XcHCw0tLSVKVKFbcdNykpSW3atFHY4Bmyh11d45SR8oNSFozRzp071bp1a7d9FgAA5U1x//12ac2OdPVRDmvWrNF7772nc+fOSZJOnjyp8+fPu3pIAAAAt3PpNNaRI0d0//336+jRo8rIyFDXrl0VFBSkKVOm6PLly5o7d6676wQAAHCJSzM7o0ePVtu2bXX27Fn5+/s72nv37q2vvvrKbcUBAADcKJevxvrPf/4jPz8/p/bIyEidOHHCLYUBAAC4g0szOzk5OcrOzs7Xfvz4ca6CAgAAXsWlsNO1a1fHwzilq/e7OX/+vCZMmFDqj5AAAAAoCZdOY73zzjvq3LmzmjRposuXL6t///46dOiQatSooU8//dTdNQIAALjMpbATHh6uXbt26dNPP1VSUpJycnI0bNgwPfHEE04LlgEAADzNpbAjSf7+/ho6dKjj0Q0AAADeyKWw8/HHHxe5fdCgQS4VAwAA4G4uhZ3Ro0c7vc/KytLFixfl5+engIAAwg4AAPAaLl2NdfbsWafX+fPndeDAAd19990sUAYAAF7F5Wdj5RUdHa233nor36wPAACAJ7kt7EhSxYoVdfLkSXceEgAA4Ia4tGZnxYoVTu+NMTp16pRmzZqljh07uqUwAAAAd3Ap7Dz88MNO7202m2655Rbde++9mjZtmjvqAgAAcAuXwk5OTo676wAAACgVbl2zAwAA4G1cmtkZO3ZssftOnz7dlY8AAABwC5fCzrfffqukpCRduXJFDRs2lCQdPHhQFStWVOvWrR39bDabe6oEAABwkUthp1evXgoKCtKCBQtUrVo1SVdvNPjkk0/qd7/7ncaNG+fWIgEAAFzl0pqdadOmKSEhwRF0JKlatWqaNGkSV2MBAACv4lLYSU9P188//5yvPTU1VefOnbvhogAAANzFpbDTu3dvPfnkk/r73/+u48eP6/jx4/r73/+uYcOGqU+fPu6uEQAAwGUurdmZO3euxo8frwEDBigrK+vqgXx8NGzYME2dOtWtBQIAANwIl8JOQECAZs+eralTp+rHH3+UMUb169dXYGCgu+sDAAC4ITd0U8FTp07p1KlTatCggQIDA2WMcVddAAAAbuFS2Pnll1/UpUsXNWjQQA888IBOnTolSfrDH/7AZecAAMCruBR2nn/+efn6+uro0aMKCAhwtPfr10+rVq1yW3EAAAA3yqU1O6tXr9aXX36pOnXqOLVHR0fryJEjbikMAADAHVya2blw4YLTjE6uM2fOyG6333BRAAAA7uJS2Lnnnnv08ccfO97bbDbl5ORo6tSp6ty5s9uKAwAAuFEuncaaOnWqYmJitGPHDmVmZurFF1/U3r179euvv+o///mPu2sEAABwmUszO02aNNHu3bvVrl07de3aVRcuXFCfPn307bff6rbbbnN3jQAAAC4r8cxOVlaWunXrpvfee08TJ04sjZoAAADcpsQzO76+vtqzZ49sNltp1AMAAOBWLp3GGjRokD788EN31wIAAOB2Li1QzszM1F//+lclJiaqbdu2+Z6JNX36dLcUBwAAcKNKFHZ++ukn1atXT3v27FHr1q0lSQcPHnTqw+ktAADgTUp0Gis6OlpnzpzRunXrtG7dOtWsWVOLFy92vF+3bp3Wrl3r1gJPnDihAQMGKCQkRAEBAbr99tu1c+dOx3ZjjOLj4xUeHi5/f3/FxMRo7969bq0BAACUXSUKO3mfav7FF1/owoULbi3oWmfPnlXHjh3l6+urL774Qvv27dO0adNUtWpVR58pU6Zo+vTpmjVrlrZv366wsDB17dpV586dK7W6AABA2eHSmp1cecOPu7399tuKiIjQvHnzHG316tVz+vwZM2bo1VdfVZ8+fSRJCxYsUGhoqBYtWqThw4eXan0AAMD7lWhmx2az5VuTU5prdFasWKG2bdvq97//vWrWrKlWrVrpgw8+cGxPTk5WSkqKunXr5miz2+3q1KmTtmzZUuhxMzIylJ6e7vQCAADWVKKZHWOMhgwZ4njY5+XLl/X000/nuxpr6dKlbinup59+0pw5czR27Fi98sor+uabb/Tcc8/Jbrdr0KBBSklJkSSFhoY67RcaGlrk09cTEhK4ISIAAOVEicLO4MGDnd4PGDDArcXklZOTo7Zt22ry5MmSpFatWmnv3r2aM2eOBg0a5OiXd3bJGFPkjFNcXJzGjh3reJ+enq6IiAg3Vw8AALxBicLOtWtnboZatWqpSZMmTm2NGzfW559/LkkKCwuTJKWkpKhWrVqOPqmpqflme65lt9sds1MAAMDaXLqD8s3SsWNHHThwwKnt4MGDioyMlCRFRUUpLCxMiYmJju2ZmZnasGGDOnTocFNrBQAA3umGrsYqbc8//7w6dOigyZMnq2/fvvrmm2/0/vvv6/3335d09fTVmDFjNHnyZEVHRys6OlqTJ09WQECA+vfv7+HqAQCAN/DqsHPHHXdo2bJliouL0+uvv66oqCjNmDFDTzzxhKPPiy++qEuXLmnEiBE6e/as2rdvr9WrVysoKMiDlQMAAG/h1WFHkmJjYxUbG1vodpvNpvj4eMXHx9+8ogAAQJnh1Wt2AAAAbhRhBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWBphBwAAWFqZCjsJCQmy2WwaM2aMo80Yo/j4eIWHh8vf318xMTHau3ev54oEAABepcyEne3bt+v9999XixYtnNqnTJmi6dOna9asWdq+fbvCwsLUtWtXnTt3zkOVAgAAb1Imws758+f1xBNP6IMPPlC1atUc7cYYzZgxQ6+++qr69OmjZs2aacGCBbp48aIWLVrkwYoBAIC3KBNh59lnn1XPnj113333ObUnJycrJSVF3bp1c7TZ7XZ16tRJW7ZsKfR4GRkZSk9Pd3oBAABr8vF0AdezePFiJSUlafv27fm2paSkSJJCQ0Od2kNDQ3XkyJFCj5mQkKCJEye6t1AAAOCVvHpm59ixYxo9erQWLlyoSpUqFdrPZrM5vTfG5Gu7VlxcnNLS0hyvY8eOua1mAADgXbx6Zmfnzp1KTU1VmzZtHG3Z2dnauHGjZs2apQMHDki6OsNTq1YtR5/U1NR8sz3XstvtstvtpVc4AADwGl49s9OlSxd9//332rVrl+PVtm1bPfHEE9q1a5duvfVWhYWFKTEx0bFPZmamNmzYoA4dOniwcgAA4C28emYnKChIzZo1c2oLDAxUSEiIo33MmDGaPHmyoqOjFR0drcmTJysgIED9+/f3RMkAAMDLeHXYKY4XX3xRly5d0ogRI3T27Fm1b99eq1evVlBQkKdLAwAAXqDMhZ3169c7vbfZbIqPj1d8fLxH6gEAAN7Nq9fsAAAA3CjCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDQfTxcAoGw7evSozpw549RWo0YN1a1b10MVAYAzwg4Alx09elQNGzXW5UsXndor+QfowH/3E3gAeAXCDgCXnTlzRpcvXVRI7Dj5hkRIkrJ+OaZfVk7TmTNnCDsAvAJhB0CJXHvaav/+/ZIk35AI2cPqe7IsACgUYQdAsRV22goAvBlhB0Cx5T1tdemnHUrbtNDTZQFAkbj0HECJ5Z628gkO9XQpAHBdhB0AAGBpnMYC4BF578/DvXkAlBbCDoCbrqCFztybB0BpIewAuOnyLnTm3jwAShNhB4DHcH8eADcDC5QBAIClMbMDlFM8wBNAeUHYAcohHuAJoDwh7ADlUHEf4Jl39if3WVgAUJYQdoByrKgFwjwHC4BVEHYAFKig2R+ehQWgLCLsACjStbM/Wb8cK/Z+157y8raFzyzOBsoXwg4At8o+f1ay2TRgwABHmzctfGZxNlD+EHYAuFVOxnnJGLfcHbk4z88q6TO2irs4G4B1EHYAlIobvTtycZ6fdSPP2OLuzUD5QdgB4JWK8/wsnrEFoDgIOwC8WnFmYPL2yXs/IBYfA+UbYQeAZRS0OFpi8TFQ3nn1g0ATEhJ0xx13KCgoSDVr1tTDDz+sAwcOOPUxxig+Pl7h4eHy9/dXTEyM9u7d66GKgbJv//79SkpKKpN3S752cXTY4BkKGzxDIbHjdPnSxXyXmgMoP7w67GzYsEHPPvustm3bpsTERF25ckXdunXThQsXHH2mTJmi6dOna9asWdq+fbvCwsLUtWtXnTt3zoOVA2XPtbMibdq0yTc7Upbkntayh9V3XHEFoPzy6tNYq1atcno/b9481axZUzt37tQ999wjY4xmzJihV199VX369JEkLViwQKGhoVq0aJGGDx/uibKBMinvJePcLRmAVXh12MkrLS1NklS9enVJUnJyslJSUtStWzdHH7vdrk6dOmnLli2Fhp2MjAxlZGQ43qenp5di1UDZkjsrUpK7JRfHtafFPHGKLPczy+LpOQA3psyEHWOMxo4dq7vvvlvNmjWTJKWkpEiSQkNDnfqGhobqyJEjhR4rISFBEydOLL1iATgUtmi4vHw+AM8rM2Fn5MiR2r17tzZv3pxvm81mc3pvjMnXdq24uDiNHTvW8T49PV0REZzXB0pD3tNj0s19oCin5wCUibAzatQorVixQhs3blSdOnUc7WFhYZKuzvDUqlXL0Z6amppvtudadrtddru99AoGkI+rDxR19+d74rMBeJZXX41ljNHIkSO1dOlSrV27VlFRUU7bo6KiFBYWpsTEREdbZmamNmzYoA4dOtzscgEAgBfy6pmdZ599VosWLdI//vEPBQUFOdboBAcHy9/fXzabTWPGjNHkyZMVHR2t6OhoTZ48WQEBAerfv7+HqwdQUp5exAzAmrw67MyZM0eSFBMT49Q+b948DRkyRJL04osv6tKlSxoxYoTOnj2r9u3ba/Xq1QoKCrrJ1QJwFYuIAZQmrw47xpjr9rHZbIqPj1d8fHzpFwSgVHh6EXNBjh49mu+uyzxjCyibvDrsAChfPL2IOdfRo0fVsFFjXb500amdZ2wBZRNhBwDyOHPmjC5fuug005T1yzH9snKazpw5Q9gByhjCDgAU4tqZplzXLpzmtBZQNhB2AKAYClpEzWktoGwg7AAoUzx1eXreRdSc1gLKDsIOgDLBWy5PL+jUFgDvRtgBUCZ44+XpAMoGwg5QTlx735iyfHdib7k8PVfesWTRMuB9CDtAOVDYfWPgusJOq7FoGfA+hB3AAvLe7Tfv7ELe+8Zw+ufGFXRarbiLlq/35wXAvQg7QBlX0KxNYbMLuaeAvOH0j1WUdMFySf68ALgHYQco4/LO2nBJtHfjzwu4+Qg7gBe5kYdP5p1h8NT9aMqy3HG6GePFJezAzUPYAbyEux4+6S33oylLGDPA2gg7gJdw18MnuR9NyeUdM3eOV97ZOmbZgJuPsAN4GXed3vC2+9GUBe5ewM0l/4B3IOwAQCkpaLaOWTbg5iPsAEApY5YN8KwKni4AAACgNDGzA5QB1y5qzcjIkN1uL3AbvMPNvIT9em7kdgaAVRB2AC9W4CXRtgqSyfFcUSiUt13C7q7bGQBlHWEH8GKFXRLNglfvVJqXsLvCXbczAMo6wg5QBuS9JJoFr97N255Bxt2aUd6xQBkAAFgaMzuAB127eNQbFrPCe7CwGHAfwg7gIdxdF4VhYTHgXoQdwEPyLh719GJWeA8WFgPuRdgBPMzbFrPCe7CwGHAPFigDAABLY2YHALzAtQvUi1qsfu22ghYss+gdyI+wAwAeVNy7LhfUL++CZRa9AwUj7ACAB+W967JU8F2x8/YraMEyi96BghF2ABfkvQfK9U4nFNYHyFXcu2IXZ9Eyi94BZ4QdoIQKOlVQnNMJ3CMFADyDsAOUUN5TBcU5ncA9UlBairuwuaj9mHWE1RF2ABeV5HQC4G7FXdhcnP2YdYTVEXYAoAwq7sLm6+3HrCPKA8IOyrWCHraYkZEhu93ueM8UP7xZcRc2F7WflP8UWN7fg7zvpdL73SjOQ1B5UCpKwjJhZ/bs2Zo6dapOnTqlpk2basaMGfrd737n6bLgxQq9J4mtgmRyHG+Z4oeVFXo6LM/vQb73Kp3fjeI8BJUHpaKkLBF2lixZojFjxmj27Nnq2LGj3nvvPfXo0UP79u3jh74McNdl3Hn7XO//RAt62GLuaQBXpviLs1DU1cWkQGkp6nRY3vv1FPRg0k2bNqlx48aSijezcr3Z1P3791/3IahFPSj12npu5mxUcdzMGavSnPkqi7fesETYmT59uoYNG6Y//OEPkqQZM2boyy+/1Jw5c5SQkODh6lAUd13GXeD/6RXz/0QLOg1QkoXFxVko6upiUuBmKer3oKDfC1cWOhd3NjXvZxWn5gJ/x27SbFRx3MwZq9Kc+Sqrt94o82EnMzNTO3fu1Msvv+zU3q1bN23ZssVDVaG43HUZd2F3ji3q/w7dpTgLRV1dTAp4K1cWOhdnNtXV34u89dzMvwOKo6jZqOLMWJWkZncdpzjHLiu33ijzYefMmTPKzs5WaGioU3toaKhSUlIK3CcjI0MZGRmO92lpaZKk9PR0t9Z2/vz5q5+X8oNyMi9LkrJ+PS5J2rlzp2N7hQoVlJPj/H8feduK08fV/TzZ58CBA5KknKwM5WReVk7W1T+Xa8fHlT7mSqbT+9z/vna/3H2c/nz+3//B5rYV9OeVd7/cfa79rNzPd6VPkfXQp1z28fTnF9WnqN9Lyfl3Pu/vqaR8v6t5fy+k/H9vFvW7W5K/A/LWV1DN7upT0Hcv7O+kG63ZXccp6P2N/J19/vx5t/87m3s8Y0zRHU0Zd+LECSPJbNmyxal90qRJpmHDhgXuM2HCBCOJFy9evHjx4mWB17Fjx4rMCmV+ZqdGjRqqWLFivlmc1NTUfLM9ueLi4jR27FjH+5ycHP36668KCQmRzWZzW23p6emKiIjQsWPHVKVKFbcdt7xjXN2PMXU/xtT9GNPSUZbH1Rijc+fOKTw8vMh+ZT7s+Pn5qU2bNkpMTFTv3r0d7YmJiXrooYcK3Mdut+dboV+1atVSq7FKlSpl7geoLGBc3Y8xdT/G1P0Y09JRVsc1ODj4un3KfNiRpLFjx2rgwIFq27at7rrrLr3//vs6evSonn76aU+XBgAAPMwSYadfv3765Zdf9Prrr+vUqVNq1qyZ/v3vfysyMtLTpQEAAA+zRNiRpBEjRmjEiBGeLsOJ3W7XhAkT8p0yw41hXN2PMXU/xtT9GNPSUR7G1WbM9a7XAgAAKLsqeLoAAACA0kTYAQAAlkbYAQAAlkbYAQAAlkbYKUWzZ89WVFSUKlWqpDZt2mjTpk2eLslrbdy4Ub169VJ4eLhsNpuWL1/utN0Yo/j4eIWHh8vf318xMTHau3evU5+MjAyNGjVKNWrUUGBgoB588EEdP378Jn4L75KQkKA77rhDQUFBqlmzph5++GHHM2tyMa4lM2fOHLVo0cJx87W77rpLX3zxhWM743njEhISZLPZNGbMGEcb41oy8fHxstlsTq+wsDDH9nI5njf8cCoUaPHixcbX19d88MEHZt++fWb06NEmMDDQHDlyxNOleaV///vf5tVXXzWff/65kWSWLVvmtP2tt94yQUFB5vPPPzfff/+96devn6lVq5ZJT0939Hn66adN7dq1TWJioklKSjKdO3c2LVu2NFeuXLnJ38Y7dO/e3cybN8/s2bPH7Nq1y/Ts2dPUrVvXnD9/3tGHcS2ZFStWmH/961/mwIED5sCBA+aVV14xvr6+Zs+ePcYYxvNGffPNN6ZevXqmRYsWZvTo0Y52xrVkJkyYYJo2bWpOnTrleKWmpjq2l8fxJOyUknbt2pmnn37aqa1Ro0bm5Zdf9lBFZUfesJOTk2PCwsLMW2+95Wi7fPmyCQ4ONnPnzjXGGPPbb78ZX19fs3jxYkefEydOmAoVKphVq1bdtNq9WWpqqpFkNmzYYIxhXN2lWrVq5q9//SvjeYPOnTtnoqOjTWJiounUqZMj7DCuJTdhwgTTsmXLAreV1/HkNFYpyMzM1M6dO9WtWzen9m7dumnLli0eqqrsSk5OVkpKitN42u12derUyTGeO3fuVFZWllOf8PBwNWvWjDH/f9LS0iRJ1atXl8S43qjs7GwtXrxYFy5c0F133cV43qBnn31WPXv21H333efUzri65tChQwoPD1dUVJQee+wx/fTTT5LK73ha5g7K3uTMmTPKzs7O99T10NDQfE9nx/XljllB43nkyBFHHz8/P1WrVi1fH8b86jn6sWPH6u6771azZs0kMa6u+v7773XXXXfp8uXLqly5spYtW6YmTZo4/hFgPEtu8eLFSkpK0vbt2/Nt4+e05Nq3b6+PP/5YDRo00M8//6xJkyapQ4cO2rt3b7kdT8JOKbLZbE7vjTH52lB8rownY37VyJEjtXv3bm3evDnfNsa1ZBo2bKhdu3bpt99+0+eff67Bgwdrw4YNju2MZ8kcO3ZMo0eP1urVq1WpUqVC+zGuxdejRw/Hfzdv3lx33XWXbrvtNi1YsEB33nmnpPI3npzGKgU1atRQxYoV8yXg1NTUfGka15d7FUFR4xkWFqbMzEydPXu20D7l1ahRo7RixQqtW7dOderUcbQzrq7x8/NT/fr11bZtWyUkJKhly5Z69913GU8X7dy5U6mpqWrTpo18fHzk4+OjDRs26H/+53/k4+PjGBfG1XWBgYFq3ry5Dh06VG5/Tgk7pcDPz09t2rRRYmKiU3tiYqI6dOjgoarKrqioKIWFhTmNZ2ZmpjZs2OAYzzZt2sjX19epz6lTp7Rnz55yO+bGGI0cOVJLly7V2rVrFRUV5bSdcXUPY4wyMjIYTxd16dJF33//vXbt2uV4tW3bVk888YR27dqlW2+9lXG9QRkZGdq/f79q1apVfn9OPbEqujzIvfT8ww8/NPv27TNjxowxgYGB5vDhw54uzSudO3fOfPvtt+bbb781ksz06dPNt99+67hU/6233jLBwcFm6dKl5vvvvzePP/54gZdK1qlTx6xZs8YkJSWZe++9t0xfKnmjnnnmGRMcHGzWr1/vdAnqxYsXHX0Y15KJi4szGzduNMnJyWb37t3mlVdeMRUqVDCrV682xjCe7nLt1VjGMK4lNW7cOLN+/Xrz008/mW3btpnY2FgTFBTk+PenPI4nYacU/eUvfzGRkZHGz8/PtG7d2nHJL/Jbt26dkZTvNXjwYGPM1cslJ0yYYMLCwozdbjf33HOP+f77752OcenSJTNy5EhTvXp14+/vb2JjY83Ro0c98G28Q0HjKcnMmzfP0YdxLZmhQ4c6fqdvueUW06VLF0fQMYbxdJe8YYdxLZnc++b4+vqa8PBw06dPH7N3717H9vI4njZjjPHMnBIAAEDpY80OAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAACwNMIOAMuIiYnRmDFjPF0GAC9D2AHgFXr16qX77ruvwG1bt26VzWZTUlLSTa4KgBUQdgB4hWHDhmnt2rU6cuRIvm0fffSRbr/9drVu3doDlQEo6wg7ALxCbGysatasqfnz5zu1X7x4UUuWLNHDDz+sxx9/XHXq1FFAQICaN2+uTz/9tMhj2mw2LV++3KmtatWqTp9x4sQJ9evXT9WqVVNISIgeeughHT582LF9/fr1ateunQIDA1W1alV17NixwEAGwHsRdgB4BR8fHw0aNEjz58/XtY/s++yzz5SZmak//OEPatOmjVauXKk9e/boqaee0sCBA/X111+7/JkXL15U586dVblyZW3cuFGbN29W5cqVdf/99yszM1NXrlzRww8/rE6dOmn37t3aunWrnnrqKdlsNnd8ZQA3iY+nCwCAXEOHDtXUqVO1fv16de7cWdLVU1h9+vRR7dq1NX78eEffUaNGadWqVfrss8/Uvn17lz5v8eLFqlChgv761786Asy8efNUtWpVrV+/Xm3btlVaWppiY2N12223SZIaN258g98SwM3GzA4Ar9GoUSN16NBBH330kSTpxx9/1KZNmzR06FBlZ2frzTffVIsWLRQSEqLKlStr9erVOnr0qMuft3PnTv3www8KCgpS5cqVVblyZVWvXl2XL1/Wjz/+qOrVq2vIkCHq3r27evXqpXfffVenTp1y19cFcJMQdgB4lWHDhunzzz9Xenq65s2bp8jISHXp0kXTpk3TO++8oxdffFFr167Vrl271L17d2VmZhZ6LJvN5nRKTJKysrIc/52Tk6M2bdpo165dTq+DBw+qf//+kq7O9GzdulUdOnTQkiVL1KBBA23btq10vjyAUkHYAeBV+vbtq4oVK2rRokVasGCBnnzySdlsNm3atEkPPfSQBgwYoJYtW+rWW2/VoUOHijzWLbfc4jQTc+jQIV28eNHxvnXr1jp06JBq1qyp+vXrO72Cg4Md/Vq1aqW4uDht2bJFzZo106JFi9z/xQGUGsIOAK9SuXJl9evXT6+88opOnjypIUOGSJLq16+vxMREbdmyRfv379fw4cOVkpJS5LHuvfdezZo1S0lJSdqxY4eefvpp+fr6OrY/8cQTqlGjhh566CFt2rRJycnJ2rBhg0aPHq3jx48rOTlZcXFx2rp1q44cOaLVq1fr4MGDrNsByhjCDgCvM2zYMJ09e1b33Xef6tatK0l67bXX1Lp1a3Xv3l0xMTEKCwvTww8/XORxpk2bpoiICN1zzz3q37+/xo8fr4CAAMf2gIAAbdy4UXXr1lWfPn3UuHFjDR06VJcuXVKVKlUUEBCg//73v3rkkUfUoEEDPfXUUxo5cqSGDx9eml8fgJvZTN4T2gAAABbCzA4AALA0wg4AALA0wg4AALA0wg4AALA0wg4AALA0wg4AALA0wg4AALA0wg4AALA0wg4AALA0wg4AALA0wg4AALA0wg4AALC0/wv2uwZfJRsswgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for col in nummeric:\n",
    "    plot_binning(df[col], bins=100, name=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for col in categorical:\n",
    "    plot_binning(df[col], bins=len(df[col].unique()), name=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(columns=['HeartDisease']).values\n",
    "y = df['HeartDisease'].values\n",
    "X_st = standardize_data(X)\n",
    "#print(X_st)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_st, y, test_size=0.2, random_state=42)\n",
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LassoCV(cv=5, random_state=0, max_iter=10000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016043208596650652"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5031830498639663"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.8831168831168831 balanced accuracy:  0.8769458858413639\n"
     ]
    }
   ],
   "source": [
    "pred = reg.predict(X_test).round()\n",
    "f1Score = f1_score(y_test, pred)\n",
    "bAccuracy = balanced_accuracy_score(y_test, pred)\n",
    "print('f1 score: ', f1Score, 'balanced accuracy: ', bAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_bar_plot(df.columns[0:11], np.abs(reg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import copy\n",
    "def model_train(model, X_train, y_train, X_val, y_val):\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    n_epochs = 250   # number of epochs to run\n",
    "    batch_size = 10  # size of each batch\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    "    # Hold the best model\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_weights = None\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # print(epoch)\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        acc = (y_pred.round() == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(11, 10)\n",
    "        #self.dropout = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(10, 5)\n",
    "        self.fc3 = nn.Linear(5, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        #x = self.dropout(x)\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        #x = nn.functional.sigmoid(self.fc3(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class Deep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(11, 60)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(60, 60)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(60, 60)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = create_dataloaders_from_arrays(X_train, y_train, X_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.3742, -0.5301, -0.1474,  0.4266,  0.3068, -0.5511,  0.4935,  0.1514,\n",
       "          1.2199,  3.8151,  0.6181]),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_loader.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.629660\n",
      "Epoch: 2 \tTraining Loss: 0.430142\n",
      "Epoch: 3 \tTraining Loss: 0.371071\n",
      "Epoch: 4 \tTraining Loss: 0.349037\n",
      "Epoch: 5 \tTraining Loss: 0.339484\n",
      "Epoch: 6 \tTraining Loss: 0.330853\n",
      "Epoch: 7 \tTraining Loss: 0.320628\n",
      "Epoch: 8 \tTraining Loss: 0.315538\n",
      "Epoch: 9 \tTraining Loss: 0.308092\n",
      "Epoch: 10 \tTraining Loss: 0.303247\n",
      "Epoch: 11 \tTraining Loss: 0.300960\n",
      "Epoch: 12 \tTraining Loss: 0.298760\n",
      "Epoch: 13 \tTraining Loss: 0.287970\n",
      "Epoch: 14 \tTraining Loss: 0.279319\n",
      "Epoch: 15 \tTraining Loss: 0.283531\n",
      "Epoch: 16 \tTraining Loss: 0.273878\n",
      "Epoch: 17 \tTraining Loss: 0.272397\n",
      "Epoch: 18 \tTraining Loss: 0.267463\n",
      "Epoch: 19 \tTraining Loss: 0.264083\n",
      "Epoch: 20 \tTraining Loss: 0.261617\n",
      "Epoch: 21 \tTraining Loss: 0.257753\n",
      "Epoch: 22 \tTraining Loss: 0.256087\n",
      "Epoch: 23 \tTraining Loss: 0.252113\n",
      "Epoch: 24 \tTraining Loss: 0.254653\n",
      "Epoch: 25 \tTraining Loss: 0.246574\n",
      "Epoch: 26 \tTraining Loss: 0.254409\n",
      "Epoch: 27 \tTraining Loss: 0.256466\n",
      "Epoch: 28 \tTraining Loss: 0.241307\n",
      "Epoch: 29 \tTraining Loss: 0.236899\n",
      "Epoch: 30 \tTraining Loss: 0.236030\n",
      "Epoch: 31 \tTraining Loss: 0.239827\n",
      "Epoch: 32 \tTraining Loss: 0.236565\n",
      "Epoch: 33 \tTraining Loss: 0.238877\n",
      "Epoch: 34 \tTraining Loss: 0.239800\n",
      "Epoch: 35 \tTraining Loss: 0.225047\n",
      "Epoch: 36 \tTraining Loss: 0.231533\n",
      "Epoch: 37 \tTraining Loss: 0.222619\n",
      "Epoch: 38 \tTraining Loss: 0.223493\n",
      "Epoch: 39 \tTraining Loss: 0.227076\n",
      "Epoch: 40 \tTraining Loss: 0.224203\n",
      "Epoch: 41 \tTraining Loss: 0.220404\n",
      "Epoch: 42 \tTraining Loss: 0.219419\n",
      "Epoch: 43 \tTraining Loss: 0.212712\n",
      "Epoch: 44 \tTraining Loss: 0.213624\n",
      "Epoch: 45 \tTraining Loss: 0.213686\n",
      "Epoch: 46 \tTraining Loss: 0.218556\n",
      "Epoch: 47 \tTraining Loss: 0.210298\n",
      "Epoch: 48 \tTraining Loss: 0.209611\n",
      "Epoch: 49 \tTraining Loss: 0.227050\n",
      "Epoch: 50 \tTraining Loss: 0.210774\n",
      "Epoch: 51 \tTraining Loss: 0.201369\n",
      "Epoch: 52 \tTraining Loss: 0.201928\n",
      "Epoch: 53 \tTraining Loss: 0.195867\n",
      "Epoch: 54 \tTraining Loss: 0.189785\n",
      "Epoch: 55 \tTraining Loss: 0.187929\n",
      "Epoch: 56 \tTraining Loss: 0.194781\n",
      "Epoch: 57 \tTraining Loss: 0.191756\n",
      "Epoch: 58 \tTraining Loss: 0.186942\n",
      "Epoch: 59 \tTraining Loss: 0.187250\n",
      "Epoch: 60 \tTraining Loss: 0.184268\n",
      "Epoch: 61 \tTraining Loss: 0.178747\n",
      "Epoch: 62 \tTraining Loss: 0.184912\n",
      "Epoch: 63 \tTraining Loss: 0.175742\n",
      "Epoch: 64 \tTraining Loss: 0.171948\n",
      "Epoch: 65 \tTraining Loss: 0.166768\n",
      "Epoch: 66 \tTraining Loss: 0.182999\n",
      "Epoch: 67 \tTraining Loss: 0.183393\n",
      "Epoch: 68 \tTraining Loss: 0.163686\n",
      "Epoch: 69 \tTraining Loss: 0.166749\n",
      "Epoch: 70 \tTraining Loss: 0.164167\n",
      "Epoch: 71 \tTraining Loss: 0.164953\n",
      "Epoch: 72 \tTraining Loss: 0.171683\n",
      "Epoch: 73 \tTraining Loss: 0.176972\n",
      "Epoch: 74 \tTraining Loss: 0.158275\n",
      "Epoch: 75 \tTraining Loss: 0.148735\n",
      "Epoch: 76 \tTraining Loss: 0.145622\n",
      "Epoch: 77 \tTraining Loss: 0.141984\n",
      "Epoch: 78 \tTraining Loss: 0.148347\n",
      "Epoch: 79 \tTraining Loss: 0.136240\n",
      "Epoch: 80 \tTraining Loss: 0.133883\n",
      "Epoch: 81 \tTraining Loss: 0.143546\n",
      "Epoch: 82 \tTraining Loss: 0.134321\n",
      "Epoch: 83 \tTraining Loss: 0.140103\n",
      "Epoch: 84 \tTraining Loss: 0.134238\n",
      "Epoch: 85 \tTraining Loss: 0.137175\n",
      "Epoch: 86 \tTraining Loss: 0.135960\n",
      "Epoch: 87 \tTraining Loss: 0.133733\n",
      "Epoch: 88 \tTraining Loss: 0.125894\n",
      "Epoch: 89 \tTraining Loss: 0.139706\n",
      "Epoch: 90 \tTraining Loss: 0.129295\n",
      "Epoch: 91 \tTraining Loss: 0.126850\n",
      "Epoch: 92 \tTraining Loss: 0.126488\n",
      "Epoch: 93 \tTraining Loss: 0.127613\n",
      "Epoch: 94 \tTraining Loss: 0.133489\n",
      "Epoch: 95 \tTraining Loss: 0.129291\n",
      "Epoch: 96 \tTraining Loss: 0.123385\n",
      "Epoch: 97 \tTraining Loss: 0.121708\n",
      "Epoch: 98 \tTraining Loss: 0.121563\n",
      "Epoch: 99 \tTraining Loss: 0.129471\n",
      "Epoch: 100 \tTraining Loss: 0.123856\n",
      "Epoch: 101 \tTraining Loss: 0.115613\n",
      "Epoch: 102 \tTraining Loss: 0.109090\n",
      "Epoch: 103 \tTraining Loss: 0.108834\n",
      "Epoch: 104 \tTraining Loss: 0.106945\n",
      "Epoch: 105 \tTraining Loss: 0.120200\n",
      "Epoch: 106 \tTraining Loss: 0.118265\n",
      "Epoch: 107 \tTraining Loss: 0.126871\n",
      "Epoch: 108 \tTraining Loss: 0.110964\n",
      "Epoch: 109 \tTraining Loss: 0.101820\n",
      "Epoch: 110 \tTraining Loss: 0.099690\n",
      "Epoch: 111 \tTraining Loss: 0.101049\n",
      "Epoch: 112 \tTraining Loss: 0.101917\n",
      "Epoch: 113 \tTraining Loss: 0.100631\n",
      "Epoch: 114 \tTraining Loss: 0.101844\n",
      "Epoch: 115 \tTraining Loss: 0.112188\n",
      "Epoch: 116 \tTraining Loss: 0.108236\n",
      "Epoch: 117 \tTraining Loss: 0.113271\n",
      "Epoch: 118 \tTraining Loss: 0.115322\n",
      "Epoch: 119 \tTraining Loss: 0.101322\n",
      "Epoch: 120 \tTraining Loss: 0.098875\n",
      "Epoch: 121 \tTraining Loss: 0.097404\n",
      "Epoch: 122 \tTraining Loss: 0.090952\n",
      "Epoch: 123 \tTraining Loss: 0.089298\n",
      "Epoch: 124 \tTraining Loss: 0.088205\n",
      "Epoch: 125 \tTraining Loss: 0.084993\n",
      "Epoch: 126 \tTraining Loss: 0.084417\n",
      "Epoch: 127 \tTraining Loss: 0.085025\n",
      "Epoch: 128 \tTraining Loss: 0.084424\n",
      "Epoch: 129 \tTraining Loss: 0.085919\n",
      "Epoch: 130 \tTraining Loss: 0.083391\n",
      "Epoch: 131 \tTraining Loss: 0.085876\n",
      "Epoch: 132 \tTraining Loss: 0.088537\n",
      "Epoch: 133 \tTraining Loss: 0.096831\n",
      "Epoch: 134 \tTraining Loss: 0.091153\n",
      "Epoch: 135 \tTraining Loss: 0.081997\n",
      "Epoch: 136 \tTraining Loss: 0.081989\n",
      "Epoch: 137 \tTraining Loss: 0.082281\n",
      "Epoch: 138 \tTraining Loss: 0.078945\n",
      "Epoch: 139 \tTraining Loss: 0.076062\n",
      "Epoch: 140 \tTraining Loss: 0.075118\n",
      "Epoch: 141 \tTraining Loss: 0.080109\n",
      "Epoch: 142 \tTraining Loss: 0.094231\n",
      "Epoch: 143 \tTraining Loss: 0.086558\n",
      "Epoch: 144 \tTraining Loss: 0.081293\n",
      "Epoch: 145 \tTraining Loss: 0.080612\n",
      "Epoch: 146 \tTraining Loss: 0.079477\n",
      "Epoch: 147 \tTraining Loss: 0.078314\n",
      "Epoch: 148 \tTraining Loss: 0.073489\n",
      "Epoch: 149 \tTraining Loss: 0.072708\n",
      "Epoch: 150 \tTraining Loss: 0.071623\n",
      "Epoch: 151 \tTraining Loss: 0.070633\n",
      "Epoch: 152 \tTraining Loss: 0.073579\n",
      "Epoch: 153 \tTraining Loss: 0.081760\n",
      "Epoch: 154 \tTraining Loss: 0.103233\n",
      "Epoch: 155 \tTraining Loss: 0.101930\n",
      "Epoch: 156 \tTraining Loss: 0.083443\n",
      "Epoch: 157 \tTraining Loss: 0.078640\n",
      "Epoch: 158 \tTraining Loss: 0.076118\n",
      "Epoch: 159 \tTraining Loss: 0.092705\n",
      "Epoch: 160 \tTraining Loss: 0.120827\n",
      "Epoch: 161 \tTraining Loss: 0.110950\n",
      "Epoch: 162 \tTraining Loss: 0.090608\n",
      "Epoch: 163 \tTraining Loss: 0.098112\n",
      "Epoch: 164 \tTraining Loss: 0.123110\n",
      "Epoch: 165 \tTraining Loss: 0.098593\n",
      "Epoch: 166 \tTraining Loss: 0.091685\n",
      "Epoch: 167 \tTraining Loss: 0.083118\n",
      "Epoch: 168 \tTraining Loss: 0.079212\n",
      "Epoch: 169 \tTraining Loss: 0.076923\n",
      "Epoch: 170 \tTraining Loss: 0.072765\n",
      "Epoch: 171 \tTraining Loss: 0.068801\n",
      "Epoch: 172 \tTraining Loss: 0.067928\n",
      "Epoch: 173 \tTraining Loss: 0.069911\n",
      "Epoch: 174 \tTraining Loss: 0.088276\n",
      "Epoch: 175 \tTraining Loss: 0.080600\n",
      "Epoch: 176 \tTraining Loss: 0.082504\n",
      "Epoch: 177 \tTraining Loss: 0.166209\n",
      "Epoch: 178 \tTraining Loss: 0.150173\n",
      "Epoch: 179 \tTraining Loss: 0.123862\n",
      "Epoch: 180 \tTraining Loss: 0.098596\n",
      "Epoch: 181 \tTraining Loss: 0.081096\n",
      "Epoch: 182 \tTraining Loss: 0.077387\n",
      "Epoch: 183 \tTraining Loss: 0.072457\n",
      "Epoch: 184 \tTraining Loss: 0.071774\n",
      "Epoch: 185 \tTraining Loss: 0.068789\n",
      "Epoch: 186 \tTraining Loss: 0.065305\n",
      "Epoch: 187 \tTraining Loss: 0.063194\n",
      "Epoch: 188 \tTraining Loss: 0.064304\n",
      "Epoch: 189 \tTraining Loss: 0.065568\n",
      "Epoch: 190 \tTraining Loss: 0.067259\n",
      "Epoch: 191 \tTraining Loss: 0.082347\n",
      "Epoch: 192 \tTraining Loss: 0.083260\n",
      "Epoch: 193 \tTraining Loss: 0.103610\n",
      "Epoch: 194 \tTraining Loss: 0.086297\n",
      "Epoch: 195 \tTraining Loss: 0.079049\n",
      "Epoch: 196 \tTraining Loss: 0.067501\n",
      "Epoch: 197 \tTraining Loss: 0.064371\n",
      "Epoch: 198 \tTraining Loss: 0.063143\n",
      "Epoch: 199 \tTraining Loss: 0.060276\n",
      "Epoch: 200 \tTraining Loss: 0.059091\n",
      "Epoch: 201 \tTraining Loss: 0.059471\n",
      "Epoch: 202 \tTraining Loss: 0.060329\n",
      "Epoch: 203 \tTraining Loss: 0.067963\n",
      "Epoch: 204 \tTraining Loss: 0.072594\n",
      "Epoch: 205 \tTraining Loss: 0.059153\n",
      "Epoch: 206 \tTraining Loss: 0.063420\n",
      "Epoch: 207 \tTraining Loss: 0.061585\n",
      "Epoch: 208 \tTraining Loss: 0.058030\n",
      "Epoch: 209 \tTraining Loss: 0.058843\n",
      "Epoch: 210 \tTraining Loss: 0.058946\n",
      "Epoch: 211 \tTraining Loss: 0.059544\n",
      "Epoch: 212 \tTraining Loss: 0.060796\n",
      "Epoch: 213 \tTraining Loss: 0.058427\n",
      "Epoch: 214 \tTraining Loss: 0.056990\n",
      "Epoch: 215 \tTraining Loss: 0.057140\n",
      "Epoch: 216 \tTraining Loss: 0.058222\n",
      "Epoch: 217 \tTraining Loss: 0.056365\n",
      "Epoch: 218 \tTraining Loss: 0.056496\n",
      "Epoch: 219 \tTraining Loss: 0.055757\n",
      "Epoch: 220 \tTraining Loss: 0.055942\n",
      "Epoch: 221 \tTraining Loss: 0.055096\n",
      "Epoch: 222 \tTraining Loss: 0.055049\n",
      "Epoch: 223 \tTraining Loss: 0.054817\n",
      "Epoch: 224 \tTraining Loss: 0.053664\n",
      "Epoch: 225 \tTraining Loss: 0.053693\n",
      "Epoch: 226 \tTraining Loss: 0.055969\n",
      "Epoch: 227 \tTraining Loss: 0.055994\n",
      "Epoch: 228 \tTraining Loss: 0.059738\n",
      "Epoch: 229 \tTraining Loss: 0.079068\n",
      "Epoch: 230 \tTraining Loss: 0.071175\n",
      "Epoch: 231 \tTraining Loss: 0.067992\n",
      "Epoch: 232 \tTraining Loss: 0.076619\n",
      "Epoch: 233 \tTraining Loss: 0.095172\n",
      "Epoch: 234 \tTraining Loss: 0.086230\n",
      "Epoch: 235 \tTraining Loss: 0.081487\n",
      "Epoch: 236 \tTraining Loss: 0.152834\n",
      "Epoch: 237 \tTraining Loss: 0.109143\n",
      "Epoch: 238 \tTraining Loss: 0.090053\n",
      "Epoch: 239 \tTraining Loss: 0.126826\n",
      "Epoch: 240 \tTraining Loss: 0.089812\n",
      "Epoch: 241 \tTraining Loss: 0.069632\n",
      "Epoch: 242 \tTraining Loss: 0.068844\n",
      "Epoch: 243 \tTraining Loss: 0.065315\n",
      "Epoch: 244 \tTraining Loss: 0.061630\n",
      "Epoch: 245 \tTraining Loss: 0.061494\n",
      "Epoch: 246 \tTraining Loss: 0.060102\n",
      "Epoch: 247 \tTraining Loss: 0.060991\n",
      "Epoch: 248 \tTraining Loss: 0.059325\n",
      "Epoch: 249 \tTraining Loss: 0.057465\n",
      "Epoch: 250 \tTraining Loss: 0.057701\n",
      "Epoch: 251 \tTraining Loss: 0.056929\n",
      "Epoch: 252 \tTraining Loss: 0.056329\n",
      "Epoch: 253 \tTraining Loss: 0.057589\n",
      "Epoch: 254 \tTraining Loss: 0.055116\n",
      "Epoch: 255 \tTraining Loss: 0.054779\n",
      "Epoch: 256 \tTraining Loss: 0.055250\n",
      "Epoch: 257 \tTraining Loss: 0.056168\n",
      "Epoch: 258 \tTraining Loss: 0.054648\n",
      "Epoch: 259 \tTraining Loss: 0.058175\n",
      "Epoch: 260 \tTraining Loss: 0.064642\n",
      "Epoch: 261 \tTraining Loss: 0.059470\n",
      "Epoch: 262 \tTraining Loss: 0.055669\n",
      "Epoch: 263 \tTraining Loss: 0.058189\n",
      "Epoch: 264 \tTraining Loss: 0.059683\n",
      "Epoch: 265 \tTraining Loss: 0.058320\n",
      "Epoch: 266 \tTraining Loss: 0.062864\n",
      "Epoch: 267 \tTraining Loss: 0.074231\n",
      "Epoch: 268 \tTraining Loss: 0.056750\n",
      "Epoch: 269 \tTraining Loss: 0.055299\n",
      "Epoch: 270 \tTraining Loss: 0.059027\n",
      "Epoch: 271 \tTraining Loss: 0.056197\n",
      "Epoch: 272 \tTraining Loss: 0.054210\n",
      "Epoch: 273 \tTraining Loss: 0.052498\n",
      "Epoch: 274 \tTraining Loss: 0.051835\n",
      "Epoch: 275 \tTraining Loss: 0.050318\n",
      "Epoch: 276 \tTraining Loss: 0.049458\n",
      "Epoch: 277 \tTraining Loss: 0.049592\n",
      "Epoch: 278 \tTraining Loss: 0.049151\n",
      "Epoch: 279 \tTraining Loss: 0.050553\n",
      "Epoch: 280 \tTraining Loss: 0.054333\n",
      "Epoch: 281 \tTraining Loss: 0.055979\n",
      "Epoch: 282 \tTraining Loss: 0.054314\n",
      "Epoch: 283 \tTraining Loss: 0.051504\n",
      "Epoch: 284 \tTraining Loss: 0.055884\n",
      "Epoch: 285 \tTraining Loss: 0.055831\n",
      "Epoch: 286 \tTraining Loss: 0.055917\n",
      "Epoch: 287 \tTraining Loss: 0.053336\n",
      "Epoch: 288 \tTraining Loss: 0.052063\n",
      "Epoch: 289 \tTraining Loss: 0.055975\n",
      "Epoch: 290 \tTraining Loss: 0.062732\n",
      "Epoch: 291 \tTraining Loss: 0.055304\n",
      "Epoch: 292 \tTraining Loss: 0.068582\n",
      "Epoch: 293 \tTraining Loss: 0.050496\n",
      "Epoch: 294 \tTraining Loss: 0.053811\n",
      "Epoch: 295 \tTraining Loss: 0.071356\n",
      "Epoch: 296 \tTraining Loss: 0.073302\n",
      "Epoch: 297 \tTraining Loss: 0.102477\n",
      "Epoch: 298 \tTraining Loss: 0.094043\n",
      "Epoch: 299 \tTraining Loss: 0.116177\n",
      "Epoch: 300 \tTraining Loss: 0.111489\n",
      "Epoch: 301 \tTraining Loss: 0.104852\n",
      "Epoch: 302 \tTraining Loss: 0.095026\n",
      "Epoch: 303 \tTraining Loss: 0.079978\n",
      "Epoch: 304 \tTraining Loss: 0.067045\n",
      "Epoch: 305 \tTraining Loss: 0.057323\n",
      "Epoch: 306 \tTraining Loss: 0.051896\n",
      "Epoch: 307 \tTraining Loss: 0.052103\n",
      "Epoch: 308 \tTraining Loss: 0.049597\n",
      "Epoch: 309 \tTraining Loss: 0.048486\n",
      "Epoch: 310 \tTraining Loss: 0.051420\n",
      "Epoch: 311 \tTraining Loss: 0.046793\n",
      "Epoch: 312 \tTraining Loss: 0.053439\n",
      "Epoch: 313 \tTraining Loss: 0.052304\n",
      "Epoch: 314 \tTraining Loss: 0.049332\n",
      "Epoch: 315 \tTraining Loss: 0.060434\n",
      "Epoch: 316 \tTraining Loss: 0.059773\n",
      "Epoch: 317 \tTraining Loss: 0.072294\n",
      "Epoch: 318 \tTraining Loss: 0.058612\n",
      "Epoch: 319 \tTraining Loss: 0.054334\n",
      "Epoch: 320 \tTraining Loss: 0.068782\n",
      "Epoch: 321 \tTraining Loss: 0.136807\n",
      "Epoch: 322 \tTraining Loss: 0.082499\n",
      "Epoch: 323 \tTraining Loss: 0.080989\n",
      "Epoch: 324 \tTraining Loss: 0.167250\n",
      "Epoch: 325 \tTraining Loss: 0.098602\n",
      "Epoch: 326 \tTraining Loss: 0.065398\n",
      "Epoch: 327 \tTraining Loss: 0.064393\n",
      "Epoch: 328 \tTraining Loss: 0.096659\n",
      "Epoch: 329 \tTraining Loss: 0.056966\n",
      "Epoch: 330 \tTraining Loss: 0.051646\n",
      "Epoch: 331 \tTraining Loss: 0.046395\n",
      "Epoch: 332 \tTraining Loss: 0.044491\n",
      "Epoch: 333 \tTraining Loss: 0.044748\n",
      "Epoch: 334 \tTraining Loss: 0.043361\n",
      "Epoch: 335 \tTraining Loss: 0.046847\n",
      "Epoch: 336 \tTraining Loss: 0.043337\n",
      "Epoch: 337 \tTraining Loss: 0.042421\n",
      "Epoch: 338 \tTraining Loss: 0.042700\n",
      "Epoch: 339 \tTraining Loss: 0.044563\n",
      "Epoch: 340 \tTraining Loss: 0.040934\n",
      "Epoch: 341 \tTraining Loss: 0.040065\n",
      "Epoch: 342 \tTraining Loss: 0.039431\n",
      "Epoch: 343 \tTraining Loss: 0.039684\n",
      "Epoch: 344 \tTraining Loss: 0.039750\n",
      "Epoch: 345 \tTraining Loss: 0.039182\n",
      "Epoch: 346 \tTraining Loss: 0.039171\n",
      "Epoch: 347 \tTraining Loss: 0.040041\n",
      "Epoch: 348 \tTraining Loss: 0.039557\n",
      "Epoch: 349 \tTraining Loss: 0.040258\n",
      "Epoch: 350 \tTraining Loss: 0.040114\n",
      "Epoch: 351 \tTraining Loss: 0.039010\n",
      "Epoch: 352 \tTraining Loss: 0.038434\n",
      "Epoch: 353 \tTraining Loss: 0.039324\n",
      "Epoch: 354 \tTraining Loss: 0.039085\n",
      "Epoch: 355 \tTraining Loss: 0.041585\n",
      "Epoch: 356 \tTraining Loss: 0.041989\n",
      "Epoch: 357 \tTraining Loss: 0.042449\n",
      "Epoch: 358 \tTraining Loss: 0.039480\n",
      "Epoch: 359 \tTraining Loss: 0.040868\n",
      "Epoch: 360 \tTraining Loss: 0.040917\n",
      "Epoch: 361 \tTraining Loss: 0.037623\n",
      "Epoch: 362 \tTraining Loss: 0.038925\n",
      "Epoch: 363 \tTraining Loss: 0.037928\n",
      "Epoch: 364 \tTraining Loss: 0.037263\n",
      "Epoch: 365 \tTraining Loss: 0.037014\n",
      "Epoch: 366 \tTraining Loss: 0.037707\n",
      "Epoch: 367 \tTraining Loss: 0.037550\n",
      "Epoch: 368 \tTraining Loss: 0.036964\n",
      "Epoch: 369 \tTraining Loss: 0.037041\n",
      "Epoch: 370 \tTraining Loss: 0.036952\n",
      "Epoch: 371 \tTraining Loss: 0.041998\n",
      "Epoch: 372 \tTraining Loss: 0.046919\n",
      "Epoch: 373 \tTraining Loss: 0.048297\n",
      "Epoch: 374 \tTraining Loss: 0.048061\n",
      "Epoch: 375 \tTraining Loss: 0.052983\n",
      "Epoch: 376 \tTraining Loss: 0.081536\n",
      "Epoch: 377 \tTraining Loss: 0.115885\n",
      "Epoch: 378 \tTraining Loss: 0.095872\n",
      "Epoch: 379 \tTraining Loss: 0.102730\n",
      "Epoch: 380 \tTraining Loss: 0.103255\n",
      "Epoch: 381 \tTraining Loss: 0.079098\n",
      "Epoch: 382 \tTraining Loss: 0.065279\n",
      "Epoch: 383 \tTraining Loss: 0.049728\n",
      "Epoch: 384 \tTraining Loss: 0.049617\n",
      "Epoch: 385 \tTraining Loss: 0.045915\n",
      "Epoch: 386 \tTraining Loss: 0.045960\n",
      "Epoch: 387 \tTraining Loss: 0.042828\n",
      "Epoch: 388 \tTraining Loss: 0.043440\n",
      "Epoch: 389 \tTraining Loss: 0.042654\n",
      "Epoch: 390 \tTraining Loss: 0.041105\n",
      "Epoch: 391 \tTraining Loss: 0.038703\n",
      "Epoch: 392 \tTraining Loss: 0.039012\n",
      "Epoch: 393 \tTraining Loss: 0.048011\n",
      "Epoch: 394 \tTraining Loss: 0.068332\n",
      "Epoch: 395 \tTraining Loss: 0.112882\n",
      "Epoch: 396 \tTraining Loss: 0.130029\n",
      "Epoch: 397 \tTraining Loss: 0.100832\n",
      "Epoch: 398 \tTraining Loss: 0.076686\n",
      "Epoch: 399 \tTraining Loss: 0.057532\n",
      "Epoch: 400 \tTraining Loss: 0.045136\n",
      "Epoch: 401 \tTraining Loss: 0.042783\n",
      "Epoch: 402 \tTraining Loss: 0.040870\n",
      "Epoch: 403 \tTraining Loss: 0.038244\n",
      "Epoch: 404 \tTraining Loss: 0.040460\n",
      "Epoch: 405 \tTraining Loss: 0.037717\n",
      "Epoch: 406 \tTraining Loss: 0.037950\n",
      "Epoch: 407 \tTraining Loss: 0.037901\n",
      "Epoch: 408 \tTraining Loss: 0.037426\n",
      "Epoch: 409 \tTraining Loss: 0.036755\n",
      "Epoch: 410 \tTraining Loss: 0.037024\n",
      "Epoch: 411 \tTraining Loss: 0.036696\n",
      "Epoch: 412 \tTraining Loss: 0.037000\n",
      "Epoch: 413 \tTraining Loss: 0.036713\n",
      "Epoch: 414 \tTraining Loss: 0.036219\n",
      "Epoch: 415 \tTraining Loss: 0.036156\n",
      "Epoch: 416 \tTraining Loss: 0.035705\n",
      "Epoch: 417 \tTraining Loss: 0.036147\n",
      "Epoch: 418 \tTraining Loss: 0.037965\n",
      "Epoch: 419 \tTraining Loss: 0.043837\n",
      "Epoch: 420 \tTraining Loss: 0.043210\n",
      "Epoch: 421 \tTraining Loss: 0.036859\n",
      "Epoch: 422 \tTraining Loss: 0.036692\n",
      "Epoch: 423 \tTraining Loss: 0.042522\n",
      "Epoch: 424 \tTraining Loss: 0.040487\n",
      "Epoch: 425 \tTraining Loss: 0.043246\n",
      "Epoch: 426 \tTraining Loss: 0.047900\n",
      "Epoch: 427 \tTraining Loss: 0.057064\n",
      "Epoch: 428 \tTraining Loss: 0.041289\n",
      "Epoch: 429 \tTraining Loss: 0.039114\n",
      "Epoch: 430 \tTraining Loss: 0.037940\n",
      "Epoch: 431 \tTraining Loss: 0.035777\n",
      "Epoch: 432 \tTraining Loss: 0.038339\n",
      "Epoch: 433 \tTraining Loss: 0.037400\n",
      "Epoch: 434 \tTraining Loss: 0.034876\n",
      "Epoch: 435 \tTraining Loss: 0.034581\n",
      "Epoch: 436 \tTraining Loss: 0.035029\n",
      "Epoch: 437 \tTraining Loss: 0.035227\n",
      "Epoch: 438 \tTraining Loss: 0.035467\n",
      "Epoch: 439 \tTraining Loss: 0.041198\n",
      "Epoch: 440 \tTraining Loss: 0.041045\n",
      "Epoch: 441 \tTraining Loss: 0.037347\n",
      "Epoch: 442 \tTraining Loss: 0.037122\n",
      "Epoch: 443 \tTraining Loss: 0.035881\n",
      "Epoch: 444 \tTraining Loss: 0.036114\n",
      "Epoch: 445 \tTraining Loss: 0.036159\n",
      "Epoch: 446 \tTraining Loss: 0.035599\n",
      "Epoch: 447 \tTraining Loss: 0.034950\n",
      "Epoch: 448 \tTraining Loss: 0.034415\n",
      "Epoch: 449 \tTraining Loss: 0.034235\n",
      "Epoch: 450 \tTraining Loss: 0.035763\n",
      "Epoch: 451 \tTraining Loss: 0.034884\n",
      "Epoch: 452 \tTraining Loss: 0.036306\n",
      "Epoch: 453 \tTraining Loss: 0.036083\n",
      "Epoch: 454 \tTraining Loss: 0.037020\n",
      "Epoch: 455 \tTraining Loss: 0.034997\n",
      "Epoch: 456 \tTraining Loss: 0.034400\n",
      "Epoch: 457 \tTraining Loss: 0.034088\n",
      "Epoch: 458 \tTraining Loss: 0.033700\n",
      "Epoch: 459 \tTraining Loss: 0.034152\n",
      "Epoch: 460 \tTraining Loss: 0.034258\n",
      "Epoch: 461 \tTraining Loss: 0.035310\n",
      "Epoch: 462 \tTraining Loss: 0.034889\n",
      "Epoch: 463 \tTraining Loss: 0.035474\n",
      "Epoch: 464 \tTraining Loss: 0.057261\n",
      "Epoch: 465 \tTraining Loss: 0.069547\n",
      "Epoch: 466 \tTraining Loss: 0.084216\n",
      "Epoch: 467 \tTraining Loss: 0.107220\n",
      "Epoch: 468 \tTraining Loss: 0.135024\n",
      "Epoch: 469 \tTraining Loss: 0.229346\n",
      "Epoch: 470 \tTraining Loss: 0.243706\n",
      "Epoch: 471 \tTraining Loss: 0.213161\n",
      "Epoch: 472 \tTraining Loss: 0.149065\n",
      "Epoch: 473 \tTraining Loss: 0.106492\n",
      "Epoch: 474 \tTraining Loss: 0.101929\n",
      "Epoch: 475 \tTraining Loss: 0.068797\n",
      "Epoch: 476 \tTraining Loss: 0.064594\n",
      "Epoch: 477 \tTraining Loss: 0.063758\n",
      "Epoch: 478 \tTraining Loss: 0.052513\n",
      "Epoch: 479 \tTraining Loss: 0.046393\n",
      "Epoch: 480 \tTraining Loss: 0.045834\n",
      "Epoch: 481 \tTraining Loss: 0.047091\n",
      "Epoch: 482 \tTraining Loss: 0.043909\n",
      "Epoch: 483 \tTraining Loss: 0.049573\n",
      "Epoch: 484 \tTraining Loss: 0.053368\n",
      "Epoch: 485 \tTraining Loss: 0.050332\n",
      "Epoch: 486 \tTraining Loss: 0.048204\n",
      "Epoch: 487 \tTraining Loss: 0.041445\n",
      "Epoch: 488 \tTraining Loss: 0.041580\n",
      "Epoch: 489 \tTraining Loss: 0.048503\n",
      "Epoch: 490 \tTraining Loss: 0.039468\n",
      "Epoch: 491 \tTraining Loss: 0.039215\n",
      "Epoch: 492 \tTraining Loss: 0.039238\n",
      "Epoch: 493 \tTraining Loss: 0.040655\n",
      "Epoch: 494 \tTraining Loss: 0.038816\n",
      "Epoch: 495 \tTraining Loss: 0.038568\n",
      "Epoch: 496 \tTraining Loss: 0.039231\n",
      "Epoch: 497 \tTraining Loss: 0.040616\n",
      "Epoch: 498 \tTraining Loss: 0.038829\n",
      "Epoch: 499 \tTraining Loss: 0.038674\n",
      "Epoch: 500 \tTraining Loss: 0.037560\n",
      "Epoch: 501 \tTraining Loss: 0.037555\n",
      "Epoch: 502 \tTraining Loss: 0.038049\n",
      "Epoch: 503 \tTraining Loss: 0.037060\n",
      "Epoch: 504 \tTraining Loss: 0.038826\n",
      "Epoch: 505 \tTraining Loss: 0.036330\n",
      "Epoch: 506 \tTraining Loss: 0.035873\n",
      "Epoch: 507 \tTraining Loss: 0.036027\n",
      "Epoch: 508 \tTraining Loss: 0.036100\n",
      "Epoch: 509 \tTraining Loss: 0.035735\n",
      "Epoch: 510 \tTraining Loss: 0.035888\n",
      "Epoch: 511 \tTraining Loss: 0.035669\n",
      "Epoch: 512 \tTraining Loss: 0.036620\n",
      "Epoch: 513 \tTraining Loss: 0.036617\n",
      "Epoch: 514 \tTraining Loss: 0.036207\n",
      "Epoch: 515 \tTraining Loss: 0.036128\n",
      "Epoch: 516 \tTraining Loss: 0.036835\n",
      "Epoch: 517 \tTraining Loss: 0.036863\n",
      "Epoch: 518 \tTraining Loss: 0.038032\n",
      "Epoch: 519 \tTraining Loss: 0.038123\n",
      "Epoch: 520 \tTraining Loss: 0.037494\n",
      "Epoch: 521 \tTraining Loss: 0.036115\n",
      "Epoch: 522 \tTraining Loss: 0.036127\n",
      "Epoch: 523 \tTraining Loss: 0.035545\n",
      "Epoch: 524 \tTraining Loss: 0.035984\n",
      "Epoch: 525 \tTraining Loss: 0.035099\n",
      "Epoch: 526 \tTraining Loss: 0.034856\n",
      "Epoch: 527 \tTraining Loss: 0.035390\n",
      "Epoch: 528 \tTraining Loss: 0.035476\n",
      "Epoch: 529 \tTraining Loss: 0.035495\n",
      "Epoch: 530 \tTraining Loss: 0.035780\n",
      "Epoch: 531 \tTraining Loss: 0.038024\n",
      "Epoch: 532 \tTraining Loss: 0.049517\n",
      "Epoch: 533 \tTraining Loss: 0.039418\n",
      "Epoch: 534 \tTraining Loss: 0.043801\n",
      "Epoch: 535 \tTraining Loss: 0.041276\n",
      "Epoch: 536 \tTraining Loss: 0.045091\n",
      "Epoch: 537 \tTraining Loss: 0.039205\n",
      "Epoch: 538 \tTraining Loss: 0.043211\n",
      "Epoch: 539 \tTraining Loss: 0.042916\n",
      "Epoch: 540 \tTraining Loss: 0.075421\n",
      "Epoch: 541 \tTraining Loss: 0.085162\n",
      "Epoch: 542 \tTraining Loss: 0.113029\n",
      "Epoch: 543 \tTraining Loss: 0.101927\n",
      "Epoch: 544 \tTraining Loss: 0.087412\n",
      "Epoch: 545 \tTraining Loss: 0.049923\n",
      "Epoch: 546 \tTraining Loss: 0.046550\n",
      "Epoch: 547 \tTraining Loss: 0.040985\n",
      "Epoch: 548 \tTraining Loss: 0.048753\n",
      "Epoch: 549 \tTraining Loss: 0.042859\n",
      "Epoch: 550 \tTraining Loss: 0.038273\n",
      "Epoch: 551 \tTraining Loss: 0.038350\n",
      "Epoch: 552 \tTraining Loss: 0.039476\n",
      "Epoch: 553 \tTraining Loss: 0.038446\n",
      "Epoch: 554 \tTraining Loss: 0.037771\n",
      "Epoch: 555 \tTraining Loss: 0.041242\n",
      "Epoch: 556 \tTraining Loss: 0.041336\n",
      "Epoch: 557 \tTraining Loss: 0.042130\n",
      "Epoch: 558 \tTraining Loss: 0.038092\n",
      "Epoch: 559 \tTraining Loss: 0.038768\n",
      "Epoch: 560 \tTraining Loss: 0.034590\n",
      "Epoch: 561 \tTraining Loss: 0.034795\n",
      "Epoch: 562 \tTraining Loss: 0.035556\n",
      "Epoch: 563 \tTraining Loss: 0.035450\n",
      "Epoch: 564 \tTraining Loss: 0.036048\n",
      "Epoch: 565 \tTraining Loss: 0.035353\n",
      "Epoch: 566 \tTraining Loss: 0.041522\n",
      "Epoch: 567 \tTraining Loss: 0.060670\n",
      "Epoch: 568 \tTraining Loss: 0.045291\n",
      "Epoch: 569 \tTraining Loss: 0.058921\n",
      "Epoch: 570 \tTraining Loss: 0.048164\n",
      "Epoch: 571 \tTraining Loss: 0.060199\n",
      "Epoch: 572 \tTraining Loss: 0.088676\n",
      "Epoch: 573 \tTraining Loss: 0.063140\n",
      "Epoch: 574 \tTraining Loss: 0.057778\n",
      "Epoch: 575 \tTraining Loss: 0.053645\n",
      "Epoch: 576 \tTraining Loss: 0.047865\n",
      "Epoch: 577 \tTraining Loss: 0.046635\n",
      "Epoch: 578 \tTraining Loss: 0.039915\n",
      "Epoch: 579 \tTraining Loss: 0.035787\n",
      "Epoch: 580 \tTraining Loss: 0.034193\n",
      "Epoch: 581 \tTraining Loss: 0.035300\n",
      "Epoch: 582 \tTraining Loss: 0.033918\n",
      "Epoch: 583 \tTraining Loss: 0.034034\n",
      "Epoch: 584 \tTraining Loss: 0.033086\n",
      "Epoch: 585 \tTraining Loss: 0.033435\n",
      "Epoch: 586 \tTraining Loss: 0.034935\n",
      "Epoch: 587 \tTraining Loss: 0.034037\n",
      "Epoch: 588 \tTraining Loss: 0.032579\n",
      "Epoch: 589 \tTraining Loss: 0.033095\n",
      "Epoch: 590 \tTraining Loss: 0.032258\n",
      "Epoch: 591 \tTraining Loss: 0.032464\n",
      "Epoch: 592 \tTraining Loss: 0.031929\n",
      "Epoch: 593 \tTraining Loss: 0.032029\n",
      "Epoch: 594 \tTraining Loss: 0.032277\n",
      "Epoch: 595 \tTraining Loss: 0.033272\n",
      "Epoch: 596 \tTraining Loss: 0.033380\n",
      "Epoch: 597 \tTraining Loss: 0.034461\n",
      "Epoch: 598 \tTraining Loss: 0.038975\n",
      "Epoch: 599 \tTraining Loss: 0.049808\n",
      "Epoch: 600 \tTraining Loss: 0.043486\n",
      "Epoch: 601 \tTraining Loss: 0.060152\n",
      "Epoch: 602 \tTraining Loss: 0.076673\n",
      "Epoch: 603 \tTraining Loss: 0.218149\n",
      "Epoch: 604 \tTraining Loss: 0.087532\n",
      "Epoch: 605 \tTraining Loss: 0.090858\n",
      "Epoch: 606 \tTraining Loss: 0.149243\n",
      "Epoch: 607 \tTraining Loss: 0.112733\n",
      "Epoch: 608 \tTraining Loss: 0.081557\n",
      "Epoch: 609 \tTraining Loss: 0.081161\n",
      "Epoch: 610 \tTraining Loss: 0.066900\n",
      "Epoch: 611 \tTraining Loss: 0.056786\n",
      "Epoch: 612 \tTraining Loss: 0.046851\n",
      "Epoch: 613 \tTraining Loss: 0.041330\n",
      "Epoch: 614 \tTraining Loss: 0.038161\n",
      "Epoch: 615 \tTraining Loss: 0.039202\n",
      "Epoch: 616 \tTraining Loss: 0.039291\n",
      "Epoch: 617 \tTraining Loss: 0.040247\n",
      "Epoch: 618 \tTraining Loss: 0.037041\n",
      "Epoch: 619 \tTraining Loss: 0.036932\n",
      "Epoch: 620 \tTraining Loss: 0.037916\n",
      "Epoch: 621 \tTraining Loss: 0.037281\n",
      "Epoch: 622 \tTraining Loss: 0.037428\n",
      "Epoch: 623 \tTraining Loss: 0.036876\n",
      "Epoch: 624 \tTraining Loss: 0.036409\n",
      "Epoch: 625 \tTraining Loss: 0.036639\n",
      "Epoch: 626 \tTraining Loss: 0.037446\n",
      "Epoch: 627 \tTraining Loss: 0.038511\n",
      "Epoch: 628 \tTraining Loss: 0.044736\n",
      "Epoch: 629 \tTraining Loss: 0.036090\n",
      "Epoch: 630 \tTraining Loss: 0.036997\n",
      "Epoch: 631 \tTraining Loss: 0.037493\n",
      "Epoch: 632 \tTraining Loss: 0.037876\n",
      "Epoch: 633 \tTraining Loss: 0.036555\n",
      "Epoch: 634 \tTraining Loss: 0.036122\n",
      "Epoch: 635 \tTraining Loss: 0.036590\n",
      "Epoch: 636 \tTraining Loss: 0.035617\n",
      "Epoch: 637 \tTraining Loss: 0.036545\n",
      "Epoch: 638 \tTraining Loss: 0.035328\n",
      "Epoch: 639 \tTraining Loss: 0.036551\n",
      "Epoch: 640 \tTraining Loss: 0.036681\n",
      "Epoch: 641 \tTraining Loss: 0.040506\n",
      "Epoch: 642 \tTraining Loss: 0.034395\n",
      "Epoch: 643 \tTraining Loss: 0.037403\n",
      "Epoch: 644 \tTraining Loss: 0.036634\n",
      "Epoch: 645 \tTraining Loss: 0.034912\n",
      "Epoch: 646 \tTraining Loss: 0.035622\n",
      "Epoch: 647 \tTraining Loss: 0.035294\n",
      "Epoch: 648 \tTraining Loss: 0.035662\n",
      "Epoch: 649 \tTraining Loss: 0.035519\n",
      "Epoch: 650 \tTraining Loss: 0.035497\n",
      "Epoch: 651 \tTraining Loss: 0.036472\n",
      "Epoch: 652 \tTraining Loss: 0.035414\n",
      "Epoch: 653 \tTraining Loss: 0.035250\n",
      "Epoch: 654 \tTraining Loss: 0.035489\n",
      "Epoch: 655 \tTraining Loss: 0.035634\n",
      "Epoch: 656 \tTraining Loss: 0.036401\n",
      "Epoch: 657 \tTraining Loss: 0.036957\n",
      "Epoch: 658 \tTraining Loss: 0.036829\n",
      "Epoch: 659 \tTraining Loss: 0.047440\n",
      "Epoch: 660 \tTraining Loss: 0.055245\n",
      "Epoch: 661 \tTraining Loss: 0.063275\n",
      "Epoch: 662 \tTraining Loss: 0.101878\n",
      "Epoch: 663 \tTraining Loss: 0.125682\n",
      "Epoch: 664 \tTraining Loss: 0.149537\n",
      "Epoch: 665 \tTraining Loss: 0.161694\n",
      "Epoch: 666 \tTraining Loss: 0.096516\n",
      "Epoch: 667 \tTraining Loss: 0.060821\n",
      "Epoch: 668 \tTraining Loss: 0.052665\n",
      "Epoch: 669 \tTraining Loss: 0.046902\n",
      "Epoch: 670 \tTraining Loss: 0.043017\n",
      "Epoch: 671 \tTraining Loss: 0.043475\n",
      "Epoch: 672 \tTraining Loss: 0.040965\n",
      "Epoch: 673 \tTraining Loss: 0.040981\n",
      "Epoch: 674 \tTraining Loss: 0.041945\n",
      "Epoch: 675 \tTraining Loss: 0.038446\n",
      "Epoch: 676 \tTraining Loss: 0.038761\n",
      "Epoch: 677 \tTraining Loss: 0.039387\n",
      "Epoch: 678 \tTraining Loss: 0.042661\n",
      "Epoch: 679 \tTraining Loss: 0.039414\n",
      "Epoch: 680 \tTraining Loss: 0.037931\n",
      "Epoch: 681 \tTraining Loss: 0.038790\n",
      "Epoch: 682 \tTraining Loss: 0.037961\n",
      "Epoch: 683 \tTraining Loss: 0.038620\n",
      "Epoch: 684 \tTraining Loss: 0.037901\n",
      "Epoch: 685 \tTraining Loss: 0.038233\n",
      "Epoch: 686 \tTraining Loss: 0.036994\n",
      "Epoch: 687 \tTraining Loss: 0.036797\n",
      "Epoch: 688 \tTraining Loss: 0.038126\n",
      "Epoch: 689 \tTraining Loss: 0.036783\n",
      "Epoch: 690 \tTraining Loss: 0.034483\n",
      "Epoch: 691 \tTraining Loss: 0.035707\n",
      "Epoch: 692 \tTraining Loss: 0.046545\n",
      "Epoch: 693 \tTraining Loss: 0.053414\n",
      "Epoch: 694 \tTraining Loss: 0.052482\n",
      "Epoch: 695 \tTraining Loss: 0.057932\n",
      "Epoch: 696 \tTraining Loss: 0.059497\n",
      "Epoch: 697 \tTraining Loss: 0.055928\n",
      "Epoch: 698 \tTraining Loss: 0.054163\n",
      "Epoch: 699 \tTraining Loss: 0.042941\n",
      "Epoch: 700 \tTraining Loss: 0.044067\n",
      "Epoch: 701 \tTraining Loss: 0.037380\n",
      "Epoch: 702 \tTraining Loss: 0.038331\n",
      "Epoch: 703 \tTraining Loss: 0.035818\n",
      "Epoch: 704 \tTraining Loss: 0.035705\n",
      "Epoch: 705 \tTraining Loss: 0.035362\n",
      "Epoch: 706 \tTraining Loss: 0.035638\n",
      "Epoch: 707 \tTraining Loss: 0.034880\n",
      "Epoch: 708 \tTraining Loss: 0.035377\n",
      "Epoch: 709 \tTraining Loss: 0.034984\n",
      "Epoch: 710 \tTraining Loss: 0.035304\n",
      "Epoch: 711 \tTraining Loss: 0.035540\n",
      "Epoch: 712 \tTraining Loss: 0.034352\n",
      "Epoch: 713 \tTraining Loss: 0.035329\n",
      "Epoch: 714 \tTraining Loss: 0.038199\n",
      "Epoch: 715 \tTraining Loss: 0.037508\n",
      "Epoch: 716 \tTraining Loss: 0.035475\n",
      "Epoch: 717 \tTraining Loss: 0.036197\n",
      "Epoch: 718 \tTraining Loss: 0.034993\n",
      "Epoch: 719 \tTraining Loss: 0.035658\n",
      "Epoch: 720 \tTraining Loss: 0.036577\n",
      "Epoch: 721 \tTraining Loss: 0.038464\n",
      "Epoch: 722 \tTraining Loss: 0.035466\n",
      "Epoch: 723 \tTraining Loss: 0.035271\n",
      "Epoch: 724 \tTraining Loss: 0.034812\n",
      "Epoch: 725 \tTraining Loss: 0.035905\n",
      "Epoch: 726 \tTraining Loss: 0.035613\n",
      "Epoch: 727 \tTraining Loss: 0.034485\n",
      "Epoch: 728 \tTraining Loss: 0.034673\n",
      "Epoch: 729 \tTraining Loss: 0.036132\n",
      "Epoch: 730 \tTraining Loss: 0.036809\n",
      "Epoch: 731 \tTraining Loss: 0.035818\n",
      "Epoch: 732 \tTraining Loss: 0.035071\n",
      "Epoch: 733 \tTraining Loss: 0.034775\n",
      "Epoch: 734 \tTraining Loss: 0.035008\n",
      "Epoch: 735 \tTraining Loss: 0.035663\n",
      "Epoch: 736 \tTraining Loss: 0.036865\n",
      "Epoch: 737 \tTraining Loss: 0.033866\n",
      "Epoch: 738 \tTraining Loss: 0.035025\n",
      "Epoch: 739 \tTraining Loss: 0.035598\n",
      "Epoch: 740 \tTraining Loss: 0.034432\n",
      "Epoch: 741 \tTraining Loss: 0.034816\n",
      "Epoch: 742 \tTraining Loss: 0.038414\n",
      "Epoch: 743 \tTraining Loss: 0.042599\n",
      "Epoch: 744 \tTraining Loss: 0.044627\n",
      "Epoch: 745 \tTraining Loss: 0.041251\n",
      "Epoch: 746 \tTraining Loss: 0.046729\n",
      "Epoch: 747 \tTraining Loss: 0.043921\n",
      "Epoch: 748 \tTraining Loss: 0.061650\n",
      "Epoch: 749 \tTraining Loss: 0.059430\n",
      "Epoch: 750 \tTraining Loss: 0.088186\n",
      "Epoch: 751 \tTraining Loss: 0.097657\n",
      "Epoch: 752 \tTraining Loss: 0.171852\n",
      "Epoch: 753 \tTraining Loss: 0.119201\n",
      "Epoch: 754 \tTraining Loss: 0.102956\n",
      "Epoch: 755 \tTraining Loss: 0.059712\n",
      "Epoch: 756 \tTraining Loss: 0.067605\n",
      "Epoch: 757 \tTraining Loss: 0.054897\n",
      "Epoch: 758 \tTraining Loss: 0.048856\n",
      "Epoch: 759 \tTraining Loss: 0.041750\n",
      "Epoch: 760 \tTraining Loss: 0.040812\n",
      "Epoch: 761 \tTraining Loss: 0.043438\n",
      "Epoch: 762 \tTraining Loss: 0.053164\n",
      "Epoch: 763 \tTraining Loss: 0.047908\n",
      "Epoch: 764 \tTraining Loss: 0.057363\n",
      "Epoch: 765 \tTraining Loss: 0.035971\n",
      "Epoch: 766 \tTraining Loss: 0.035970\n",
      "Epoch: 767 \tTraining Loss: 0.040264\n",
      "Epoch: 768 \tTraining Loss: 0.040738\n",
      "Epoch: 769 \tTraining Loss: 0.035752\n",
      "Epoch: 770 \tTraining Loss: 0.034037\n",
      "Epoch: 771 \tTraining Loss: 0.032195\n",
      "Epoch: 772 \tTraining Loss: 0.032957\n",
      "Epoch: 773 \tTraining Loss: 0.032576\n",
      "Epoch: 774 \tTraining Loss: 0.032885\n",
      "Epoch: 775 \tTraining Loss: 0.034542\n",
      "Epoch: 776 \tTraining Loss: 0.032454\n",
      "Epoch: 777 \tTraining Loss: 0.032496\n",
      "Epoch: 778 \tTraining Loss: 0.033489\n",
      "Epoch: 779 \tTraining Loss: 0.036982\n",
      "Epoch: 780 \tTraining Loss: 0.035603\n",
      "Epoch: 781 \tTraining Loss: 0.032344\n",
      "Epoch: 782 \tTraining Loss: 0.032234\n",
      "Epoch: 783 \tTraining Loss: 0.031560\n",
      "Epoch: 784 \tTraining Loss: 0.031925\n",
      "Epoch: 785 \tTraining Loss: 0.032584\n",
      "Epoch: 786 \tTraining Loss: 0.032406\n",
      "Epoch: 787 \tTraining Loss: 0.032169\n",
      "Epoch: 788 \tTraining Loss: 0.032173\n",
      "Epoch: 789 \tTraining Loss: 0.032428\n",
      "Epoch: 790 \tTraining Loss: 0.032886\n",
      "Epoch: 791 \tTraining Loss: 0.033019\n",
      "Epoch: 792 \tTraining Loss: 0.031882\n",
      "Epoch: 793 \tTraining Loss: 0.032474\n",
      "Epoch: 794 \tTraining Loss: 0.032333\n",
      "Epoch: 795 \tTraining Loss: 0.034308\n",
      "Epoch: 796 \tTraining Loss: 0.032301\n",
      "Epoch: 797 \tTraining Loss: 0.032693\n",
      "Epoch: 798 \tTraining Loss: 0.030771\n",
      "Epoch: 799 \tTraining Loss: 0.032775\n",
      "Epoch: 800 \tTraining Loss: 0.033592\n",
      "Epoch: 801 \tTraining Loss: 0.031742\n",
      "Epoch: 802 \tTraining Loss: 0.033301\n",
      "Epoch: 803 \tTraining Loss: 0.036587\n",
      "Epoch: 804 \tTraining Loss: 0.031935\n",
      "Epoch: 805 \tTraining Loss: 0.031398\n",
      "Epoch: 806 \tTraining Loss: 0.032497\n",
      "Epoch: 807 \tTraining Loss: 0.036533\n",
      "Epoch: 808 \tTraining Loss: 0.032081\n",
      "Epoch: 809 \tTraining Loss: 0.031484\n",
      "Epoch: 810 \tTraining Loss: 0.031575\n",
      "Epoch: 811 \tTraining Loss: 0.031548\n",
      "Epoch: 812 \tTraining Loss: 0.031808\n",
      "Epoch: 813 \tTraining Loss: 0.031818\n",
      "Epoch: 814 \tTraining Loss: 0.031435\n",
      "Epoch: 815 \tTraining Loss: 0.031908\n",
      "Epoch: 816 \tTraining Loss: 0.032055\n",
      "Epoch: 817 \tTraining Loss: 0.031463\n",
      "Epoch: 818 \tTraining Loss: 0.032175\n",
      "Epoch: 819 \tTraining Loss: 0.034714\n",
      "Epoch: 820 \tTraining Loss: 0.035660\n",
      "Epoch: 821 \tTraining Loss: 0.033011\n",
      "Epoch: 822 \tTraining Loss: 0.034926\n",
      "Epoch: 823 \tTraining Loss: 0.033034\n",
      "Epoch: 824 \tTraining Loss: 0.032816\n",
      "Epoch: 825 \tTraining Loss: 0.031691\n",
      "Epoch: 826 \tTraining Loss: 0.032734\n",
      "Epoch: 827 \tTraining Loss: 0.031682\n",
      "Epoch: 828 \tTraining Loss: 0.031393\n",
      "Epoch: 829 \tTraining Loss: 0.031150\n",
      "Epoch: 830 \tTraining Loss: 0.032213\n",
      "Epoch: 831 \tTraining Loss: 0.031044\n",
      "Epoch: 832 \tTraining Loss: 0.030908\n",
      "Epoch: 833 \tTraining Loss: 0.031774\n",
      "Epoch: 834 \tTraining Loss: 0.031892\n",
      "Epoch: 835 \tTraining Loss: 0.036840\n",
      "Epoch: 836 \tTraining Loss: 0.050198\n",
      "Epoch: 837 \tTraining Loss: 0.060726\n",
      "Epoch: 838 \tTraining Loss: 0.161753\n",
      "Epoch: 839 \tTraining Loss: 0.208074\n",
      "Epoch: 840 \tTraining Loss: 0.460268\n",
      "Epoch: 841 \tTraining Loss: 0.159093\n",
      "Epoch: 842 \tTraining Loss: 0.083051\n",
      "Epoch: 843 \tTraining Loss: 0.067252\n",
      "Epoch: 844 \tTraining Loss: 0.063974\n",
      "Epoch: 845 \tTraining Loss: 0.060478\n",
      "Epoch: 846 \tTraining Loss: 0.057790\n",
      "Epoch: 847 \tTraining Loss: 0.058957\n",
      "Epoch: 848 \tTraining Loss: 0.053414\n",
      "Epoch: 849 \tTraining Loss: 0.051634\n",
      "Epoch: 850 \tTraining Loss: 0.055674\n",
      "Epoch: 851 \tTraining Loss: 0.051054\n",
      "Epoch: 852 \tTraining Loss: 0.050115\n",
      "Epoch: 853 \tTraining Loss: 0.049855\n",
      "Epoch: 854 \tTraining Loss: 0.049748\n",
      "Epoch: 855 \tTraining Loss: 0.048450\n",
      "Epoch: 856 \tTraining Loss: 0.048636\n",
      "Epoch: 857 \tTraining Loss: 0.048018\n",
      "Epoch: 858 \tTraining Loss: 0.048331\n",
      "Epoch: 859 \tTraining Loss: 0.047625\n",
      "Epoch: 860 \tTraining Loss: 0.046929\n",
      "Epoch: 861 \tTraining Loss: 0.045241\n",
      "Epoch: 862 \tTraining Loss: 0.045461\n",
      "Epoch: 863 \tTraining Loss: 0.045447\n",
      "Epoch: 864 \tTraining Loss: 0.044712\n",
      "Epoch: 865 \tTraining Loss: 0.044883\n",
      "Epoch: 866 \tTraining Loss: 0.044733\n",
      "Epoch: 867 \tTraining Loss: 0.044658\n",
      "Epoch: 868 \tTraining Loss: 0.045176\n",
      "Epoch: 869 \tTraining Loss: 0.043824\n",
      "Epoch: 870 \tTraining Loss: 0.042589\n",
      "Epoch: 871 \tTraining Loss: 0.044343\n",
      "Epoch: 872 \tTraining Loss: 0.043864\n",
      "Epoch: 873 \tTraining Loss: 0.043016\n",
      "Epoch: 874 \tTraining Loss: 0.042355\n",
      "Epoch: 875 \tTraining Loss: 0.042103\n",
      "Epoch: 876 \tTraining Loss: 0.044757\n",
      "Epoch: 877 \tTraining Loss: 0.045063\n",
      "Epoch: 878 \tTraining Loss: 0.042949\n",
      "Epoch: 879 \tTraining Loss: 0.043256\n",
      "Epoch: 880 \tTraining Loss: 0.042111\n",
      "Epoch: 881 \tTraining Loss: 0.041999\n",
      "Epoch: 882 \tTraining Loss: 0.042500\n",
      "Epoch: 883 \tTraining Loss: 0.040653\n",
      "Epoch: 884 \tTraining Loss: 0.040656\n",
      "Epoch: 885 \tTraining Loss: 0.040777\n",
      "Epoch: 886 \tTraining Loss: 0.040679\n",
      "Epoch: 887 \tTraining Loss: 0.040637\n",
      "Epoch: 888 \tTraining Loss: 0.040481\n",
      "Epoch: 889 \tTraining Loss: 0.040786\n",
      "Epoch: 890 \tTraining Loss: 0.040277\n",
      "Epoch: 891 \tTraining Loss: 0.041071\n",
      "Epoch: 892 \tTraining Loss: 0.040184\n",
      "Epoch: 893 \tTraining Loss: 0.040418\n",
      "Epoch: 894 \tTraining Loss: 0.040539\n",
      "Epoch: 895 \tTraining Loss: 0.040285\n",
      "Epoch: 896 \tTraining Loss: 0.041114\n",
      "Epoch: 897 \tTraining Loss: 0.040336\n",
      "Epoch: 898 \tTraining Loss: 0.040380\n",
      "Epoch: 899 \tTraining Loss: 0.040002\n",
      "Epoch: 900 \tTraining Loss: 0.040075\n",
      "Epoch: 901 \tTraining Loss: 0.040335\n",
      "Epoch: 902 \tTraining Loss: 0.040871\n",
      "Epoch: 903 \tTraining Loss: 0.039915\n",
      "Epoch: 904 \tTraining Loss: 0.041968\n",
      "Epoch: 905 \tTraining Loss: 0.040432\n",
      "Epoch: 906 \tTraining Loss: 0.040857\n",
      "Epoch: 907 \tTraining Loss: 0.039951\n",
      "Epoch: 908 \tTraining Loss: 0.039955\n",
      "Epoch: 909 \tTraining Loss: 0.040090\n",
      "Epoch: 910 \tTraining Loss: 0.040744\n",
      "Epoch: 911 \tTraining Loss: 0.040754\n",
      "Epoch: 912 \tTraining Loss: 0.040656\n",
      "Epoch: 913 \tTraining Loss: 0.041116\n",
      "Epoch: 914 \tTraining Loss: 0.040067\n",
      "Epoch: 915 \tTraining Loss: 0.040357\n",
      "Epoch: 916 \tTraining Loss: 0.040929\n",
      "Epoch: 917 \tTraining Loss: 0.042907\n",
      "Epoch: 918 \tTraining Loss: 0.041268\n",
      "Epoch: 919 \tTraining Loss: 0.040239\n",
      "Epoch: 920 \tTraining Loss: 0.039943\n",
      "Epoch: 921 \tTraining Loss: 0.040114\n",
      "Epoch: 922 \tTraining Loss: 0.045426\n",
      "Epoch: 923 \tTraining Loss: 0.039894\n",
      "Epoch: 924 \tTraining Loss: 0.040555\n",
      "Epoch: 925 \tTraining Loss: 0.041595\n",
      "Epoch: 926 \tTraining Loss: 0.041890\n",
      "Epoch: 927 \tTraining Loss: 0.044231\n",
      "Epoch: 928 \tTraining Loss: 0.052349\n",
      "Epoch: 929 \tTraining Loss: 0.052036\n",
      "Epoch: 930 \tTraining Loss: 0.049721\n",
      "Epoch: 931 \tTraining Loss: 0.050997\n",
      "Epoch: 932 \tTraining Loss: 0.046830\n",
      "Epoch: 933 \tTraining Loss: 0.068753\n",
      "Epoch: 934 \tTraining Loss: 0.174584\n",
      "Epoch: 935 \tTraining Loss: 0.115275\n",
      "Epoch: 936 \tTraining Loss: 0.058487\n",
      "Epoch: 937 \tTraining Loss: 0.058231\n",
      "Epoch: 938 \tTraining Loss: 0.090416\n",
      "Epoch: 939 \tTraining Loss: 0.132661\n",
      "Epoch: 940 \tTraining Loss: 0.066420\n",
      "Epoch: 941 \tTraining Loss: 0.070361\n",
      "Epoch: 942 \tTraining Loss: 0.059230\n",
      "Epoch: 943 \tTraining Loss: 0.052139\n",
      "Epoch: 944 \tTraining Loss: 0.048254\n",
      "Epoch: 945 \tTraining Loss: 0.048125\n",
      "Epoch: 946 \tTraining Loss: 0.043728\n",
      "Epoch: 947 \tTraining Loss: 0.045774\n",
      "Epoch: 948 \tTraining Loss: 0.040449\n",
      "Epoch: 949 \tTraining Loss: 0.037925\n",
      "Epoch: 950 \tTraining Loss: 0.038650\n",
      "Epoch: 951 \tTraining Loss: 0.036834\n",
      "Epoch: 952 \tTraining Loss: 0.036728\n",
      "Epoch: 953 \tTraining Loss: 0.036089\n",
      "Epoch: 954 \tTraining Loss: 0.036125\n",
      "Epoch: 955 \tTraining Loss: 0.035945\n",
      "Epoch: 956 \tTraining Loss: 0.036393\n",
      "Epoch: 957 \tTraining Loss: 0.036563\n",
      "Epoch: 958 \tTraining Loss: 0.035760\n",
      "Epoch: 959 \tTraining Loss: 0.048321\n",
      "Epoch: 960 \tTraining Loss: 0.046820\n",
      "Epoch: 961 \tTraining Loss: 0.043925\n",
      "Epoch: 962 \tTraining Loss: 0.043019\n",
      "Epoch: 963 \tTraining Loss: 0.042830\n",
      "Epoch: 964 \tTraining Loss: 0.042549\n",
      "Epoch: 965 \tTraining Loss: 0.042826\n",
      "Epoch: 966 \tTraining Loss: 0.041926\n",
      "Epoch: 967 \tTraining Loss: 0.040593\n",
      "Epoch: 968 \tTraining Loss: 0.039501\n",
      "Epoch: 969 \tTraining Loss: 0.040906\n",
      "Epoch: 970 \tTraining Loss: 0.051871\n",
      "Epoch: 971 \tTraining Loss: 0.047523\n",
      "Epoch: 972 \tTraining Loss: 0.042677\n",
      "Epoch: 973 \tTraining Loss: 0.038489\n",
      "Epoch: 974 \tTraining Loss: 0.038056\n",
      "Epoch: 975 \tTraining Loss: 0.038082\n",
      "Epoch: 976 \tTraining Loss: 0.038709\n",
      "Epoch: 977 \tTraining Loss: 0.038387\n",
      "Epoch: 978 \tTraining Loss: 0.037928\n",
      "Epoch: 979 \tTraining Loss: 0.038156\n",
      "Epoch: 980 \tTraining Loss: 0.038628\n",
      "Epoch: 981 \tTraining Loss: 0.038756\n",
      "Epoch: 982 \tTraining Loss: 0.037446\n",
      "Epoch: 983 \tTraining Loss: 0.037802\n",
      "Epoch: 984 \tTraining Loss: 0.037299\n",
      "Epoch: 985 \tTraining Loss: 0.037610\n",
      "Epoch: 986 \tTraining Loss: 0.037767\n",
      "Epoch: 987 \tTraining Loss: 0.037520\n",
      "Epoch: 988 \tTraining Loss: 0.037918\n",
      "Epoch: 989 \tTraining Loss: 0.038785\n",
      "Epoch: 990 \tTraining Loss: 0.037280\n",
      "Epoch: 991 \tTraining Loss: 0.037310\n",
      "Epoch: 992 \tTraining Loss: 0.039003\n",
      "Epoch: 993 \tTraining Loss: 0.037745\n",
      "Epoch: 994 \tTraining Loss: 0.036942\n",
      "Epoch: 995 \tTraining Loss: 0.037711\n",
      "Epoch: 996 \tTraining Loss: 0.037399\n",
      "Epoch: 997 \tTraining Loss: 0.037554\n",
      "Epoch: 998 \tTraining Loss: 0.037303\n",
      "Epoch: 999 \tTraining Loss: 0.037612\n",
      "Epoch: 1000 \tTraining Loss: 0.037361\n",
      "Epoch: 1001 \tTraining Loss: 0.036974\n",
      "Epoch: 1002 \tTraining Loss: 0.038094\n",
      "Epoch: 1003 \tTraining Loss: 0.036712\n",
      "Epoch: 1004 \tTraining Loss: 0.037654\n",
      "Epoch: 1005 \tTraining Loss: 0.040462\n",
      "Epoch: 1006 \tTraining Loss: 0.037479\n",
      "Epoch: 1007 \tTraining Loss: 0.042457\n",
      "Epoch: 1008 \tTraining Loss: 0.049211\n",
      "Epoch: 1009 \tTraining Loss: 0.052010\n",
      "Epoch: 1010 \tTraining Loss: 0.041867\n",
      "Epoch: 1011 \tTraining Loss: 0.046057\n",
      "Epoch: 1012 \tTraining Loss: 0.044783\n",
      "Epoch: 1013 \tTraining Loss: 0.070090\n",
      "Epoch: 1014 \tTraining Loss: 0.081736\n",
      "Epoch: 1015 \tTraining Loss: 0.078468\n",
      "Epoch: 1016 \tTraining Loss: 0.111542\n",
      "Epoch: 1017 \tTraining Loss: 0.165282\n",
      "Epoch: 1018 \tTraining Loss: 0.123769\n",
      "Epoch: 1019 \tTraining Loss: 0.104796\n",
      "Epoch: 1020 \tTraining Loss: 0.066516\n",
      "Epoch: 1021 \tTraining Loss: 0.049875\n",
      "Epoch: 1022 \tTraining Loss: 0.048409\n",
      "Epoch: 1023 \tTraining Loss: 0.043427\n",
      "Epoch: 1024 \tTraining Loss: 0.043982\n",
      "Epoch: 1025 \tTraining Loss: 0.040384\n",
      "Epoch: 1026 \tTraining Loss: 0.041243\n",
      "Epoch: 1027 \tTraining Loss: 0.040915\n",
      "Epoch: 1028 \tTraining Loss: 0.041631\n",
      "Epoch: 1029 \tTraining Loss: 0.040313\n",
      "Epoch: 1030 \tTraining Loss: 0.039684\n",
      "Epoch: 1031 \tTraining Loss: 0.039838\n",
      "Epoch: 1032 \tTraining Loss: 0.039960\n",
      "Epoch: 1033 \tTraining Loss: 0.038406\n",
      "Epoch: 1034 \tTraining Loss: 0.042013\n",
      "Epoch: 1035 \tTraining Loss: 0.043437\n",
      "Epoch: 1036 \tTraining Loss: 0.042348\n",
      "Epoch: 1037 \tTraining Loss: 0.038516\n",
      "Epoch: 1038 \tTraining Loss: 0.042442\n",
      "Epoch: 1039 \tTraining Loss: 0.044192\n",
      "Epoch: 1040 \tTraining Loss: 0.038505\n",
      "Epoch: 1041 \tTraining Loss: 0.037987\n",
      "Epoch: 1042 \tTraining Loss: 0.039965\n",
      "Epoch: 1043 \tTraining Loss: 0.036763\n",
      "Epoch: 1044 \tTraining Loss: 0.038453\n",
      "Epoch: 1045 \tTraining Loss: 0.037313\n",
      "Epoch: 1046 \tTraining Loss: 0.036979\n",
      "Epoch: 1047 \tTraining Loss: 0.038447\n",
      "Epoch: 1048 \tTraining Loss: 0.039112\n",
      "Epoch: 1049 \tTraining Loss: 0.036502\n",
      "Epoch: 1050 \tTraining Loss: 0.037495\n",
      "Epoch: 1051 \tTraining Loss: 0.036978\n",
      "Epoch: 1052 \tTraining Loss: 0.036896\n",
      "Epoch: 1053 \tTraining Loss: 0.036904\n",
      "Epoch: 1054 \tTraining Loss: 0.036589\n",
      "Epoch: 1055 \tTraining Loss: 0.036858\n",
      "Epoch: 1056 \tTraining Loss: 0.037216\n",
      "Epoch: 1057 \tTraining Loss: 0.038662\n",
      "Epoch: 1058 \tTraining Loss: 0.041682\n",
      "Epoch: 1059 \tTraining Loss: 0.038985\n",
      "Epoch: 1060 \tTraining Loss: 0.038376\n",
      "Epoch: 1061 \tTraining Loss: 0.034587\n",
      "Epoch: 1062 \tTraining Loss: 0.034760\n",
      "Epoch: 1063 \tTraining Loss: 0.033305\n",
      "Epoch: 1064 \tTraining Loss: 0.033936\n",
      "Epoch: 1065 \tTraining Loss: 0.033996\n",
      "Epoch: 1066 \tTraining Loss: 0.034495\n",
      "Epoch: 1067 \tTraining Loss: 0.034433\n",
      "Epoch: 1068 \tTraining Loss: 0.033366\n",
      "Epoch: 1069 \tTraining Loss: 0.033683\n",
      "Epoch: 1070 \tTraining Loss: 0.034355\n",
      "Epoch: 1071 \tTraining Loss: 0.037329\n",
      "Epoch: 1072 \tTraining Loss: 0.033934\n",
      "Epoch: 1073 \tTraining Loss: 0.035139\n",
      "Epoch: 1074 \tTraining Loss: 0.033763\n",
      "Epoch: 1075 \tTraining Loss: 0.033253\n",
      "Epoch: 1076 \tTraining Loss: 0.036118\n",
      "Epoch: 1077 \tTraining Loss: 0.036372\n",
      "Epoch: 1078 \tTraining Loss: 0.035138\n",
      "Epoch: 1079 \tTraining Loss: 0.035093\n",
      "Epoch: 1080 \tTraining Loss: 0.036788\n",
      "Epoch: 1081 \tTraining Loss: 0.037687\n",
      "Epoch: 1082 \tTraining Loss: 0.034688\n",
      "Epoch: 1083 \tTraining Loss: 0.036825\n",
      "Epoch: 1084 \tTraining Loss: 0.034632\n",
      "Epoch: 1085 \tTraining Loss: 0.036515\n",
      "Epoch: 1086 \tTraining Loss: 0.036741\n",
      "Epoch: 1087 \tTraining Loss: 0.036075\n",
      "Epoch: 1088 \tTraining Loss: 0.036554\n",
      "Epoch: 1089 \tTraining Loss: 0.035267\n",
      "Epoch: 1090 \tTraining Loss: 0.042930\n",
      "Epoch: 1091 \tTraining Loss: 0.048406\n",
      "Epoch: 1092 \tTraining Loss: 0.067957\n",
      "Epoch: 1093 \tTraining Loss: 0.228340\n",
      "Epoch: 1094 \tTraining Loss: 0.329866\n",
      "Epoch: 1095 \tTraining Loss: 0.195283\n",
      "Epoch: 1096 \tTraining Loss: 0.176504\n",
      "Epoch: 1097 \tTraining Loss: 0.230877\n",
      "Epoch: 1098 \tTraining Loss: 0.134473\n",
      "Epoch: 1099 \tTraining Loss: 0.067415\n",
      "Epoch: 1100 \tTraining Loss: 0.057719\n",
      "Epoch: 1101 \tTraining Loss: 0.047003\n",
      "Epoch: 1102 \tTraining Loss: 0.043452\n",
      "Epoch: 1103 \tTraining Loss: 0.041101\n",
      "Epoch: 1104 \tTraining Loss: 0.041106\n",
      "Epoch: 1105 \tTraining Loss: 0.041453\n",
      "Epoch: 1106 \tTraining Loss: 0.042863\n",
      "Epoch: 1107 \tTraining Loss: 0.042387\n",
      "Epoch: 1108 \tTraining Loss: 0.042441\n",
      "Epoch: 1109 \tTraining Loss: 0.039471\n",
      "Epoch: 1110 \tTraining Loss: 0.039755\n",
      "Epoch: 1111 \tTraining Loss: 0.040045\n",
      "Epoch: 1112 \tTraining Loss: 0.040313\n",
      "Epoch: 1113 \tTraining Loss: 0.042157\n",
      "Epoch: 1114 \tTraining Loss: 0.039102\n",
      "Epoch: 1115 \tTraining Loss: 0.040623\n",
      "Epoch: 1116 \tTraining Loss: 0.039691\n",
      "Epoch: 1117 \tTraining Loss: 0.040012\n",
      "Epoch: 1118 \tTraining Loss: 0.039176\n",
      "Epoch: 1119 \tTraining Loss: 0.039132\n",
      "Epoch: 1120 \tTraining Loss: 0.040551\n",
      "Epoch: 1121 \tTraining Loss: 0.039277\n",
      "Epoch: 1122 \tTraining Loss: 0.038437\n",
      "Epoch: 1123 \tTraining Loss: 0.038514\n",
      "Epoch: 1124 \tTraining Loss: 0.038369\n",
      "Epoch: 1125 \tTraining Loss: 0.037212\n",
      "Epoch: 1126 \tTraining Loss: 0.038413\n",
      "Epoch: 1127 \tTraining Loss: 0.037670\n",
      "Epoch: 1128 \tTraining Loss: 0.037057\n",
      "Epoch: 1129 \tTraining Loss: 0.036681\n",
      "Epoch: 1130 \tTraining Loss: 0.037942\n",
      "Epoch: 1131 \tTraining Loss: 0.036855\n",
      "Epoch: 1132 \tTraining Loss: 0.037626\n",
      "Epoch: 1133 \tTraining Loss: 0.039013\n",
      "Epoch: 1134 \tTraining Loss: 0.037547\n",
      "Epoch: 1135 \tTraining Loss: 0.037107\n",
      "Epoch: 1136 \tTraining Loss: 0.037584\n",
      "Epoch: 1137 \tTraining Loss: 0.037324\n",
      "Epoch: 1138 \tTraining Loss: 0.037665\n",
      "Epoch: 1139 \tTraining Loss: 0.037625\n",
      "Epoch: 1140 \tTraining Loss: 0.037444\n",
      "Epoch: 1141 \tTraining Loss: 0.037283\n",
      "Epoch: 1142 \tTraining Loss: 0.037684\n",
      "Epoch: 1143 \tTraining Loss: 0.037295\n",
      "Epoch: 1144 \tTraining Loss: 0.037718\n",
      "Epoch: 1145 \tTraining Loss: 0.037671\n",
      "Epoch: 1146 \tTraining Loss: 0.037717\n",
      "Epoch: 1147 \tTraining Loss: 0.037997\n",
      "Epoch: 1148 \tTraining Loss: 0.036744\n",
      "Epoch: 1149 \tTraining Loss: 0.036640\n",
      "Epoch: 1150 \tTraining Loss: 0.037584\n",
      "Epoch: 1151 \tTraining Loss: 0.036474\n",
      "Epoch: 1152 \tTraining Loss: 0.037495\n",
      "Epoch: 1153 \tTraining Loss: 0.036648\n",
      "Epoch: 1154 \tTraining Loss: 0.036950\n",
      "Epoch: 1155 \tTraining Loss: 0.037327\n",
      "Epoch: 1156 \tTraining Loss: 0.036805\n",
      "Epoch: 1157 \tTraining Loss: 0.046520\n",
      "Epoch: 1158 \tTraining Loss: 0.064066\n",
      "Epoch: 1159 \tTraining Loss: 0.061767\n",
      "Epoch: 1160 \tTraining Loss: 0.066228\n",
      "Epoch: 1161 \tTraining Loss: 0.052865\n",
      "Epoch: 1162 \tTraining Loss: 0.069806\n",
      "Epoch: 1163 \tTraining Loss: 0.055280\n",
      "Epoch: 1164 \tTraining Loss: 0.067127\n",
      "Epoch: 1165 \tTraining Loss: 0.049009\n",
      "Epoch: 1166 \tTraining Loss: 0.050446\n",
      "Epoch: 1167 \tTraining Loss: 0.043417\n",
      "Epoch: 1168 \tTraining Loss: 0.041364\n",
      "Epoch: 1169 \tTraining Loss: 0.041750\n",
      "Epoch: 1170 \tTraining Loss: 0.040666\n",
      "Epoch: 1171 \tTraining Loss: 0.040049\n",
      "Epoch: 1172 \tTraining Loss: 0.040086\n",
      "Epoch: 1173 \tTraining Loss: 0.039482\n",
      "Epoch: 1174 \tTraining Loss: 0.040958\n",
      "Epoch: 1175 \tTraining Loss: 0.041820\n",
      "Epoch: 1176 \tTraining Loss: 0.038843\n",
      "Epoch: 1177 \tTraining Loss: 0.039365\n",
      "Epoch: 1178 \tTraining Loss: 0.038779\n",
      "Epoch: 1179 \tTraining Loss: 0.039518\n",
      "Epoch: 1180 \tTraining Loss: 0.038218\n",
      "Epoch: 1181 \tTraining Loss: 0.040025\n",
      "Epoch: 1182 \tTraining Loss: 0.040122\n",
      "Epoch: 1183 \tTraining Loss: 0.038493\n",
      "Epoch: 1184 \tTraining Loss: 0.039824\n",
      "Epoch: 1185 \tTraining Loss: 0.041273\n",
      "Epoch: 1186 \tTraining Loss: 0.040376\n",
      "Epoch: 1187 \tTraining Loss: 0.042069\n",
      "Epoch: 1188 \tTraining Loss: 0.037863\n",
      "Epoch: 1189 \tTraining Loss: 0.039262\n",
      "Epoch: 1190 \tTraining Loss: 0.038681\n",
      "Epoch: 1191 \tTraining Loss: 0.038174\n",
      "Epoch: 1192 \tTraining Loss: 0.038816\n",
      "Epoch: 1193 \tTraining Loss: 0.037434\n",
      "Epoch: 1194 \tTraining Loss: 0.038163\n",
      "Epoch: 1195 \tTraining Loss: 0.039609\n",
      "Epoch: 1196 \tTraining Loss: 0.039311\n",
      "Epoch: 1197 \tTraining Loss: 0.038361\n",
      "Epoch: 1198 \tTraining Loss: 0.038147\n",
      "Epoch: 1199 \tTraining Loss: 0.038632\n",
      "Epoch: 1200 \tTraining Loss: 0.038243\n",
      "Epoch: 1201 \tTraining Loss: 0.037389\n",
      "Epoch: 1202 \tTraining Loss: 0.038022\n",
      "Epoch: 1203 \tTraining Loss: 0.038410\n",
      "Epoch: 1204 \tTraining Loss: 0.037270\n",
      "Epoch: 1205 \tTraining Loss: 0.037942\n",
      "Epoch: 1206 \tTraining Loss: 0.037451\n",
      "Epoch: 1207 \tTraining Loss: 0.037548\n",
      "Epoch: 1208 \tTraining Loss: 0.038138\n",
      "Epoch: 1209 \tTraining Loss: 0.037260\n",
      "Epoch: 1210 \tTraining Loss: 0.038138\n",
      "Epoch: 1211 \tTraining Loss: 0.037919\n",
      "Epoch: 1212 \tTraining Loss: 0.038054\n",
      "Epoch: 1213 \tTraining Loss: 0.038429\n",
      "Epoch: 1214 \tTraining Loss: 0.038401\n",
      "Epoch: 1215 \tTraining Loss: 0.038822\n",
      "Epoch: 1216 \tTraining Loss: 0.037045\n",
      "Epoch: 1217 \tTraining Loss: 0.037838\n",
      "Epoch: 1218 \tTraining Loss: 0.037042\n",
      "Epoch: 1219 \tTraining Loss: 0.038078\n",
      "Epoch: 1220 \tTraining Loss: 0.037364\n",
      "Epoch: 1221 \tTraining Loss: 0.037513\n",
      "Epoch: 1222 \tTraining Loss: 0.037660\n",
      "Epoch: 1223 \tTraining Loss: 0.037869\n",
      "Epoch: 1224 \tTraining Loss: 0.038356\n",
      "Epoch: 1225 \tTraining Loss: 0.037967\n",
      "Epoch: 1226 \tTraining Loss: 0.037373\n",
      "Epoch: 1227 \tTraining Loss: 0.037863\n",
      "Epoch: 1228 \tTraining Loss: 0.037740\n",
      "Epoch: 1229 \tTraining Loss: 0.039646\n",
      "Epoch: 1230 \tTraining Loss: 0.041635\n",
      "Epoch: 1231 \tTraining Loss: 0.046855\n",
      "Epoch: 1232 \tTraining Loss: 0.077532\n",
      "Epoch: 1233 \tTraining Loss: 0.070506\n",
      "Epoch: 1234 \tTraining Loss: 0.085025\n",
      "Epoch: 1235 \tTraining Loss: 0.113610\n",
      "Epoch: 1236 \tTraining Loss: 0.136438\n",
      "Epoch: 1237 \tTraining Loss: 0.336843\n",
      "Epoch: 1238 \tTraining Loss: 0.165505\n",
      "Epoch: 1239 \tTraining Loss: 0.145720\n",
      "Epoch: 1240 \tTraining Loss: 0.085971\n",
      "Epoch: 1241 \tTraining Loss: 0.077498\n",
      "Epoch: 1242 \tTraining Loss: 0.072950\n",
      "Epoch: 1243 \tTraining Loss: 0.065230\n",
      "Epoch: 1244 \tTraining Loss: 0.065469\n",
      "Epoch: 1245 \tTraining Loss: 0.063434\n",
      "Epoch: 1246 \tTraining Loss: 0.061429\n",
      "Epoch: 1247 \tTraining Loss: 0.058950\n",
      "Epoch: 1248 \tTraining Loss: 0.058266\n",
      "Epoch: 1249 \tTraining Loss: 0.056216\n",
      "Epoch: 1250 \tTraining Loss: 0.053981\n",
      "Epoch: 1251 \tTraining Loss: 0.053493\n",
      "Epoch: 1252 \tTraining Loss: 0.050322\n",
      "Epoch: 1253 \tTraining Loss: 0.049923\n",
      "Epoch: 1254 \tTraining Loss: 0.052215\n",
      "Epoch: 1255 \tTraining Loss: 0.048936\n",
      "Epoch: 1256 \tTraining Loss: 0.046339\n",
      "Epoch: 1257 \tTraining Loss: 0.045106\n",
      "Epoch: 1258 \tTraining Loss: 0.044132\n",
      "Epoch: 1259 \tTraining Loss: 0.044564\n",
      "Epoch: 1260 \tTraining Loss: 0.044305\n",
      "Epoch: 1261 \tTraining Loss: 0.043760\n",
      "Epoch: 1262 \tTraining Loss: 0.046313\n",
      "Epoch: 1263 \tTraining Loss: 0.045896\n",
      "Epoch: 1264 \tTraining Loss: 0.045715\n",
      "Epoch: 1265 \tTraining Loss: 0.042982\n",
      "Epoch: 1266 \tTraining Loss: 0.043839\n",
      "Epoch: 1267 \tTraining Loss: 0.044580\n",
      "Epoch: 1268 \tTraining Loss: 0.043066\n",
      "Epoch: 1269 \tTraining Loss: 0.044532\n",
      "Epoch: 1270 \tTraining Loss: 0.044082\n",
      "Epoch: 1271 \tTraining Loss: 0.043441\n",
      "Epoch: 1272 \tTraining Loss: 0.043542\n",
      "Epoch: 1273 \tTraining Loss: 0.044304\n",
      "Epoch: 1274 \tTraining Loss: 0.044169\n",
      "Epoch: 1275 \tTraining Loss: 0.043815\n",
      "Epoch: 1276 \tTraining Loss: 0.043915\n",
      "Epoch: 1277 \tTraining Loss: 0.044253\n",
      "Epoch: 1278 \tTraining Loss: 0.043408\n",
      "Epoch: 1279 \tTraining Loss: 0.043216\n",
      "Epoch: 1280 \tTraining Loss: 0.043495\n",
      "Epoch: 1281 \tTraining Loss: 0.043564\n",
      "Epoch: 1282 \tTraining Loss: 0.042268\n",
      "Epoch: 1283 \tTraining Loss: 0.043803\n",
      "Epoch: 1284 \tTraining Loss: 0.044761\n",
      "Epoch: 1285 \tTraining Loss: 0.044927\n",
      "Epoch: 1286 \tTraining Loss: 0.045942\n",
      "Epoch: 1287 \tTraining Loss: 0.046410\n",
      "Epoch: 1288 \tTraining Loss: 0.053122\n",
      "Epoch: 1289 \tTraining Loss: 0.064847\n",
      "Epoch: 1290 \tTraining Loss: 0.091853\n",
      "Epoch: 1291 \tTraining Loss: 0.105465\n",
      "Epoch: 1292 \tTraining Loss: 0.068676\n",
      "Epoch: 1293 \tTraining Loss: 0.050362\n",
      "Epoch: 1294 \tTraining Loss: 0.050031\n",
      "Epoch: 1295 \tTraining Loss: 0.049547\n",
      "Epoch: 1296 \tTraining Loss: 0.042280\n",
      "Epoch: 1297 \tTraining Loss: 0.041464\n",
      "Epoch: 1298 \tTraining Loss: 0.043218\n",
      "Epoch: 1299 \tTraining Loss: 0.041299\n",
      "Epoch: 1300 \tTraining Loss: 0.041403\n",
      "Epoch: 1301 \tTraining Loss: 0.040665\n",
      "Epoch: 1302 \tTraining Loss: 0.040541\n",
      "Epoch: 1303 \tTraining Loss: 0.039801\n",
      "Epoch: 1304 \tTraining Loss: 0.040422\n",
      "Epoch: 1305 \tTraining Loss: 0.041084\n",
      "Epoch: 1306 \tTraining Loss: 0.040100\n",
      "Epoch: 1307 \tTraining Loss: 0.040095\n",
      "Epoch: 1308 \tTraining Loss: 0.039196\n",
      "Epoch: 1309 \tTraining Loss: 0.039659\n",
      "Epoch: 1310 \tTraining Loss: 0.045312\n",
      "Epoch: 1311 \tTraining Loss: 0.042574\n",
      "Epoch: 1312 \tTraining Loss: 0.039671\n",
      "Epoch: 1313 \tTraining Loss: 0.038858\n",
      "Epoch: 1314 \tTraining Loss: 0.038849\n",
      "Epoch: 1315 \tTraining Loss: 0.038541\n",
      "Epoch: 1316 \tTraining Loss: 0.037890\n",
      "Epoch: 1317 \tTraining Loss: 0.039434\n",
      "Epoch: 1318 \tTraining Loss: 0.039668\n",
      "Epoch: 1319 \tTraining Loss: 0.037970\n",
      "Epoch: 1320 \tTraining Loss: 0.038698\n",
      "Epoch: 1321 \tTraining Loss: 0.037844\n",
      "Epoch: 1322 \tTraining Loss: 0.040112\n",
      "Epoch: 1323 \tTraining Loss: 0.037530\n",
      "Epoch: 1324 \tTraining Loss: 0.038227\n",
      "Epoch: 1325 \tTraining Loss: 0.038660\n",
      "Epoch: 1326 \tTraining Loss: 0.037613\n",
      "Epoch: 1327 \tTraining Loss: 0.037115\n",
      "Epoch: 1328 \tTraining Loss: 0.037112\n",
      "Epoch: 1329 \tTraining Loss: 0.037636\n",
      "Epoch: 1330 \tTraining Loss: 0.039772\n",
      "Epoch: 1331 \tTraining Loss: 0.041331\n",
      "Epoch: 1332 \tTraining Loss: 0.037234\n",
      "Epoch: 1333 \tTraining Loss: 0.038130\n",
      "Epoch: 1334 \tTraining Loss: 0.037671\n",
      "Epoch: 1335 \tTraining Loss: 0.038008\n",
      "Epoch: 1336 \tTraining Loss: 0.038279\n",
      "Epoch: 1337 \tTraining Loss: 0.039054\n",
      "Epoch: 1338 \tTraining Loss: 0.039683\n",
      "Epoch: 1339 \tTraining Loss: 0.036940\n",
      "Epoch: 1340 \tTraining Loss: 0.037694\n",
      "Epoch: 1341 \tTraining Loss: 0.037356\n",
      "Epoch: 1342 \tTraining Loss: 0.037520\n",
      "Epoch: 1343 \tTraining Loss: 0.037173\n",
      "Epoch: 1344 \tTraining Loss: 0.036514\n",
      "Epoch: 1345 \tTraining Loss: 0.038373\n",
      "Epoch: 1346 \tTraining Loss: 0.037528\n",
      "Epoch: 1347 \tTraining Loss: 0.037870\n",
      "Epoch: 1348 \tTraining Loss: 0.039830\n",
      "Epoch: 1349 \tTraining Loss: 0.047914\n",
      "Epoch: 1350 \tTraining Loss: 0.046093\n",
      "Epoch: 1351 \tTraining Loss: 0.054743\n",
      "Epoch: 1352 \tTraining Loss: 0.066551\n",
      "Epoch: 1353 \tTraining Loss: 0.094885\n",
      "Epoch: 1354 \tTraining Loss: 0.167705\n",
      "Epoch: 1355 \tTraining Loss: 0.184550\n",
      "Epoch: 1356 \tTraining Loss: 0.124169\n",
      "Epoch: 1357 \tTraining Loss: 0.075562\n",
      "Epoch: 1358 \tTraining Loss: 0.064532\n",
      "Epoch: 1359 \tTraining Loss: 0.067482\n",
      "Epoch: 1360 \tTraining Loss: 0.048327\n",
      "Epoch: 1361 \tTraining Loss: 0.044138\n",
      "Epoch: 1362 \tTraining Loss: 0.044838\n",
      "Epoch: 1363 \tTraining Loss: 0.043166\n",
      "Epoch: 1364 \tTraining Loss: 0.040891\n",
      "Epoch: 1365 \tTraining Loss: 0.045501\n",
      "Epoch: 1366 \tTraining Loss: 0.049364\n",
      "Epoch: 1367 \tTraining Loss: 0.045384\n",
      "Epoch: 1368 \tTraining Loss: 0.043404\n",
      "Epoch: 1369 \tTraining Loss: 0.042270\n",
      "Epoch: 1370 \tTraining Loss: 0.042927\n",
      "Epoch: 1371 \tTraining Loss: 0.048817\n",
      "Epoch: 1372 \tTraining Loss: 0.049854\n",
      "Epoch: 1373 \tTraining Loss: 0.048527\n",
      "Epoch: 1374 \tTraining Loss: 0.081231\n",
      "Epoch: 1375 \tTraining Loss: 0.077280\n",
      "Epoch: 1376 \tTraining Loss: 0.048482\n",
      "Epoch: 1377 \tTraining Loss: 0.048462\n",
      "Epoch: 1378 \tTraining Loss: 0.042072\n",
      "Epoch: 1379 \tTraining Loss: 0.044528\n",
      "Epoch: 1380 \tTraining Loss: 0.043414\n",
      "Epoch: 1381 \tTraining Loss: 0.041290\n",
      "Epoch: 1382 \tTraining Loss: 0.040364\n",
      "Epoch: 1383 \tTraining Loss: 0.040727\n",
      "Epoch: 1384 \tTraining Loss: 0.041280\n",
      "Epoch: 1385 \tTraining Loss: 0.041329\n",
      "Epoch: 1386 \tTraining Loss: 0.041356\n",
      "Epoch: 1387 \tTraining Loss: 0.041706\n",
      "Epoch: 1388 \tTraining Loss: 0.042507\n",
      "Epoch: 1389 \tTraining Loss: 0.040584\n",
      "Epoch: 1390 \tTraining Loss: 0.042444\n",
      "Epoch: 1391 \tTraining Loss: 0.040785\n",
      "Epoch: 1392 \tTraining Loss: 0.042041\n",
      "Epoch: 1393 \tTraining Loss: 0.040554\n",
      "Epoch: 1394 \tTraining Loss: 0.041890\n",
      "Epoch: 1395 \tTraining Loss: 0.042141\n",
      "Epoch: 1396 \tTraining Loss: 0.042698\n",
      "Epoch: 1397 \tTraining Loss: 0.041987\n",
      "Epoch: 1398 \tTraining Loss: 0.043308\n",
      "Epoch: 1399 \tTraining Loss: 0.040458\n",
      "Epoch: 1400 \tTraining Loss: 0.041713\n",
      "Epoch: 1401 \tTraining Loss: 0.041642\n",
      "Epoch: 1402 \tTraining Loss: 0.040031\n",
      "Epoch: 1403 \tTraining Loss: 0.041484\n",
      "Epoch: 1404 \tTraining Loss: 0.042306\n",
      "Epoch: 1405 \tTraining Loss: 0.041657\n",
      "Epoch: 1406 \tTraining Loss: 0.040256\n",
      "Epoch: 1407 \tTraining Loss: 0.040599\n",
      "Epoch: 1408 \tTraining Loss: 0.040132\n",
      "Epoch: 1409 \tTraining Loss: 0.040725\n",
      "Epoch: 1410 \tTraining Loss: 0.040440\n",
      "Epoch: 1411 \tTraining Loss: 0.042487\n",
      "Epoch: 1412 \tTraining Loss: 0.040252\n",
      "Epoch: 1413 \tTraining Loss: 0.040677\n",
      "Epoch: 1414 \tTraining Loss: 0.040779\n",
      "Epoch: 1415 \tTraining Loss: 0.041416\n",
      "Epoch: 1416 \tTraining Loss: 0.040573\n",
      "Epoch: 1417 \tTraining Loss: 0.040967\n",
      "Epoch: 1418 \tTraining Loss: 0.040244\n",
      "Epoch: 1419 \tTraining Loss: 0.040850\n",
      "Epoch: 1420 \tTraining Loss: 0.040343\n",
      "Epoch: 1421 \tTraining Loss: 0.041160\n",
      "Epoch: 1422 \tTraining Loss: 0.040752\n",
      "Epoch: 1423 \tTraining Loss: 0.040386\n",
      "Epoch: 1424 \tTraining Loss: 0.040261\n",
      "Epoch: 1425 \tTraining Loss: 0.040203\n",
      "Epoch: 1426 \tTraining Loss: 0.040548\n",
      "Epoch: 1427 \tTraining Loss: 0.041804\n",
      "Epoch: 1428 \tTraining Loss: 0.039812\n",
      "Epoch: 1429 \tTraining Loss: 0.039972\n",
      "Epoch: 1430 \tTraining Loss: 0.040619\n",
      "Epoch: 1431 \tTraining Loss: 0.040089\n",
      "Epoch: 1432 \tTraining Loss: 0.040047\n",
      "Epoch: 1433 \tTraining Loss: 0.040413\n",
      "Epoch: 1434 \tTraining Loss: 0.040689\n",
      "Epoch: 1435 \tTraining Loss: 0.040338\n",
      "Epoch: 1436 \tTraining Loss: 0.040701\n",
      "Epoch: 1437 \tTraining Loss: 0.040478\n",
      "Epoch: 1438 \tTraining Loss: 0.040208\n",
      "Epoch: 1439 \tTraining Loss: 0.039679\n",
      "Epoch: 1440 \tTraining Loss: 0.040218\n",
      "Epoch: 1441 \tTraining Loss: 0.040271\n",
      "Epoch: 1442 \tTraining Loss: 0.045083\n",
      "Epoch: 1443 \tTraining Loss: 0.041321\n",
      "Epoch: 1444 \tTraining Loss: 0.040541\n",
      "Epoch: 1445 \tTraining Loss: 0.040528\n",
      "Epoch: 1446 \tTraining Loss: 0.040082\n",
      "Epoch: 1447 \tTraining Loss: 0.041431\n",
      "Epoch: 1448 \tTraining Loss: 0.039708\n",
      "Epoch: 1449 \tTraining Loss: 0.040323\n",
      "Epoch: 1450 \tTraining Loss: 0.039838\n",
      "Epoch: 1451 \tTraining Loss: 0.040586\n",
      "Epoch: 1452 \tTraining Loss: 0.039808\n",
      "Epoch: 1453 \tTraining Loss: 0.040517\n",
      "Epoch: 1454 \tTraining Loss: 0.039963\n",
      "Epoch: 1455 \tTraining Loss: 0.040587\n",
      "Epoch: 1456 \tTraining Loss: 0.040163\n",
      "Epoch: 1457 \tTraining Loss: 0.040425\n",
      "Epoch: 1458 \tTraining Loss: 0.039970\n",
      "Epoch: 1459 \tTraining Loss: 0.041224\n",
      "Epoch: 1460 \tTraining Loss: 0.040694\n",
      "Epoch: 1461 \tTraining Loss: 0.041068\n",
      "Epoch: 1462 \tTraining Loss: 0.039394\n",
      "Epoch: 1463 \tTraining Loss: 0.040193\n",
      "Epoch: 1464 \tTraining Loss: 0.040183\n",
      "Epoch: 1465 \tTraining Loss: 0.041040\n",
      "Epoch: 1466 \tTraining Loss: 0.040707\n",
      "Epoch: 1467 \tTraining Loss: 0.041850\n",
      "Epoch: 1468 \tTraining Loss: 0.041655\n",
      "Epoch: 1469 \tTraining Loss: 0.041176\n",
      "Epoch: 1470 \tTraining Loss: 0.042592\n",
      "Epoch: 1471 \tTraining Loss: 0.043023\n",
      "Epoch: 1472 \tTraining Loss: 0.040594\n",
      "Epoch: 1473 \tTraining Loss: 0.040090\n",
      "Epoch: 1474 \tTraining Loss: 0.040754\n",
      "Epoch: 1475 \tTraining Loss: 0.039696\n",
      "Epoch: 1476 \tTraining Loss: 0.040703\n",
      "Epoch: 1477 \tTraining Loss: 0.040289\n",
      "Epoch: 1478 \tTraining Loss: 0.040363\n",
      "Epoch: 1479 \tTraining Loss: 0.039639\n",
      "Epoch: 1480 \tTraining Loss: 0.039928\n",
      "Epoch: 1481 \tTraining Loss: 0.041471\n",
      "Epoch: 1482 \tTraining Loss: 0.040816\n",
      "Epoch: 1483 \tTraining Loss: 0.056402\n",
      "Epoch: 1484 \tTraining Loss: 0.104973\n",
      "Epoch: 1485 \tTraining Loss: 0.137870\n",
      "Epoch: 1486 \tTraining Loss: 0.168689\n",
      "Epoch: 1487 \tTraining Loss: 0.305767\n",
      "Epoch: 1488 \tTraining Loss: 0.199868\n",
      "Epoch: 1489 \tTraining Loss: 0.138259\n",
      "Epoch: 1490 \tTraining Loss: 0.252151\n",
      "Epoch: 1491 \tTraining Loss: 0.109959\n",
      "Epoch: 1492 \tTraining Loss: 0.071585\n",
      "Epoch: 1493 \tTraining Loss: 0.062304\n",
      "Epoch: 1494 \tTraining Loss: 0.053351\n",
      "Epoch: 1495 \tTraining Loss: 0.059286\n",
      "Epoch: 1496 \tTraining Loss: 0.052702\n",
      "Epoch: 1497 \tTraining Loss: 0.052256\n",
      "Epoch: 1498 \tTraining Loss: 0.050488\n",
      "Epoch: 1499 \tTraining Loss: 0.051121\n",
      "Epoch: 1500 \tTraining Loss: 0.048265\n",
      "Epoch: 1501 \tTraining Loss: 0.046798\n",
      "Epoch: 1502 \tTraining Loss: 0.048316\n",
      "Epoch: 1503 \tTraining Loss: 0.057929\n",
      "Epoch: 1504 \tTraining Loss: 0.047232\n",
      "Epoch: 1505 \tTraining Loss: 0.043984\n",
      "Epoch: 1506 \tTraining Loss: 0.042714\n",
      "Epoch: 1507 \tTraining Loss: 0.043501\n",
      "Epoch: 1508 \tTraining Loss: 0.040578\n",
      "Epoch: 1509 \tTraining Loss: 0.039689\n",
      "Epoch: 1510 \tTraining Loss: 0.040781\n",
      "Epoch: 1511 \tTraining Loss: 0.042207\n",
      "Epoch: 1512 \tTraining Loss: 0.038335\n",
      "Epoch: 1513 \tTraining Loss: 0.038442\n",
      "Epoch: 1514 \tTraining Loss: 0.037646\n",
      "Epoch: 1515 \tTraining Loss: 0.039808\n",
      "Epoch: 1516 \tTraining Loss: 0.037158\n",
      "Epoch: 1517 \tTraining Loss: 0.037904\n",
      "Epoch: 1518 \tTraining Loss: 0.039237\n",
      "Epoch: 1519 \tTraining Loss: 0.036925\n",
      "Epoch: 1520 \tTraining Loss: 0.039019\n",
      "Epoch: 1521 \tTraining Loss: 0.036480\n",
      "Epoch: 1522 \tTraining Loss: 0.036671\n",
      "Epoch: 1523 \tTraining Loss: 0.035796\n",
      "Epoch: 1524 \tTraining Loss: 0.037124\n",
      "Epoch: 1525 \tTraining Loss: 0.037652\n",
      "Epoch: 1526 \tTraining Loss: 0.036630\n",
      "Epoch: 1527 \tTraining Loss: 0.036503\n",
      "Epoch: 1528 \tTraining Loss: 0.035836\n",
      "Epoch: 1529 \tTraining Loss: 0.037126\n",
      "Epoch: 1530 \tTraining Loss: 0.035644\n",
      "Epoch: 1531 \tTraining Loss: 0.035523\n",
      "Epoch: 1532 \tTraining Loss: 0.035571\n",
      "Epoch: 1533 \tTraining Loss: 0.035281\n",
      "Epoch: 1534 \tTraining Loss: 0.035599\n",
      "Epoch: 1535 \tTraining Loss: 0.035810\n",
      "Epoch: 1536 \tTraining Loss: 0.037085\n",
      "Epoch: 1537 \tTraining Loss: 0.035008\n",
      "Epoch: 1538 \tTraining Loss: 0.035339\n",
      "Epoch: 1539 \tTraining Loss: 0.035917\n",
      "Epoch: 1540 \tTraining Loss: 0.035412\n",
      "Epoch: 1541 \tTraining Loss: 0.035013\n",
      "Epoch: 1542 \tTraining Loss: 0.034923\n",
      "Epoch: 1543 \tTraining Loss: 0.035285\n",
      "Epoch: 1544 \tTraining Loss: 0.035978\n",
      "Epoch: 1545 \tTraining Loss: 0.036390\n",
      "Epoch: 1546 \tTraining Loss: 0.035382\n",
      "Epoch: 1547 \tTraining Loss: 0.035087\n",
      "Epoch: 1548 \tTraining Loss: 0.035819\n",
      "Epoch: 1549 \tTraining Loss: 0.035707\n",
      "Epoch: 1550 \tTraining Loss: 0.036643\n",
      "Epoch: 1551 \tTraining Loss: 0.034803\n",
      "Epoch: 1552 \tTraining Loss: 0.034826\n",
      "Epoch: 1553 \tTraining Loss: 0.035682\n",
      "Epoch: 1554 \tTraining Loss: 0.034378\n",
      "Epoch: 1555 \tTraining Loss: 0.035690\n",
      "Epoch: 1556 \tTraining Loss: 0.037116\n",
      "Epoch: 1557 \tTraining Loss: 0.036383\n",
      "Epoch: 1558 \tTraining Loss: 0.047520\n",
      "Epoch: 1559 \tTraining Loss: 0.038404\n",
      "Epoch: 1560 \tTraining Loss: 0.046233\n",
      "Epoch: 1561 \tTraining Loss: 0.041736\n",
      "Epoch: 1562 \tTraining Loss: 0.075842\n",
      "Epoch: 1563 \tTraining Loss: 0.091683\n",
      "Epoch: 1564 \tTraining Loss: 0.147358\n",
      "Epoch: 1565 \tTraining Loss: 0.118606\n",
      "Epoch: 1566 \tTraining Loss: 0.085274\n",
      "Epoch: 1567 \tTraining Loss: 0.092404\n",
      "Epoch: 1568 \tTraining Loss: 0.062910\n",
      "Epoch: 1569 \tTraining Loss: 0.050797\n",
      "Epoch: 1570 \tTraining Loss: 0.051721\n",
      "Epoch: 1571 \tTraining Loss: 0.044536\n",
      "Epoch: 1572 \tTraining Loss: 0.042410\n",
      "Epoch: 1573 \tTraining Loss: 0.040339\n",
      "Epoch: 1574 \tTraining Loss: 0.037671\n",
      "Epoch: 1575 \tTraining Loss: 0.038637\n",
      "Epoch: 1576 \tTraining Loss: 0.038055\n",
      "Epoch: 1577 \tTraining Loss: 0.038156\n",
      "Epoch: 1578 \tTraining Loss: 0.037496\n",
      "Epoch: 1579 \tTraining Loss: 0.037199\n",
      "Epoch: 1580 \tTraining Loss: 0.036352\n",
      "Epoch: 1581 \tTraining Loss: 0.037365\n",
      "Epoch: 1582 \tTraining Loss: 0.036383\n",
      "Epoch: 1583 \tTraining Loss: 0.036206\n",
      "Epoch: 1584 \tTraining Loss: 0.037683\n",
      "Epoch: 1585 \tTraining Loss: 0.036346\n",
      "Epoch: 1586 \tTraining Loss: 0.036703\n",
      "Epoch: 1587 \tTraining Loss: 0.035572\n",
      "Epoch: 1588 \tTraining Loss: 0.037013\n",
      "Epoch: 1589 \tTraining Loss: 0.035324\n",
      "Epoch: 1590 \tTraining Loss: 0.037256\n",
      "Epoch: 1591 \tTraining Loss: 0.035455\n",
      "Epoch: 1592 \tTraining Loss: 0.037480\n",
      "Epoch: 1593 \tTraining Loss: 0.035922\n",
      "Epoch: 1594 \tTraining Loss: 0.035970\n",
      "Epoch: 1595 \tTraining Loss: 0.034251\n",
      "Epoch: 1596 \tTraining Loss: 0.034997\n",
      "Epoch: 1597 \tTraining Loss: 0.035055\n",
      "Epoch: 1598 \tTraining Loss: 0.035391\n",
      "Epoch: 1599 \tTraining Loss: 0.034152\n",
      "Epoch: 1600 \tTraining Loss: 0.033750\n",
      "Epoch: 1601 \tTraining Loss: 0.035063\n",
      "Epoch: 1602 \tTraining Loss: 0.034378\n",
      "Epoch: 1603 \tTraining Loss: 0.037817\n",
      "Epoch: 1604 \tTraining Loss: 0.038239\n",
      "Epoch: 1605 \tTraining Loss: 0.037580\n",
      "Epoch: 1606 \tTraining Loss: 0.033495\n",
      "Epoch: 1607 \tTraining Loss: 0.034814\n",
      "Epoch: 1608 \tTraining Loss: 0.033984\n",
      "Epoch: 1609 \tTraining Loss: 0.034307\n",
      "Epoch: 1610 \tTraining Loss: 0.033770\n",
      "Epoch: 1611 \tTraining Loss: 0.033918\n",
      "Epoch: 1612 \tTraining Loss: 0.033791\n",
      "Epoch: 1613 \tTraining Loss: 0.034032\n",
      "Epoch: 1614 \tTraining Loss: 0.034031\n",
      "Epoch: 1615 \tTraining Loss: 0.034118\n",
      "Epoch: 1616 \tTraining Loss: 0.033547\n",
      "Epoch: 1617 \tTraining Loss: 0.039995\n",
      "Epoch: 1618 \tTraining Loss: 0.036852\n",
      "Epoch: 1619 \tTraining Loss: 0.041678\n",
      "Epoch: 1620 \tTraining Loss: 0.036546\n",
      "Epoch: 1621 \tTraining Loss: 0.035181\n",
      "Epoch: 1622 \tTraining Loss: 0.037934\n",
      "Epoch: 1623 \tTraining Loss: 0.033510\n",
      "Epoch: 1624 \tTraining Loss: 0.034154\n",
      "Epoch: 1625 \tTraining Loss: 0.033748\n",
      "Epoch: 1626 \tTraining Loss: 0.036708\n",
      "Epoch: 1627 \tTraining Loss: 0.037706\n",
      "Epoch: 1628 \tTraining Loss: 0.047806\n",
      "Epoch: 1629 \tTraining Loss: 0.139495\n",
      "Epoch: 1630 \tTraining Loss: 0.205806\n",
      "Epoch: 1631 \tTraining Loss: 0.122629\n",
      "Epoch: 1632 \tTraining Loss: 0.139835\n",
      "Epoch: 1633 \tTraining Loss: 0.170234\n",
      "Epoch: 1634 \tTraining Loss: 0.105235\n",
      "Epoch: 1635 \tTraining Loss: 0.109458\n",
      "Epoch: 1636 \tTraining Loss: 0.064913\n",
      "Epoch: 1637 \tTraining Loss: 0.058201\n",
      "Epoch: 1638 \tTraining Loss: 0.053804\n",
      "Epoch: 1639 \tTraining Loss: 0.047894\n",
      "Epoch: 1640 \tTraining Loss: 0.044942\n",
      "Epoch: 1641 \tTraining Loss: 0.043791\n",
      "Epoch: 1642 \tTraining Loss: 0.043679\n",
      "Epoch: 1643 \tTraining Loss: 0.041075\n",
      "Epoch: 1644 \tTraining Loss: 0.040744\n",
      "Epoch: 1645 \tTraining Loss: 0.038024\n",
      "Epoch: 1646 \tTraining Loss: 0.039495\n",
      "Epoch: 1647 \tTraining Loss: 0.036990\n",
      "Epoch: 1648 \tTraining Loss: 0.037464\n",
      "Epoch: 1649 \tTraining Loss: 0.037314\n",
      "Epoch: 1650 \tTraining Loss: 0.037370\n",
      "Epoch: 1651 \tTraining Loss: 0.036917\n",
      "Epoch: 1652 \tTraining Loss: 0.037896\n",
      "Epoch: 1653 \tTraining Loss: 0.035698\n",
      "Epoch: 1654 \tTraining Loss: 0.037402\n",
      "Epoch: 1655 \tTraining Loss: 0.036019\n",
      "Epoch: 1656 \tTraining Loss: 0.042300\n",
      "Epoch: 1657 \tTraining Loss: 0.037444\n",
      "Epoch: 1658 \tTraining Loss: 0.035717\n",
      "Epoch: 1659 \tTraining Loss: 0.035476\n",
      "Epoch: 1660 \tTraining Loss: 0.034925\n",
      "Epoch: 1661 \tTraining Loss: 0.034558\n",
      "Epoch: 1662 \tTraining Loss: 0.034475\n",
      "Epoch: 1663 \tTraining Loss: 0.035466\n",
      "Epoch: 1664 \tTraining Loss: 0.034745\n",
      "Epoch: 1665 \tTraining Loss: 0.034968\n",
      "Epoch: 1666 \tTraining Loss: 0.034384\n",
      "Epoch: 1667 \tTraining Loss: 0.035527\n",
      "Epoch: 1668 \tTraining Loss: 0.039814\n",
      "Epoch: 1669 \tTraining Loss: 0.038463\n",
      "Epoch: 1670 \tTraining Loss: 0.037643\n",
      "Epoch: 1671 \tTraining Loss: 0.034070\n",
      "Epoch: 1672 \tTraining Loss: 0.034791\n",
      "Epoch: 1673 \tTraining Loss: 0.033567\n",
      "Epoch: 1674 \tTraining Loss: 0.033245\n",
      "Epoch: 1675 \tTraining Loss: 0.033005\n",
      "Epoch: 1676 \tTraining Loss: 0.032821\n",
      "Epoch: 1677 \tTraining Loss: 0.032710\n",
      "Epoch: 1678 \tTraining Loss: 0.040974\n",
      "Epoch: 1679 \tTraining Loss: 0.034610\n",
      "Epoch: 1680 \tTraining Loss: 0.034303\n",
      "Epoch: 1681 \tTraining Loss: 0.036678\n",
      "Epoch: 1682 \tTraining Loss: 0.035985\n",
      "Epoch: 1683 \tTraining Loss: 0.033313\n",
      "Epoch: 1684 \tTraining Loss: 0.034105\n",
      "Epoch: 1685 \tTraining Loss: 0.032783\n",
      "Epoch: 1686 \tTraining Loss: 0.032162\n",
      "Epoch: 1687 \tTraining Loss: 0.032530\n",
      "Epoch: 1688 \tTraining Loss: 0.032740\n",
      "Epoch: 1689 \tTraining Loss: 0.032066\n",
      "Epoch: 1690 \tTraining Loss: 0.032551\n",
      "Epoch: 1691 \tTraining Loss: 0.033199\n",
      "Epoch: 1692 \tTraining Loss: 0.032314\n",
      "Epoch: 1693 \tTraining Loss: 0.032377\n",
      "Epoch: 1694 \tTraining Loss: 0.032487\n",
      "Epoch: 1695 \tTraining Loss: 0.032183\n",
      "Epoch: 1696 \tTraining Loss: 0.031315\n",
      "Epoch: 1697 \tTraining Loss: 0.032759\n",
      "Epoch: 1698 \tTraining Loss: 0.031747\n",
      "Epoch: 1699 \tTraining Loss: 0.032694\n",
      "Epoch: 1700 \tTraining Loss: 0.031892\n",
      "Epoch: 1701 \tTraining Loss: 0.031323\n",
      "Epoch: 1702 \tTraining Loss: 0.031578\n",
      "Epoch: 1703 \tTraining Loss: 0.033393\n",
      "Epoch: 1704 \tTraining Loss: 0.033186\n",
      "Epoch: 1705 \tTraining Loss: 0.030739\n",
      "Epoch: 1706 \tTraining Loss: 0.032079\n",
      "Epoch: 1707 \tTraining Loss: 0.032893\n",
      "Epoch: 1708 \tTraining Loss: 0.031791\n",
      "Epoch: 1709 \tTraining Loss: 0.032011\n",
      "Epoch: 1710 \tTraining Loss: 0.031266\n",
      "Epoch: 1711 \tTraining Loss: 0.033431\n",
      "Epoch: 1712 \tTraining Loss: 0.031936\n",
      "Epoch: 1713 \tTraining Loss: 0.031457\n",
      "Epoch: 1714 \tTraining Loss: 0.031566\n",
      "Epoch: 1715 \tTraining Loss: 0.032772\n",
      "Epoch: 1716 \tTraining Loss: 0.032605\n",
      "Epoch: 1717 \tTraining Loss: 0.031890\n",
      "Epoch: 1718 \tTraining Loss: 0.032586\n",
      "Epoch: 1719 \tTraining Loss: 0.031583\n",
      "Epoch: 1720 \tTraining Loss: 0.031443\n",
      "Epoch: 1721 \tTraining Loss: 0.032093\n",
      "Epoch: 1722 \tTraining Loss: 0.037058\n",
      "Epoch: 1723 \tTraining Loss: 0.045846\n",
      "Epoch: 1724 \tTraining Loss: 0.052453\n",
      "Epoch: 1725 \tTraining Loss: 0.080243\n",
      "Epoch: 1726 \tTraining Loss: 0.161891\n",
      "Epoch: 1727 \tTraining Loss: 0.223704\n",
      "Epoch: 1728 \tTraining Loss: 0.126678\n",
      "Epoch: 1729 \tTraining Loss: 0.073494\n",
      "Epoch: 1730 \tTraining Loss: 0.052330\n",
      "Epoch: 1731 \tTraining Loss: 0.045268\n",
      "Epoch: 1732 \tTraining Loss: 0.045380\n",
      "Epoch: 1733 \tTraining Loss: 0.054963\n",
      "Epoch: 1734 \tTraining Loss: 0.047069\n",
      "Epoch: 1735 \tTraining Loss: 0.045732\n",
      "Epoch: 1736 \tTraining Loss: 0.041226\n",
      "Epoch: 1737 \tTraining Loss: 0.043417\n",
      "Epoch: 1738 \tTraining Loss: 0.050562\n",
      "Epoch: 1739 \tTraining Loss: 0.041033\n",
      "Epoch: 1740 \tTraining Loss: 0.039011\n",
      "Epoch: 1741 \tTraining Loss: 0.038945\n",
      "Epoch: 1742 \tTraining Loss: 0.038381\n",
      "Epoch: 1743 \tTraining Loss: 0.040633\n",
      "Epoch: 1744 \tTraining Loss: 0.039050\n",
      "Epoch: 1745 \tTraining Loss: 0.041141\n",
      "Epoch: 1746 \tTraining Loss: 0.037981\n",
      "Epoch: 1747 \tTraining Loss: 0.038447\n",
      "Epoch: 1748 \tTraining Loss: 0.043680\n",
      "Epoch: 1749 \tTraining Loss: 0.038110\n",
      "Epoch: 1750 \tTraining Loss: 0.038690\n",
      "Epoch: 1751 \tTraining Loss: 0.038759\n",
      "Epoch: 1752 \tTraining Loss: 0.038614\n",
      "Epoch: 1753 \tTraining Loss: 0.038356\n",
      "Epoch: 1754 \tTraining Loss: 0.038743\n",
      "Epoch: 1755 \tTraining Loss: 0.038265\n",
      "Epoch: 1756 \tTraining Loss: 0.039230\n",
      "Epoch: 1757 \tTraining Loss: 0.037982\n",
      "Epoch: 1758 \tTraining Loss: 0.039353\n",
      "Epoch: 1759 \tTraining Loss: 0.039692\n",
      "Epoch: 1760 \tTraining Loss: 0.037335\n",
      "Epoch: 1761 \tTraining Loss: 0.035872\n",
      "Epoch: 1762 \tTraining Loss: 0.036511\n",
      "Epoch: 1763 \tTraining Loss: 0.036102\n",
      "Epoch: 1764 \tTraining Loss: 0.035891\n",
      "Epoch: 1765 \tTraining Loss: 0.035789\n",
      "Epoch: 1766 \tTraining Loss: 0.035187\n",
      "Epoch: 1767 \tTraining Loss: 0.035899\n",
      "Epoch: 1768 \tTraining Loss: 0.036701\n",
      "Epoch: 1769 \tTraining Loss: 0.038019\n",
      "Epoch: 1770 \tTraining Loss: 0.035098\n",
      "Epoch: 1771 \tTraining Loss: 0.036002\n",
      "Epoch: 1772 \tTraining Loss: 0.035907\n",
      "Epoch: 1773 \tTraining Loss: 0.035566\n",
      "Epoch: 1774 \tTraining Loss: 0.035735\n",
      "Epoch: 1775 \tTraining Loss: 0.035396\n",
      "Epoch: 1776 \tTraining Loss: 0.036625\n",
      "Epoch: 1777 \tTraining Loss: 0.040780\n",
      "Epoch: 1778 \tTraining Loss: 0.035622\n",
      "Epoch: 1779 \tTraining Loss: 0.035794\n",
      "Epoch: 1780 \tTraining Loss: 0.035497\n",
      "Epoch: 1781 \tTraining Loss: 0.035701\n",
      "Epoch: 1782 \tTraining Loss: 0.035583\n",
      "Epoch: 1783 \tTraining Loss: 0.036099\n",
      "Epoch: 1784 \tTraining Loss: 0.036129\n",
      "Epoch: 1785 \tTraining Loss: 0.035526\n",
      "Epoch: 1786 \tTraining Loss: 0.035765\n",
      "Epoch: 1787 \tTraining Loss: 0.035744\n",
      "Epoch: 1788 \tTraining Loss: 0.035661\n",
      "Epoch: 1789 \tTraining Loss: 0.036848\n",
      "Epoch: 1790 \tTraining Loss: 0.037266\n",
      "Epoch: 1791 \tTraining Loss: 0.035035\n",
      "Epoch: 1792 \tTraining Loss: 0.035467\n",
      "Epoch: 1793 \tTraining Loss: 0.034719\n",
      "Epoch: 1794 \tTraining Loss: 0.035965\n",
      "Epoch: 1795 \tTraining Loss: 0.034842\n",
      "Epoch: 1796 \tTraining Loss: 0.034894\n",
      "Epoch: 1797 \tTraining Loss: 0.034647\n",
      "Epoch: 1798 \tTraining Loss: 0.035051\n",
      "Epoch: 1799 \tTraining Loss: 0.035663\n",
      "Epoch: 1800 \tTraining Loss: 0.035498\n",
      "Epoch: 1801 \tTraining Loss: 0.034916\n",
      "Epoch: 1802 \tTraining Loss: 0.034649\n",
      "Epoch: 1803 \tTraining Loss: 0.035386\n",
      "Epoch: 1804 \tTraining Loss: 0.035026\n",
      "Epoch: 1805 \tTraining Loss: 0.036822\n",
      "Epoch: 1806 \tTraining Loss: 0.037186\n",
      "Epoch: 1807 \tTraining Loss: 0.035671\n",
      "Epoch: 1808 \tTraining Loss: 0.035186\n",
      "Epoch: 1809 \tTraining Loss: 0.035287\n",
      "Epoch: 1810 \tTraining Loss: 0.035760\n",
      "Epoch: 1811 \tTraining Loss: 0.035865\n",
      "Epoch: 1812 \tTraining Loss: 0.035817\n",
      "Epoch: 1813 \tTraining Loss: 0.035230\n",
      "Epoch: 1814 \tTraining Loss: 0.036037\n",
      "Epoch: 1815 \tTraining Loss: 0.035783\n",
      "Epoch: 1816 \tTraining Loss: 0.036576\n",
      "Epoch: 1817 \tTraining Loss: 0.040839\n",
      "Epoch: 1818 \tTraining Loss: 0.036160\n",
      "Epoch: 1819 \tTraining Loss: 0.036002\n",
      "Epoch: 1820 \tTraining Loss: 0.035292\n",
      "Epoch: 1821 \tTraining Loss: 0.036568\n",
      "Epoch: 1822 \tTraining Loss: 0.034223\n",
      "Epoch: 1823 \tTraining Loss: 0.035412\n",
      "Epoch: 1824 \tTraining Loss: 0.035122\n",
      "Epoch: 1825 \tTraining Loss: 0.034735\n",
      "Epoch: 1826 \tTraining Loss: 0.035470\n",
      "Epoch: 1827 \tTraining Loss: 0.037161\n",
      "Epoch: 1828 \tTraining Loss: 0.034611\n",
      "Epoch: 1829 \tTraining Loss: 0.035962\n",
      "Epoch: 1830 \tTraining Loss: 0.034807\n",
      "Epoch: 1831 \tTraining Loss: 0.035590\n",
      "Epoch: 1832 \tTraining Loss: 0.035032\n",
      "Epoch: 1833 \tTraining Loss: 0.036536\n",
      "Epoch: 1834 \tTraining Loss: 0.034929\n",
      "Epoch: 1835 \tTraining Loss: 0.032939\n",
      "Epoch: 1836 \tTraining Loss: 0.033119\n",
      "Epoch: 1837 \tTraining Loss: 0.032921\n",
      "Epoch: 1838 \tTraining Loss: 0.032287\n",
      "Epoch: 1839 \tTraining Loss: 0.031715\n",
      "Epoch: 1840 \tTraining Loss: 0.031681\n",
      "Epoch: 1841 \tTraining Loss: 0.031982\n",
      "Epoch: 1842 \tTraining Loss: 0.031983\n",
      "Epoch: 1843 \tTraining Loss: 0.032497\n",
      "Epoch: 1844 \tTraining Loss: 0.031814\n",
      "Epoch: 1845 \tTraining Loss: 0.044461\n",
      "Epoch: 1846 \tTraining Loss: 0.044640\n",
      "Epoch: 1847 \tTraining Loss: 0.151097\n",
      "Epoch: 1848 \tTraining Loss: 0.246838\n",
      "Epoch: 1849 \tTraining Loss: 0.232016\n",
      "Epoch: 1850 \tTraining Loss: 0.123323\n",
      "Epoch: 1851 \tTraining Loss: 0.082181\n",
      "Epoch: 1852 \tTraining Loss: 0.052433\n",
      "Epoch: 1853 \tTraining Loss: 0.042736\n",
      "Epoch: 1854 \tTraining Loss: 0.045815\n",
      "Epoch: 1855 \tTraining Loss: 0.044297\n",
      "Epoch: 1856 \tTraining Loss: 0.039759\n",
      "Epoch: 1857 \tTraining Loss: 0.039723\n",
      "Epoch: 1858 \tTraining Loss: 0.041948\n",
      "Epoch: 1859 \tTraining Loss: 0.040149\n",
      "Epoch: 1860 \tTraining Loss: 0.040011\n",
      "Epoch: 1861 \tTraining Loss: 0.039922\n",
      "Epoch: 1862 \tTraining Loss: 0.039119\n",
      "Epoch: 1863 \tTraining Loss: 0.038431\n",
      "Epoch: 1864 \tTraining Loss: 0.039799\n",
      "Epoch: 1865 \tTraining Loss: 0.043393\n",
      "Epoch: 1866 \tTraining Loss: 0.039127\n",
      "Epoch: 1867 \tTraining Loss: 0.039747\n",
      "Epoch: 1868 \tTraining Loss: 0.038554\n",
      "Epoch: 1869 \tTraining Loss: 0.040169\n",
      "Epoch: 1870 \tTraining Loss: 0.039442\n",
      "Epoch: 1871 \tTraining Loss: 0.039318\n",
      "Epoch: 1872 \tTraining Loss: 0.038418\n",
      "Epoch: 1873 \tTraining Loss: 0.039352\n",
      "Epoch: 1874 \tTraining Loss: 0.038985\n",
      "Epoch: 1875 \tTraining Loss: 0.038619\n",
      "Epoch: 1876 \tTraining Loss: 0.038872\n",
      "Epoch: 1877 \tTraining Loss: 0.038212\n",
      "Epoch: 1878 \tTraining Loss: 0.038470\n",
      "Epoch: 1879 \tTraining Loss: 0.038134\n",
      "Epoch: 1880 \tTraining Loss: 0.037930\n",
      "Epoch: 1881 \tTraining Loss: 0.038314\n",
      "Epoch: 1882 \tTraining Loss: 0.041621\n",
      "Epoch: 1883 \tTraining Loss: 0.040768\n",
      "Epoch: 1884 \tTraining Loss: 0.038899\n",
      "Epoch: 1885 \tTraining Loss: 0.037127\n",
      "Epoch: 1886 \tTraining Loss: 0.037484\n",
      "Epoch: 1887 \tTraining Loss: 0.037680\n",
      "Epoch: 1888 \tTraining Loss: 0.037568\n",
      "Epoch: 1889 \tTraining Loss: 0.038364\n",
      "Epoch: 1890 \tTraining Loss: 0.038197\n",
      "Epoch: 1891 \tTraining Loss: 0.037371\n",
      "Epoch: 1892 \tTraining Loss: 0.037173\n",
      "Epoch: 1893 \tTraining Loss: 0.038554\n",
      "Epoch: 1894 \tTraining Loss: 0.037664\n",
      "Epoch: 1895 \tTraining Loss: 0.037677\n",
      "Epoch: 1896 \tTraining Loss: 0.036819\n",
      "Epoch: 1897 \tTraining Loss: 0.038243\n",
      "Epoch: 1898 \tTraining Loss: 0.036558\n",
      "Epoch: 1899 \tTraining Loss: 0.035638\n",
      "Epoch: 1900 \tTraining Loss: 0.034887\n",
      "Epoch: 1901 \tTraining Loss: 0.035730\n",
      "Epoch: 1902 \tTraining Loss: 0.037080\n",
      "Epoch: 1903 \tTraining Loss: 0.035673\n",
      "Epoch: 1904 \tTraining Loss: 0.035670\n",
      "Epoch: 1905 \tTraining Loss: 0.035610\n",
      "Epoch: 1906 \tTraining Loss: 0.035480\n",
      "Epoch: 1907 \tTraining Loss: 0.036650\n",
      "Epoch: 1908 \tTraining Loss: 0.035510\n",
      "Epoch: 1909 \tTraining Loss: 0.035228\n",
      "Epoch: 1910 \tTraining Loss: 0.034547\n",
      "Epoch: 1911 \tTraining Loss: 0.034369\n",
      "Epoch: 1912 \tTraining Loss: 0.034814\n",
      "Epoch: 1913 \tTraining Loss: 0.035510\n",
      "Epoch: 1914 \tTraining Loss: 0.034742\n",
      "Epoch: 1915 \tTraining Loss: 0.034973\n",
      "Epoch: 1916 \tTraining Loss: 0.034696\n",
      "Epoch: 1917 \tTraining Loss: 0.036026\n",
      "Epoch: 1918 \tTraining Loss: 0.037784\n",
      "Epoch: 1919 \tTraining Loss: 0.037261\n",
      "Epoch: 1920 \tTraining Loss: 0.035817\n",
      "Epoch: 1921 \tTraining Loss: 0.035236\n",
      "Epoch: 1922 \tTraining Loss: 0.035414\n",
      "Epoch: 1923 \tTraining Loss: 0.034823\n",
      "Epoch: 1924 \tTraining Loss: 0.035157\n",
      "Epoch: 1925 \tTraining Loss: 0.039462\n",
      "Epoch: 1926 \tTraining Loss: 0.034817\n",
      "Epoch: 1927 \tTraining Loss: 0.039290\n",
      "Epoch: 1928 \tTraining Loss: 0.056651\n",
      "Epoch: 1929 \tTraining Loss: 0.087446\n",
      "Epoch: 1930 \tTraining Loss: 0.203709\n",
      "Epoch: 1931 \tTraining Loss: 0.440677\n",
      "Epoch: 1932 \tTraining Loss: 0.263197\n",
      "Epoch: 1933 \tTraining Loss: 0.108792\n",
      "Epoch: 1934 \tTraining Loss: 0.065473\n",
      "Epoch: 1935 \tTraining Loss: 0.058055\n",
      "Epoch: 1936 \tTraining Loss: 0.051414\n",
      "Epoch: 1937 \tTraining Loss: 0.048168\n",
      "Epoch: 1938 \tTraining Loss: 0.046517\n",
      "Epoch: 1939 \tTraining Loss: 0.046046\n",
      "Epoch: 1940 \tTraining Loss: 0.045325\n",
      "Epoch: 1941 \tTraining Loss: 0.045287\n",
      "Epoch: 1942 \tTraining Loss: 0.042733\n",
      "Epoch: 1943 \tTraining Loss: 0.041026\n",
      "Epoch: 1944 \tTraining Loss: 0.039889\n",
      "Epoch: 1945 \tTraining Loss: 0.039678\n",
      "Epoch: 1946 \tTraining Loss: 0.038482\n",
      "Epoch: 1947 \tTraining Loss: 0.037197\n",
      "Epoch: 1948 \tTraining Loss: 0.037939\n",
      "Epoch: 1949 \tTraining Loss: 0.036800\n",
      "Epoch: 1950 \tTraining Loss: 0.037230\n",
      "Epoch: 1951 \tTraining Loss: 0.043895\n",
      "Epoch: 1952 \tTraining Loss: 0.043941\n",
      "Epoch: 1953 \tTraining Loss: 0.038050\n",
      "Epoch: 1954 \tTraining Loss: 0.037010\n",
      "Epoch: 1955 \tTraining Loss: 0.038124\n",
      "Epoch: 1956 \tTraining Loss: 0.037079\n",
      "Epoch: 1957 \tTraining Loss: 0.038251\n",
      "Epoch: 1958 \tTraining Loss: 0.037288\n",
      "Epoch: 1959 \tTraining Loss: 0.037556\n",
      "Epoch: 1960 \tTraining Loss: 0.037319\n",
      "Epoch: 1961 \tTraining Loss: 0.036575\n",
      "Epoch: 1962 \tTraining Loss: 0.036137\n",
      "Epoch: 1963 \tTraining Loss: 0.037005\n",
      "Epoch: 1964 \tTraining Loss: 0.037315\n",
      "Epoch: 1965 \tTraining Loss: 0.036249\n",
      "Epoch: 1966 \tTraining Loss: 0.036326\n",
      "Epoch: 1967 \tTraining Loss: 0.036292\n",
      "Epoch: 1968 \tTraining Loss: 0.037476\n",
      "Epoch: 1969 \tTraining Loss: 0.036795\n",
      "Epoch: 1970 \tTraining Loss: 0.036493\n",
      "Epoch: 1971 \tTraining Loss: 0.036375\n",
      "Epoch: 1972 \tTraining Loss: 0.037403\n",
      "Epoch: 1973 \tTraining Loss: 0.035480\n",
      "Epoch: 1974 \tTraining Loss: 0.035730\n",
      "Epoch: 1975 \tTraining Loss: 0.035417\n",
      "Epoch: 1976 \tTraining Loss: 0.035466\n",
      "Epoch: 1977 \tTraining Loss: 0.036483\n",
      "Epoch: 1978 \tTraining Loss: 0.039711\n",
      "Epoch: 1979 \tTraining Loss: 0.037742\n",
      "Epoch: 1980 \tTraining Loss: 0.035665\n",
      "Epoch: 1981 \tTraining Loss: 0.035962\n",
      "Epoch: 1982 \tTraining Loss: 0.038161\n",
      "Epoch: 1983 \tTraining Loss: 0.037453\n",
      "Epoch: 1984 \tTraining Loss: 0.038164\n",
      "Epoch: 1985 \tTraining Loss: 0.035478\n",
      "Epoch: 1986 \tTraining Loss: 0.035799\n",
      "Epoch: 1987 \tTraining Loss: 0.035093\n",
      "Epoch: 1988 \tTraining Loss: 0.035567\n",
      "Epoch: 1989 \tTraining Loss: 0.035837\n",
      "Epoch: 1990 \tTraining Loss: 0.035609\n",
      "Epoch: 1991 \tTraining Loss: 0.035608\n",
      "Epoch: 1992 \tTraining Loss: 0.036231\n",
      "Epoch: 1993 \tTraining Loss: 0.034937\n",
      "Epoch: 1994 \tTraining Loss: 0.035998\n",
      "Epoch: 1995 \tTraining Loss: 0.035020\n",
      "Epoch: 1996 \tTraining Loss: 0.035402\n",
      "Epoch: 1997 \tTraining Loss: 0.035416\n",
      "Epoch: 1998 \tTraining Loss: 0.036214\n",
      "Epoch: 1999 \tTraining Loss: 0.036286\n",
      "Epoch: 2000 \tTraining Loss: 0.035731\n",
      "Epoch: 2001 \tTraining Loss: 0.036466\n",
      "Epoch: 2002 \tTraining Loss: 0.037921\n",
      "Epoch: 2003 \tTraining Loss: 0.037474\n",
      "Epoch: 2004 \tTraining Loss: 0.037235\n",
      "Epoch: 2005 \tTraining Loss: 0.037538\n",
      "Epoch: 2006 \tTraining Loss: 0.036808\n",
      "Epoch: 2007 \tTraining Loss: 0.036438\n",
      "Epoch: 2008 \tTraining Loss: 0.035730\n",
      "Epoch: 2009 \tTraining Loss: 0.036558\n",
      "Epoch: 2010 \tTraining Loss: 0.036543\n",
      "Epoch: 2011 \tTraining Loss: 0.036218\n",
      "Epoch: 2012 \tTraining Loss: 0.036804\n",
      "Epoch: 2013 \tTraining Loss: 0.036744\n",
      "Epoch: 2014 \tTraining Loss: 0.036374\n",
      "Epoch: 2015 \tTraining Loss: 0.036038\n",
      "Epoch: 2016 \tTraining Loss: 0.035691\n",
      "Epoch: 2017 \tTraining Loss: 0.036271\n",
      "Epoch: 2018 \tTraining Loss: 0.035764\n",
      "Epoch: 2019 \tTraining Loss: 0.035796\n",
      "Epoch: 2020 \tTraining Loss: 0.035692\n",
      "Epoch: 2021 \tTraining Loss: 0.035194\n",
      "Epoch: 2022 \tTraining Loss: 0.034880\n",
      "Epoch: 2023 \tTraining Loss: 0.037790\n",
      "Epoch: 2024 \tTraining Loss: 0.037207\n",
      "Epoch: 2025 \tTraining Loss: 0.036586\n",
      "Epoch: 2026 \tTraining Loss: 0.037087\n",
      "Epoch: 2027 \tTraining Loss: 0.035874\n",
      "Epoch: 2028 \tTraining Loss: 0.036060\n",
      "Epoch: 2029 \tTraining Loss: 0.035865\n",
      "Epoch: 2030 \tTraining Loss: 0.034911\n",
      "Epoch: 2031 \tTraining Loss: 0.035291\n",
      "Epoch: 2032 \tTraining Loss: 0.035664\n",
      "Epoch: 2033 \tTraining Loss: 0.034954\n",
      "Epoch: 2034 \tTraining Loss: 0.034969\n",
      "Epoch: 2035 \tTraining Loss: 0.035202\n",
      "Epoch: 2036 \tTraining Loss: 0.035231\n",
      "Epoch: 2037 \tTraining Loss: 0.036196\n",
      "Epoch: 2038 \tTraining Loss: 0.037789\n",
      "Epoch: 2039 \tTraining Loss: 0.036531\n",
      "Epoch: 2040 \tTraining Loss: 0.047077\n",
      "Epoch: 2041 \tTraining Loss: 0.039641\n",
      "Epoch: 2042 \tTraining Loss: 0.039690\n",
      "Epoch: 2043 \tTraining Loss: 0.038069\n",
      "Epoch: 2044 \tTraining Loss: 0.036744\n",
      "Epoch: 2045 \tTraining Loss: 0.036661\n",
      "Epoch: 2046 \tTraining Loss: 0.035551\n",
      "Epoch: 2047 \tTraining Loss: 0.034734\n",
      "Epoch: 2048 \tTraining Loss: 0.035678\n",
      "Epoch: 2049 \tTraining Loss: 0.035049\n",
      "Epoch: 2050 \tTraining Loss: 0.034629\n",
      "Epoch: 2051 \tTraining Loss: 0.035448\n",
      "Epoch: 2052 \tTraining Loss: 0.036042\n",
      "Epoch: 2053 \tTraining Loss: 0.039167\n",
      "Epoch: 2054 \tTraining Loss: 0.043725\n",
      "Epoch: 2055 \tTraining Loss: 0.094770\n",
      "Epoch: 2056 \tTraining Loss: 0.316923\n",
      "Epoch: 2057 \tTraining Loss: 0.181332\n",
      "Epoch: 2058 \tTraining Loss: 0.142078\n",
      "Epoch: 2059 \tTraining Loss: 0.094662\n",
      "Epoch: 2060 \tTraining Loss: 0.080990\n",
      "Epoch: 2061 \tTraining Loss: 0.055541\n",
      "Epoch: 2062 \tTraining Loss: 0.053368\n",
      "Epoch: 2063 \tTraining Loss: 0.053845\n",
      "Epoch: 2064 \tTraining Loss: 0.055032\n",
      "Epoch: 2065 \tTraining Loss: 0.048488\n",
      "Epoch: 2066 \tTraining Loss: 0.045579\n",
      "Epoch: 2067 \tTraining Loss: 0.046632\n",
      "Epoch: 2068 \tTraining Loss: 0.046472\n",
      "Epoch: 2069 \tTraining Loss: 0.047341\n",
      "Epoch: 2070 \tTraining Loss: 0.047318\n",
      "Epoch: 2071 \tTraining Loss: 0.039895\n",
      "Epoch: 2072 \tTraining Loss: 0.040820\n",
      "Epoch: 2073 \tTraining Loss: 0.040466\n",
      "Epoch: 2074 \tTraining Loss: 0.040754\n",
      "Epoch: 2075 \tTraining Loss: 0.045459\n",
      "Epoch: 2076 \tTraining Loss: 0.035929\n",
      "Epoch: 2077 \tTraining Loss: 0.037427\n",
      "Epoch: 2078 \tTraining Loss: 0.039503\n",
      "Epoch: 2079 \tTraining Loss: 0.037399\n",
      "Epoch: 2080 \tTraining Loss: 0.037555\n",
      "Epoch: 2081 \tTraining Loss: 0.036056\n",
      "Epoch: 2082 \tTraining Loss: 0.035622\n",
      "Epoch: 2083 \tTraining Loss: 0.035537\n",
      "Epoch: 2084 \tTraining Loss: 0.036713\n",
      "Epoch: 2085 \tTraining Loss: 0.035917\n",
      "Epoch: 2086 \tTraining Loss: 0.035669\n",
      "Epoch: 2087 \tTraining Loss: 0.035886\n",
      "Epoch: 2088 \tTraining Loss: 0.036662\n",
      "Epoch: 2089 \tTraining Loss: 0.035125\n",
      "Epoch: 2090 \tTraining Loss: 0.035651\n",
      "Epoch: 2091 \tTraining Loss: 0.035049\n",
      "Epoch: 2092 \tTraining Loss: 0.035802\n",
      "Epoch: 2093 \tTraining Loss: 0.035287\n",
      "Epoch: 2094 \tTraining Loss: 0.037279\n",
      "Epoch: 2095 \tTraining Loss: 0.036469\n",
      "Epoch: 2096 \tTraining Loss: 0.037036\n",
      "Epoch: 2097 \tTraining Loss: 0.036553\n",
      "Epoch: 2098 \tTraining Loss: 0.037007\n",
      "Epoch: 2099 \tTraining Loss: 0.041230\n",
      "Epoch: 2100 \tTraining Loss: 0.038170\n",
      "Epoch: 2101 \tTraining Loss: 0.036458\n",
      "Epoch: 2102 \tTraining Loss: 0.035351\n",
      "Epoch: 2103 \tTraining Loss: 0.035904\n",
      "Epoch: 2104 \tTraining Loss: 0.035818\n",
      "Epoch: 2105 \tTraining Loss: 0.035205\n",
      "Epoch: 2106 \tTraining Loss: 0.035307\n",
      "Epoch: 2107 \tTraining Loss: 0.035020\n",
      "Epoch: 2108 \tTraining Loss: 0.034755\n",
      "Epoch: 2109 \tTraining Loss: 0.035529\n",
      "Epoch: 2110 \tTraining Loss: 0.034791\n",
      "Epoch: 2111 \tTraining Loss: 0.035569\n",
      "Epoch: 2112 \tTraining Loss: 0.035840\n",
      "Epoch: 2113 \tTraining Loss: 0.036431\n",
      "Epoch: 2114 \tTraining Loss: 0.038513\n",
      "Epoch: 2115 \tTraining Loss: 0.034874\n",
      "Epoch: 2116 \tTraining Loss: 0.036122\n",
      "Epoch: 2117 \tTraining Loss: 0.034862\n",
      "Epoch: 2118 \tTraining Loss: 0.034833\n",
      "Epoch: 2119 \tTraining Loss: 0.034582\n",
      "Epoch: 2120 \tTraining Loss: 0.034729\n",
      "Epoch: 2121 \tTraining Loss: 0.035261\n",
      "Epoch: 2122 \tTraining Loss: 0.045461\n",
      "Epoch: 2123 \tTraining Loss: 0.037282\n",
      "Epoch: 2124 \tTraining Loss: 0.035619\n",
      "Epoch: 2125 \tTraining Loss: 0.034979\n",
      "Epoch: 2126 \tTraining Loss: 0.051001\n",
      "Epoch: 2127 \tTraining Loss: 0.038202\n",
      "Epoch: 2128 \tTraining Loss: 0.036192\n",
      "Epoch: 2129 \tTraining Loss: 0.037822\n",
      "Epoch: 2130 \tTraining Loss: 0.035394\n",
      "Epoch: 2131 \tTraining Loss: 0.043034\n",
      "Epoch: 2132 \tTraining Loss: 0.035017\n",
      "Epoch: 2133 \tTraining Loss: 0.036192\n",
      "Epoch: 2134 \tTraining Loss: 0.036470\n",
      "Epoch: 2135 \tTraining Loss: 0.037632\n",
      "Epoch: 2136 \tTraining Loss: 0.044545\n",
      "Epoch: 2137 \tTraining Loss: 0.044746\n",
      "Epoch: 2138 \tTraining Loss: 0.041188\n",
      "Epoch: 2139 \tTraining Loss: 0.036185\n",
      "Epoch: 2140 \tTraining Loss: 0.038759\n",
      "Epoch: 2141 \tTraining Loss: 0.035168\n",
      "Epoch: 2142 \tTraining Loss: 0.034562\n",
      "Epoch: 2143 \tTraining Loss: 0.035451\n",
      "Epoch: 2144 \tTraining Loss: 0.035467\n",
      "Epoch: 2145 \tTraining Loss: 0.034015\n",
      "Epoch: 2146 \tTraining Loss: 0.035414\n",
      "Epoch: 2147 \tTraining Loss: 0.033846\n",
      "Epoch: 2148 \tTraining Loss: 0.035839\n",
      "Epoch: 2149 \tTraining Loss: 0.034332\n",
      "Epoch: 2150 \tTraining Loss: 0.034624\n",
      "Epoch: 2151 \tTraining Loss: 0.033795\n",
      "Epoch: 2152 \tTraining Loss: 0.034003\n",
      "Epoch: 2153 \tTraining Loss: 0.034600\n",
      "Epoch: 2154 \tTraining Loss: 0.034271\n",
      "Epoch: 2155 \tTraining Loss: 0.035252\n",
      "Epoch: 2156 \tTraining Loss: 0.036185\n",
      "Epoch: 2157 \tTraining Loss: 0.036185\n",
      "Epoch: 2158 \tTraining Loss: 0.044440\n",
      "Epoch: 2159 \tTraining Loss: 0.039622\n",
      "Epoch: 2160 \tTraining Loss: 0.040996\n",
      "Epoch: 2161 \tTraining Loss: 0.039686\n",
      "Epoch: 2162 \tTraining Loss: 0.041550\n",
      "Epoch: 2163 \tTraining Loss: 0.050864\n",
      "Epoch: 2164 \tTraining Loss: 0.056045\n",
      "Epoch: 2165 \tTraining Loss: 0.115723\n",
      "Epoch: 2166 \tTraining Loss: 0.188659\n",
      "Epoch: 2167 \tTraining Loss: 0.066727\n",
      "Epoch: 2168 \tTraining Loss: 0.057886\n",
      "Epoch: 2169 \tTraining Loss: 0.046874\n",
      "Epoch: 2170 \tTraining Loss: 0.044028\n",
      "Epoch: 2171 \tTraining Loss: 0.044498\n",
      "Epoch: 2172 \tTraining Loss: 0.045117\n",
      "Epoch: 2173 \tTraining Loss: 0.044224\n",
      "Epoch: 2174 \tTraining Loss: 0.044979\n",
      "Epoch: 2175 \tTraining Loss: 0.044740\n",
      "Epoch: 2176 \tTraining Loss: 0.043631\n",
      "Epoch: 2177 \tTraining Loss: 0.042184\n",
      "Epoch: 2178 \tTraining Loss: 0.044370\n",
      "Epoch: 2179 \tTraining Loss: 0.042629\n",
      "Epoch: 2180 \tTraining Loss: 0.041921\n",
      "Epoch: 2181 \tTraining Loss: 0.041069\n",
      "Epoch: 2182 \tTraining Loss: 0.040017\n",
      "Epoch: 2183 \tTraining Loss: 0.040856\n",
      "Epoch: 2184 \tTraining Loss: 0.040433\n",
      "Epoch: 2185 \tTraining Loss: 0.041498\n",
      "Epoch: 2186 \tTraining Loss: 0.039821\n",
      "Epoch: 2187 \tTraining Loss: 0.037800\n",
      "Epoch: 2188 \tTraining Loss: 0.043970\n",
      "Epoch: 2189 \tTraining Loss: 0.035305\n",
      "Epoch: 2190 \tTraining Loss: 0.035659\n",
      "Epoch: 2191 \tTraining Loss: 0.036020\n",
      "Epoch: 2192 \tTraining Loss: 0.038173\n",
      "Epoch: 2193 \tTraining Loss: 0.035081\n",
      "Epoch: 2194 \tTraining Loss: 0.037575\n",
      "Epoch: 2195 \tTraining Loss: 0.034547\n",
      "Epoch: 2196 \tTraining Loss: 0.034420\n",
      "Epoch: 2197 \tTraining Loss: 0.034133\n",
      "Epoch: 2198 \tTraining Loss: 0.037486\n",
      "Epoch: 2199 \tTraining Loss: 0.034580\n",
      "Epoch: 2200 \tTraining Loss: 0.034210\n",
      "Epoch: 2201 \tTraining Loss: 0.033408\n",
      "Epoch: 2202 \tTraining Loss: 0.033794\n",
      "Epoch: 2203 \tTraining Loss: 0.033216\n",
      "Epoch: 2204 \tTraining Loss: 0.033694\n",
      "Epoch: 2205 \tTraining Loss: 0.033456\n",
      "Epoch: 2206 \tTraining Loss: 0.033927\n",
      "Epoch: 2207 \tTraining Loss: 0.033954\n",
      "Epoch: 2208 \tTraining Loss: 0.033427\n",
      "Epoch: 2209 \tTraining Loss: 0.034506\n",
      "Epoch: 2210 \tTraining Loss: 0.033213\n",
      "Epoch: 2211 \tTraining Loss: 0.033747\n",
      "Epoch: 2212 \tTraining Loss: 0.033117\n",
      "Epoch: 2213 \tTraining Loss: 0.034407\n",
      "Epoch: 2214 \tTraining Loss: 0.037757\n",
      "Epoch: 2215 \tTraining Loss: 0.038073\n",
      "Epoch: 2216 \tTraining Loss: 0.042338\n",
      "Epoch: 2217 \tTraining Loss: 0.086893\n",
      "Epoch: 2218 \tTraining Loss: 0.073334\n",
      "Epoch: 2219 \tTraining Loss: 0.083359\n",
      "Epoch: 2220 \tTraining Loss: 0.058793\n",
      "Epoch: 2221 \tTraining Loss: 0.063518\n",
      "Epoch: 2222 \tTraining Loss: 0.102580\n",
      "Epoch: 2223 \tTraining Loss: 0.048834\n",
      "Epoch: 2224 \tTraining Loss: 0.045233\n",
      "Epoch: 2225 \tTraining Loss: 0.037946\n",
      "Epoch: 2226 \tTraining Loss: 0.038947\n",
      "Epoch: 2227 \tTraining Loss: 0.037613\n",
      "Epoch: 2228 \tTraining Loss: 0.039396\n",
      "Epoch: 2229 \tTraining Loss: 0.038213\n",
      "Epoch: 2230 \tTraining Loss: 0.048890\n",
      "Epoch: 2231 \tTraining Loss: 0.046901\n",
      "Epoch: 2232 \tTraining Loss: 0.046551\n",
      "Epoch: 2233 \tTraining Loss: 0.037186\n",
      "Epoch: 2234 \tTraining Loss: 0.036850\n",
      "Epoch: 2235 \tTraining Loss: 0.035920\n",
      "Epoch: 2236 \tTraining Loss: 0.036871\n",
      "Epoch: 2237 \tTraining Loss: 0.035953\n",
      "Epoch: 2238 \tTraining Loss: 0.035708\n",
      "Epoch: 2239 \tTraining Loss: 0.035719\n",
      "Epoch: 2240 \tTraining Loss: 0.035941\n",
      "Epoch: 2241 \tTraining Loss: 0.035175\n",
      "Epoch: 2242 \tTraining Loss: 0.034091\n",
      "Epoch: 2243 \tTraining Loss: 0.033766\n",
      "Epoch: 2244 \tTraining Loss: 0.033266\n",
      "Epoch: 2245 \tTraining Loss: 0.033168\n",
      "Epoch: 2246 \tTraining Loss: 0.033082\n",
      "Epoch: 2247 \tTraining Loss: 0.032577\n",
      "Epoch: 2248 \tTraining Loss: 0.032790\n",
      "Epoch: 2249 \tTraining Loss: 0.032811\n",
      "Epoch: 2250 \tTraining Loss: 0.032599\n",
      "Epoch: 2251 \tTraining Loss: 0.033328\n",
      "Epoch: 2252 \tTraining Loss: 0.038453\n",
      "Epoch: 2253 \tTraining Loss: 0.032799\n",
      "Epoch: 2254 \tTraining Loss: 0.034065\n",
      "Epoch: 2255 \tTraining Loss: 0.034010\n",
      "Epoch: 2256 \tTraining Loss: 0.036047\n",
      "Epoch: 2257 \tTraining Loss: 0.033980\n",
      "Epoch: 2258 \tTraining Loss: 0.033187\n",
      "Epoch: 2259 \tTraining Loss: 0.033276\n",
      "Epoch: 2260 \tTraining Loss: 0.034118\n",
      "Epoch: 2261 \tTraining Loss: 0.041717\n",
      "Epoch: 2262 \tTraining Loss: 0.038393\n",
      "Epoch: 2263 \tTraining Loss: 0.038097\n",
      "Epoch: 2264 \tTraining Loss: 0.034075\n",
      "Epoch: 2265 \tTraining Loss: 0.033575\n",
      "Epoch: 2266 \tTraining Loss: 0.034619\n",
      "Epoch: 2267 \tTraining Loss: 0.033187\n",
      "Epoch: 2268 \tTraining Loss: 0.032256\n",
      "Epoch: 2269 \tTraining Loss: 0.032738\n",
      "Epoch: 2270 \tTraining Loss: 0.032059\n",
      "Epoch: 2271 \tTraining Loss: 0.031936\n",
      "Epoch: 2272 \tTraining Loss: 0.032310\n",
      "Epoch: 2273 \tTraining Loss: 0.031966\n",
      "Epoch: 2274 \tTraining Loss: 0.032714\n",
      "Epoch: 2275 \tTraining Loss: 0.032153\n",
      "Epoch: 2276 \tTraining Loss: 0.031788\n",
      "Epoch: 2277 \tTraining Loss: 0.032169\n",
      "Epoch: 2278 \tTraining Loss: 0.031938\n",
      "Epoch: 2279 \tTraining Loss: 0.032207\n",
      "Epoch: 2280 \tTraining Loss: 0.031695\n",
      "Epoch: 2281 \tTraining Loss: 0.032153\n",
      "Epoch: 2282 \tTraining Loss: 0.031698\n",
      "Epoch: 2283 \tTraining Loss: 0.031768\n",
      "Epoch: 2284 \tTraining Loss: 0.032665\n",
      "Epoch: 2285 \tTraining Loss: 0.032338\n",
      "Epoch: 2286 \tTraining Loss: 0.031961\n",
      "Epoch: 2287 \tTraining Loss: 0.031619\n",
      "Epoch: 2288 \tTraining Loss: 0.031827\n",
      "Epoch: 2289 \tTraining Loss: 0.031870\n",
      "Epoch: 2290 \tTraining Loss: 0.031985\n",
      "Epoch: 2291 \tTraining Loss: 0.031672\n",
      "Epoch: 2292 \tTraining Loss: 0.031920\n",
      "Epoch: 2293 \tTraining Loss: 0.031642\n",
      "Epoch: 2294 \tTraining Loss: 0.031555\n",
      "Epoch: 2295 \tTraining Loss: 0.031602\n",
      "Epoch: 2296 \tTraining Loss: 0.031837\n",
      "Epoch: 2297 \tTraining Loss: 0.031523\n",
      "Epoch: 2298 \tTraining Loss: 0.032724\n",
      "Epoch: 2299 \tTraining Loss: 0.031648\n",
      "Epoch: 2300 \tTraining Loss: 0.032136\n",
      "Epoch: 2301 \tTraining Loss: 0.032095\n",
      "Epoch: 2302 \tTraining Loss: 0.032250\n",
      "Epoch: 2303 \tTraining Loss: 0.032575\n",
      "Epoch: 2304 \tTraining Loss: 0.031808\n",
      "Epoch: 2305 \tTraining Loss: 0.031460\n",
      "Epoch: 2306 \tTraining Loss: 0.031510\n",
      "Epoch: 2307 \tTraining Loss: 0.031660\n",
      "Epoch: 2308 \tTraining Loss: 0.031352\n",
      "Epoch: 2309 \tTraining Loss: 0.032103\n",
      "Epoch: 2310 \tTraining Loss: 0.031810\n",
      "Epoch: 2311 \tTraining Loss: 0.031954\n",
      "Epoch: 2312 \tTraining Loss: 0.036176\n",
      "Epoch: 2313 \tTraining Loss: 0.032812\n",
      "Epoch: 2314 \tTraining Loss: 0.031982\n",
      "Epoch: 2315 \tTraining Loss: 0.031879\n",
      "Epoch: 2316 \tTraining Loss: 0.031789\n",
      "Epoch: 2317 \tTraining Loss: 0.031162\n",
      "Epoch: 2318 \tTraining Loss: 0.032167\n",
      "Epoch: 2319 \tTraining Loss: 0.031466\n",
      "Epoch: 2320 \tTraining Loss: 0.032507\n",
      "Epoch: 2321 \tTraining Loss: 0.031530\n",
      "Epoch: 2322 \tTraining Loss: 0.031351\n",
      "Epoch: 2323 \tTraining Loss: 0.031548\n",
      "Epoch: 2324 \tTraining Loss: 0.031428\n",
      "Epoch: 2325 \tTraining Loss: 0.031228\n",
      "Epoch: 2326 \tTraining Loss: 0.031288\n",
      "Epoch: 2327 \tTraining Loss: 0.031235\n",
      "Epoch: 2328 \tTraining Loss: 0.031199\n",
      "Epoch: 2329 \tTraining Loss: 0.031784\n",
      "Epoch: 2330 \tTraining Loss: 0.031069\n",
      "Epoch: 2331 \tTraining Loss: 0.031468\n",
      "Epoch: 2332 \tTraining Loss: 0.031145\n",
      "Epoch: 2333 \tTraining Loss: 0.031227\n",
      "Epoch: 2334 \tTraining Loss: 0.031194\n",
      "Epoch: 2335 \tTraining Loss: 0.032063\n",
      "Epoch: 2336 \tTraining Loss: 0.031004\n",
      "Epoch: 2337 \tTraining Loss: 0.031600\n",
      "Epoch: 2338 \tTraining Loss: 0.031597\n",
      "Epoch: 2339 \tTraining Loss: 0.031420\n",
      "Epoch: 2340 \tTraining Loss: 0.032573\n",
      "Epoch: 2341 \tTraining Loss: 0.040067\n",
      "Epoch: 2342 \tTraining Loss: 0.083574\n",
      "Epoch: 2343 \tTraining Loss: 0.165154\n",
      "Epoch: 2344 \tTraining Loss: 0.261574\n",
      "Epoch: 2345 \tTraining Loss: 0.270743\n",
      "Epoch: 2346 \tTraining Loss: 0.111967\n",
      "Epoch: 2347 \tTraining Loss: 0.083010\n",
      "Epoch: 2348 \tTraining Loss: 0.057784\n",
      "Epoch: 2349 \tTraining Loss: 0.049792\n",
      "Epoch: 2350 \tTraining Loss: 0.049727\n",
      "Epoch: 2351 \tTraining Loss: 0.045749\n",
      "Epoch: 2352 \tTraining Loss: 0.044914\n",
      "Epoch: 2353 \tTraining Loss: 0.044343\n",
      "Epoch: 2354 \tTraining Loss: 0.040710\n",
      "Epoch: 2355 \tTraining Loss: 0.038016\n",
      "Epoch: 2356 \tTraining Loss: 0.040428\n",
      "Epoch: 2357 \tTraining Loss: 0.039550\n",
      "Epoch: 2358 \tTraining Loss: 0.038667\n",
      "Epoch: 2359 \tTraining Loss: 0.036669\n",
      "Epoch: 2360 \tTraining Loss: 0.039846\n",
      "Epoch: 2361 \tTraining Loss: 0.039047\n",
      "Epoch: 2362 \tTraining Loss: 0.038896\n",
      "Epoch: 2363 \tTraining Loss: 0.038349\n",
      "Epoch: 2364 \tTraining Loss: 0.038331\n",
      "Epoch: 2365 \tTraining Loss: 0.038360\n",
      "Epoch: 2366 \tTraining Loss: 0.038543\n",
      "Epoch: 2367 \tTraining Loss: 0.038561\n",
      "Epoch: 2368 \tTraining Loss: 0.038138\n",
      "Epoch: 2369 \tTraining Loss: 0.035903\n",
      "Epoch: 2370 \tTraining Loss: 0.036625\n",
      "Epoch: 2371 \tTraining Loss: 0.040023\n",
      "Epoch: 2372 \tTraining Loss: 0.057646\n",
      "Epoch: 2373 \tTraining Loss: 0.063758\n",
      "Epoch: 2374 \tTraining Loss: 0.055017\n",
      "Epoch: 2375 \tTraining Loss: 0.049563\n",
      "Epoch: 2376 \tTraining Loss: 0.042028\n",
      "Epoch: 2377 \tTraining Loss: 0.046586\n",
      "Epoch: 2378 \tTraining Loss: 0.037276\n",
      "Epoch: 2379 \tTraining Loss: 0.037090\n",
      "Epoch: 2380 \tTraining Loss: 0.035763\n",
      "Epoch: 2381 \tTraining Loss: 0.035147\n",
      "Epoch: 2382 \tTraining Loss: 0.035142\n",
      "Epoch: 2383 \tTraining Loss: 0.035694\n",
      "Epoch: 2384 \tTraining Loss: 0.034651\n",
      "Epoch: 2385 \tTraining Loss: 0.034799\n",
      "Epoch: 2386 \tTraining Loss: 0.034751\n",
      "Epoch: 2387 \tTraining Loss: 0.034292\n",
      "Epoch: 2388 \tTraining Loss: 0.034205\n",
      "Epoch: 2389 \tTraining Loss: 0.034204\n",
      "Epoch: 2390 \tTraining Loss: 0.033721\n",
      "Epoch: 2391 \tTraining Loss: 0.034063\n",
      "Epoch: 2392 \tTraining Loss: 0.034375\n",
      "Epoch: 2393 \tTraining Loss: 0.038125\n",
      "Epoch: 2394 \tTraining Loss: 0.036435\n",
      "Epoch: 2395 \tTraining Loss: 0.034126\n",
      "Epoch: 2396 \tTraining Loss: 0.033734\n",
      "Epoch: 2397 \tTraining Loss: 0.033799\n",
      "Epoch: 2398 \tTraining Loss: 0.033632\n",
      "Epoch: 2399 \tTraining Loss: 0.034217\n",
      "Epoch: 2400 \tTraining Loss: 0.034446\n",
      "Epoch: 2401 \tTraining Loss: 0.034105\n",
      "Epoch: 2402 \tTraining Loss: 0.033786\n",
      "Epoch: 2403 \tTraining Loss: 0.033986\n",
      "Epoch: 2404 \tTraining Loss: 0.034662\n",
      "Epoch: 2405 \tTraining Loss: 0.034366\n",
      "Epoch: 2406 \tTraining Loss: 0.033490\n",
      "Epoch: 2407 \tTraining Loss: 0.033522\n",
      "Epoch: 2408 \tTraining Loss: 0.033366\n",
      "Epoch: 2409 \tTraining Loss: 0.033433\n",
      "Epoch: 2410 \tTraining Loss: 0.033425\n",
      "Epoch: 2411 \tTraining Loss: 0.033205\n",
      "Epoch: 2412 \tTraining Loss: 0.034315\n",
      "Epoch: 2413 \tTraining Loss: 0.033119\n",
      "Epoch: 2414 \tTraining Loss: 0.033511\n",
      "Epoch: 2415 \tTraining Loss: 0.035544\n",
      "Epoch: 2416 \tTraining Loss: 0.034681\n",
      "Epoch: 2417 \tTraining Loss: 0.032918\n",
      "Epoch: 2418 \tTraining Loss: 0.033237\n",
      "Epoch: 2419 \tTraining Loss: 0.033084\n",
      "Epoch: 2420 \tTraining Loss: 0.033123\n",
      "Epoch: 2421 \tTraining Loss: 0.032979\n",
      "Epoch: 2422 \tTraining Loss: 0.032959\n",
      "Epoch: 2423 \tTraining Loss: 0.032955\n",
      "Epoch: 2424 \tTraining Loss: 0.032857\n",
      "Epoch: 2425 \tTraining Loss: 0.032989\n",
      "Epoch: 2426 \tTraining Loss: 0.033147\n",
      "Epoch: 2427 \tTraining Loss: 0.033013\n",
      "Epoch: 2428 \tTraining Loss: 0.033005\n",
      "Epoch: 2429 \tTraining Loss: 0.033027\n",
      "Epoch: 2430 \tTraining Loss: 0.032866\n",
      "Epoch: 2431 \tTraining Loss: 0.032972\n",
      "Epoch: 2432 \tTraining Loss: 0.032884\n",
      "Epoch: 2433 \tTraining Loss: 0.032788\n",
      "Epoch: 2434 \tTraining Loss: 0.032868\n",
      "Epoch: 2435 \tTraining Loss: 0.033141\n",
      "Epoch: 2436 \tTraining Loss: 0.033271\n",
      "Epoch: 2437 \tTraining Loss: 0.034037\n",
      "Epoch: 2438 \tTraining Loss: 0.033044\n",
      "Epoch: 2439 \tTraining Loss: 0.032906\n",
      "Epoch: 2440 \tTraining Loss: 0.032852\n",
      "Epoch: 2441 \tTraining Loss: 0.032976\n",
      "Epoch: 2442 \tTraining Loss: 0.033007\n",
      "Epoch: 2443 \tTraining Loss: 0.034090\n",
      "Epoch: 2444 \tTraining Loss: 0.033027\n",
      "Epoch: 2445 \tTraining Loss: 0.032804\n",
      "Epoch: 2446 \tTraining Loss: 0.033544\n",
      "Epoch: 2447 \tTraining Loss: 0.035349\n",
      "Epoch: 2448 \tTraining Loss: 0.064752\n",
      "Epoch: 2449 \tTraining Loss: 0.275732\n",
      "Epoch: 2450 \tTraining Loss: 0.116791\n",
      "Epoch: 2451 \tTraining Loss: 0.256013\n",
      "Epoch: 2452 \tTraining Loss: 0.138455\n",
      "Epoch: 2453 \tTraining Loss: 0.085005\n",
      "Epoch: 2454 \tTraining Loss: 0.091439\n",
      "Epoch: 2455 \tTraining Loss: 0.073825\n",
      "Epoch: 2456 \tTraining Loss: 0.039249\n",
      "Epoch: 2457 \tTraining Loss: 0.083588\n",
      "Epoch: 2458 \tTraining Loss: 0.054651\n",
      "Epoch: 2459 \tTraining Loss: 0.042947\n",
      "Epoch: 2460 \tTraining Loss: 0.038814\n",
      "Epoch: 2461 \tTraining Loss: 0.040560\n",
      "Epoch: 2462 \tTraining Loss: 0.035307\n",
      "Epoch: 2463 \tTraining Loss: 0.036041\n",
      "Epoch: 2464 \tTraining Loss: 0.035663\n",
      "Epoch: 2465 \tTraining Loss: 0.035251\n",
      "Epoch: 2466 \tTraining Loss: 0.035948\n",
      "Epoch: 2467 \tTraining Loss: 0.034500\n",
      "Epoch: 2468 \tTraining Loss: 0.034366\n",
      "Epoch: 2469 \tTraining Loss: 0.034180\n",
      "Epoch: 2470 \tTraining Loss: 0.035136\n",
      "Epoch: 2471 \tTraining Loss: 0.034306\n",
      "Epoch: 2472 \tTraining Loss: 0.034321\n",
      "Epoch: 2473 \tTraining Loss: 0.034781\n",
      "Epoch: 2474 \tTraining Loss: 0.034663\n",
      "Epoch: 2475 \tTraining Loss: 0.034739\n",
      "Epoch: 2476 \tTraining Loss: 0.034102\n",
      "Epoch: 2477 \tTraining Loss: 0.034859\n",
      "Epoch: 2478 \tTraining Loss: 0.034604\n",
      "Epoch: 2479 \tTraining Loss: 0.034263\n",
      "Epoch: 2480 \tTraining Loss: 0.034296\n",
      "Epoch: 2481 \tTraining Loss: 0.034141\n",
      "Epoch: 2482 \tTraining Loss: 0.034294\n",
      "Epoch: 2483 \tTraining Loss: 0.033778\n",
      "Epoch: 2484 \tTraining Loss: 0.034742\n",
      "Epoch: 2485 \tTraining Loss: 0.034475\n",
      "Epoch: 2486 \tTraining Loss: 0.036446\n",
      "Epoch: 2487 \tTraining Loss: 0.034739\n",
      "Epoch: 2488 \tTraining Loss: 0.033906\n",
      "Epoch: 2489 \tTraining Loss: 0.033648\n",
      "Epoch: 2490 \tTraining Loss: 0.033636\n",
      "Epoch: 2491 \tTraining Loss: 0.033954\n",
      "Epoch: 2492 \tTraining Loss: 0.034238\n",
      "Epoch: 2493 \tTraining Loss: 0.033921\n",
      "Epoch: 2494 \tTraining Loss: 0.033745\n",
      "Epoch: 2495 \tTraining Loss: 0.033515\n",
      "Epoch: 2496 \tTraining Loss: 0.034058\n",
      "Epoch: 2497 \tTraining Loss: 0.033668\n",
      "Epoch: 2498 \tTraining Loss: 0.034021\n",
      "Epoch: 2499 \tTraining Loss: 0.033725\n",
      "Epoch: 2500 \tTraining Loss: 0.033948\n",
      "Epoch: 2501 \tTraining Loss: 0.033874\n",
      "Epoch: 2502 \tTraining Loss: 0.033771\n",
      "Epoch: 2503 \tTraining Loss: 0.033592\n",
      "Epoch: 2504 \tTraining Loss: 0.033662\n",
      "Epoch: 2505 \tTraining Loss: 0.033484\n",
      "Epoch: 2506 \tTraining Loss: 0.033737\n",
      "Epoch: 2507 \tTraining Loss: 0.033607\n",
      "Epoch: 2508 \tTraining Loss: 0.034346\n",
      "Epoch: 2509 \tTraining Loss: 0.033482\n",
      "Epoch: 2510 \tTraining Loss: 0.033995\n",
      "Epoch: 2511 \tTraining Loss: 0.033787\n",
      "Epoch: 2512 \tTraining Loss: 0.033719\n",
      "Epoch: 2513 \tTraining Loss: 0.033312\n",
      "Epoch: 2514 \tTraining Loss: 0.033690\n",
      "Epoch: 2515 \tTraining Loss: 0.033660\n",
      "Epoch: 2516 \tTraining Loss: 0.033296\n",
      "Epoch: 2517 \tTraining Loss: 0.033424\n",
      "Epoch: 2518 \tTraining Loss: 0.033743\n",
      "Epoch: 2519 \tTraining Loss: 0.033353\n",
      "Epoch: 2520 \tTraining Loss: 0.033560\n",
      "Epoch: 2521 \tTraining Loss: 0.033391\n",
      "Epoch: 2522 \tTraining Loss: 0.033346\n",
      "Epoch: 2523 \tTraining Loss: 0.033752\n",
      "Epoch: 2524 \tTraining Loss: 0.038631\n",
      "Epoch: 2525 \tTraining Loss: 0.033508\n",
      "Epoch: 2526 \tTraining Loss: 0.033716\n",
      "Epoch: 2527 \tTraining Loss: 0.033852\n",
      "Epoch: 2528 \tTraining Loss: 0.041520\n",
      "Epoch: 2529 \tTraining Loss: 0.033620\n",
      "Epoch: 2530 \tTraining Loss: 0.034103\n",
      "Epoch: 2531 \tTraining Loss: 0.033505\n",
      "Epoch: 2532 \tTraining Loss: 0.033252\n",
      "Epoch: 2533 \tTraining Loss: 0.033482\n",
      "Epoch: 2534 \tTraining Loss: 0.035207\n",
      "Epoch: 2535 \tTraining Loss: 0.034023\n",
      "Epoch: 2536 \tTraining Loss: 0.033729\n",
      "Epoch: 2537 \tTraining Loss: 0.032886\n",
      "Epoch: 2538 \tTraining Loss: 0.035171\n",
      "Epoch: 2539 \tTraining Loss: 0.034112\n",
      "Epoch: 2540 \tTraining Loss: 0.035233\n",
      "Epoch: 2541 \tTraining Loss: 0.034124\n",
      "Epoch: 2542 \tTraining Loss: 0.033692\n",
      "Epoch: 2543 \tTraining Loss: 0.033781\n",
      "Epoch: 2544 \tTraining Loss: 0.033511\n",
      "Epoch: 2545 \tTraining Loss: 0.034438\n",
      "Epoch: 2546 \tTraining Loss: 0.033625\n",
      "Epoch: 2547 \tTraining Loss: 0.033097\n",
      "Epoch: 2548 \tTraining Loss: 0.033087\n",
      "Epoch: 2549 \tTraining Loss: 0.033034\n",
      "Epoch: 2550 \tTraining Loss: 0.033159\n",
      "Epoch: 2551 \tTraining Loss: 0.033184\n",
      "Epoch: 2552 \tTraining Loss: 0.033003\n",
      "Epoch: 2553 \tTraining Loss: 0.033614\n",
      "Epoch: 2554 \tTraining Loss: 0.033140\n",
      "Epoch: 2555 \tTraining Loss: 0.033210\n",
      "Epoch: 2556 \tTraining Loss: 0.033015\n",
      "Epoch: 2557 \tTraining Loss: 0.033224\n",
      "Epoch: 2558 \tTraining Loss: 0.033331\n",
      "Epoch: 2559 \tTraining Loss: 0.033591\n",
      "Epoch: 2560 \tTraining Loss: 0.032959\n",
      "Epoch: 2561 \tTraining Loss: 0.033302\n",
      "Epoch: 2562 \tTraining Loss: 0.032920\n",
      "Epoch: 2563 \tTraining Loss: 0.033008\n",
      "Epoch: 2564 \tTraining Loss: 0.033696\n",
      "Epoch: 2565 \tTraining Loss: 0.033665\n",
      "Epoch: 2566 \tTraining Loss: 0.033424\n",
      "Epoch: 2567 \tTraining Loss: 0.033691\n",
      "Epoch: 2568 \tTraining Loss: 0.033460\n",
      "Epoch: 2569 \tTraining Loss: 0.033483\n",
      "Epoch: 2570 \tTraining Loss: 0.033878\n",
      "Epoch: 2571 \tTraining Loss: 0.034154\n",
      "Epoch: 2572 \tTraining Loss: 0.034765\n",
      "Epoch: 2573 \tTraining Loss: 0.034968\n",
      "Epoch: 2574 \tTraining Loss: 0.034833\n",
      "Epoch: 2575 \tTraining Loss: 0.036447\n",
      "Epoch: 2576 \tTraining Loss: 0.034904\n",
      "Epoch: 2577 \tTraining Loss: 0.035014\n",
      "Epoch: 2578 \tTraining Loss: 0.034722\n",
      "Epoch: 2579 \tTraining Loss: 0.034794\n",
      "Epoch: 2580 \tTraining Loss: 0.035201\n",
      "Epoch: 2581 \tTraining Loss: 0.034236\n",
      "Epoch: 2582 \tTraining Loss: 0.035799\n",
      "Epoch: 2583 \tTraining Loss: 0.034752\n",
      "Epoch: 2584 \tTraining Loss: 0.034726\n",
      "Epoch: 2585 \tTraining Loss: 0.034804\n",
      "Epoch: 2586 \tTraining Loss: 0.034429\n",
      "Epoch: 2587 \tTraining Loss: 0.036183\n",
      "Epoch: 2588 \tTraining Loss: 0.035331\n",
      "Epoch: 2589 \tTraining Loss: 0.034945\n",
      "Epoch: 2590 \tTraining Loss: 0.034727\n",
      "Epoch: 2591 \tTraining Loss: 0.034554\n",
      "Epoch: 2592 \tTraining Loss: 0.034593\n",
      "Epoch: 2593 \tTraining Loss: 0.034552\n",
      "Epoch: 2594 \tTraining Loss: 0.034623\n",
      "Epoch: 2595 \tTraining Loss: 0.034439\n",
      "Epoch: 2596 \tTraining Loss: 0.053841\n",
      "Epoch: 2597 \tTraining Loss: 0.036655\n",
      "Epoch: 2598 \tTraining Loss: 0.035332\n",
      "Epoch: 2599 \tTraining Loss: 0.034789\n",
      "Epoch: 2600 \tTraining Loss: 0.034651\n",
      "Epoch: 2601 \tTraining Loss: 0.034569\n",
      "Epoch: 2602 \tTraining Loss: 0.034607\n",
      "Epoch: 2603 \tTraining Loss: 0.034396\n",
      "Epoch: 2604 \tTraining Loss: 0.034690\n",
      "Epoch: 2605 \tTraining Loss: 0.034792\n",
      "Epoch: 2606 \tTraining Loss: 0.034822\n",
      "Epoch: 2607 \tTraining Loss: 0.034659\n",
      "Epoch: 2608 \tTraining Loss: 0.035802\n",
      "Epoch: 2609 \tTraining Loss: 0.035514\n",
      "Epoch: 2610 \tTraining Loss: 0.043652\n",
      "Epoch: 2611 \tTraining Loss: 0.084157\n",
      "Epoch: 2612 \tTraining Loss: 0.180148\n",
      "Epoch: 2613 \tTraining Loss: 0.268320\n",
      "Epoch: 2614 \tTraining Loss: 0.225253\n",
      "Epoch: 2615 \tTraining Loss: 0.229918\n",
      "Epoch: 2616 \tTraining Loss: 0.087153\n",
      "Epoch: 2617 \tTraining Loss: 0.071659\n",
      "Epoch: 2618 \tTraining Loss: 0.076516\n",
      "Epoch: 2619 \tTraining Loss: 0.051897\n",
      "Epoch: 2620 \tTraining Loss: 0.052324\n",
      "Epoch: 2621 \tTraining Loss: 0.042632\n",
      "Epoch: 2622 \tTraining Loss: 0.035868\n",
      "Epoch: 2623 \tTraining Loss: 0.033763\n",
      "Epoch: 2624 \tTraining Loss: 0.033499\n",
      "Epoch: 2625 \tTraining Loss: 0.033059\n",
      "Epoch: 2626 \tTraining Loss: 0.033573\n",
      "Epoch: 2627 \tTraining Loss: 0.032977\n",
      "Epoch: 2628 \tTraining Loss: 0.033639\n",
      "Epoch: 2629 \tTraining Loss: 0.032185\n",
      "Epoch: 2630 \tTraining Loss: 0.032684\n",
      "Epoch: 2631 \tTraining Loss: 0.032469\n",
      "Epoch: 2632 \tTraining Loss: 0.031952\n",
      "Epoch: 2633 \tTraining Loss: 0.032461\n",
      "Epoch: 2634 \tTraining Loss: 0.031813\n",
      "Epoch: 2635 \tTraining Loss: 0.031921\n",
      "Epoch: 2636 \tTraining Loss: 0.031872\n",
      "Epoch: 2637 \tTraining Loss: 0.032355\n",
      "Epoch: 2638 \tTraining Loss: 0.032196\n",
      "Epoch: 2639 \tTraining Loss: 0.031472\n",
      "Epoch: 2640 \tTraining Loss: 0.032068\n",
      "Epoch: 2641 \tTraining Loss: 0.031268\n",
      "Epoch: 2642 \tTraining Loss: 0.032153\n",
      "Epoch: 2643 \tTraining Loss: 0.031662\n",
      "Epoch: 2644 \tTraining Loss: 0.031587\n",
      "Epoch: 2645 \tTraining Loss: 0.032165\n",
      "Epoch: 2646 \tTraining Loss: 0.031598\n",
      "Epoch: 2647 \tTraining Loss: 0.031307\n",
      "Epoch: 2648 \tTraining Loss: 0.031367\n",
      "Epoch: 2649 \tTraining Loss: 0.031286\n",
      "Epoch: 2650 \tTraining Loss: 0.031203\n",
      "Epoch: 2651 \tTraining Loss: 0.031496\n",
      "Epoch: 2652 \tTraining Loss: 0.031462\n",
      "Epoch: 2653 \tTraining Loss: 0.031263\n",
      "Epoch: 2654 \tTraining Loss: 0.031311\n",
      "Epoch: 2655 \tTraining Loss: 0.031172\n",
      "Epoch: 2656 \tTraining Loss: 0.031219\n",
      "Epoch: 2657 \tTraining Loss: 0.031364\n",
      "Epoch: 2658 \tTraining Loss: 0.031420\n",
      "Epoch: 2659 \tTraining Loss: 0.031218\n",
      "Epoch: 2660 \tTraining Loss: 0.031539\n",
      "Epoch: 2661 \tTraining Loss: 0.031823\n",
      "Epoch: 2662 \tTraining Loss: 0.031400\n",
      "Epoch: 2663 \tTraining Loss: 0.031401\n",
      "Epoch: 2664 \tTraining Loss: 0.031305\n",
      "Epoch: 2665 \tTraining Loss: 0.031329\n",
      "Epoch: 2666 \tTraining Loss: 0.031184\n",
      "Epoch: 2667 \tTraining Loss: 0.031306\n",
      "Epoch: 2668 \tTraining Loss: 0.031121\n",
      "Epoch: 2669 \tTraining Loss: 0.031038\n",
      "Epoch: 2670 \tTraining Loss: 0.031247\n",
      "Epoch: 2671 \tTraining Loss: 0.031157\n",
      "Epoch: 2672 \tTraining Loss: 0.031091\n",
      "Epoch: 2673 \tTraining Loss: 0.031050\n",
      "Epoch: 2674 \tTraining Loss: 0.031111\n",
      "Epoch: 2675 \tTraining Loss: 0.031176\n",
      "Epoch: 2676 \tTraining Loss: 0.031327\n",
      "Epoch: 2677 \tTraining Loss: 0.030802\n",
      "Epoch: 2678 \tTraining Loss: 0.031465\n",
      "Epoch: 2679 \tTraining Loss: 0.031237\n",
      "Epoch: 2680 \tTraining Loss: 0.031418\n",
      "Epoch: 2681 \tTraining Loss: 0.033912\n",
      "Epoch: 2682 \tTraining Loss: 0.030987\n",
      "Epoch: 2683 \tTraining Loss: 0.031073\n",
      "Epoch: 2684 \tTraining Loss: 0.031104\n",
      "Epoch: 2685 \tTraining Loss: 0.031406\n",
      "Epoch: 2686 \tTraining Loss: 0.031222\n",
      "Epoch: 2687 \tTraining Loss: 0.031140\n",
      "Epoch: 2688 \tTraining Loss: 0.031287\n",
      "Epoch: 2689 \tTraining Loss: 0.031356\n",
      "Epoch: 2690 \tTraining Loss: 0.030967\n",
      "Epoch: 2691 \tTraining Loss: 0.030981\n",
      "Epoch: 2692 \tTraining Loss: 0.030890\n",
      "Epoch: 2693 \tTraining Loss: 0.030876\n",
      "Epoch: 2694 \tTraining Loss: 0.030929\n",
      "Epoch: 2695 \tTraining Loss: 0.030819\n",
      "Epoch: 2696 \tTraining Loss: 0.030988\n",
      "Epoch: 2697 \tTraining Loss: 0.031302\n",
      "Epoch: 2698 \tTraining Loss: 0.030931\n",
      "Epoch: 2699 \tTraining Loss: 0.030754\n",
      "Epoch: 2700 \tTraining Loss: 0.031032\n",
      "Epoch: 2701 \tTraining Loss: 0.030967\n",
      "Epoch: 2702 \tTraining Loss: 0.030957\n",
      "Epoch: 2703 \tTraining Loss: 0.030773\n",
      "Epoch: 2704 \tTraining Loss: 0.038187\n",
      "Epoch: 2705 \tTraining Loss: 0.132403\n",
      "Epoch: 2706 \tTraining Loss: 0.186205\n",
      "Epoch: 2707 \tTraining Loss: 0.190105\n",
      "Epoch: 2708 \tTraining Loss: 0.191244\n",
      "Epoch: 2709 \tTraining Loss: 0.114129\n",
      "Epoch: 2710 \tTraining Loss: 0.053568\n",
      "Epoch: 2711 \tTraining Loss: 0.055212\n",
      "Epoch: 2712 \tTraining Loss: 0.051740\n",
      "Epoch: 2713 \tTraining Loss: 0.046097\n",
      "Epoch: 2714 \tTraining Loss: 0.046944\n",
      "Epoch: 2715 \tTraining Loss: 0.045439\n",
      "Epoch: 2716 \tTraining Loss: 0.042776\n",
      "Epoch: 2717 \tTraining Loss: 0.039582\n",
      "Epoch: 2718 \tTraining Loss: 0.039067\n",
      "Epoch: 2719 \tTraining Loss: 0.039050\n",
      "Epoch: 2720 \tTraining Loss: 0.038938\n",
      "Epoch: 2721 \tTraining Loss: 0.039116\n",
      "Epoch: 2722 \tTraining Loss: 0.039675\n",
      "Epoch: 2723 \tTraining Loss: 0.039436\n",
      "Epoch: 2724 \tTraining Loss: 0.039740\n",
      "Epoch: 2725 \tTraining Loss: 0.038848\n",
      "Epoch: 2726 \tTraining Loss: 0.038737\n",
      "Epoch: 2727 \tTraining Loss: 0.038730\n",
      "Epoch: 2728 \tTraining Loss: 0.038611\n",
      "Epoch: 2729 \tTraining Loss: 0.038630\n",
      "Epoch: 2730 \tTraining Loss: 0.038672\n",
      "Epoch: 2731 \tTraining Loss: 0.038703\n",
      "Epoch: 2732 \tTraining Loss: 0.038484\n",
      "Epoch: 2733 \tTraining Loss: 0.038459\n",
      "Epoch: 2734 \tTraining Loss: 0.038834\n",
      "Epoch: 2735 \tTraining Loss: 0.039150\n",
      "Epoch: 2736 \tTraining Loss: 0.038300\n",
      "Epoch: 2737 \tTraining Loss: 0.038928\n",
      "Epoch: 2738 \tTraining Loss: 0.038386\n",
      "Epoch: 2739 \tTraining Loss: 0.038583\n",
      "Epoch: 2740 \tTraining Loss: 0.041572\n",
      "Epoch: 2741 \tTraining Loss: 0.037861\n",
      "Epoch: 2742 \tTraining Loss: 0.038869\n",
      "Epoch: 2743 \tTraining Loss: 0.038055\n",
      "Epoch: 2744 \tTraining Loss: 0.038049\n",
      "Epoch: 2745 \tTraining Loss: 0.038922\n",
      "Epoch: 2746 \tTraining Loss: 0.034955\n",
      "Epoch: 2747 \tTraining Loss: 0.036636\n",
      "Epoch: 2748 \tTraining Loss: 0.036402\n",
      "Epoch: 2749 \tTraining Loss: 0.036474\n",
      "Epoch: 2750 \tTraining Loss: 0.036286\n",
      "Epoch: 2751 \tTraining Loss: 0.036320\n",
      "Epoch: 2752 \tTraining Loss: 0.036728\n",
      "Epoch: 2753 \tTraining Loss: 0.036299\n",
      "Epoch: 2754 \tTraining Loss: 0.036421\n",
      "Epoch: 2755 \tTraining Loss: 0.036298\n",
      "Epoch: 2756 \tTraining Loss: 0.039398\n",
      "Epoch: 2757 \tTraining Loss: 0.035539\n",
      "Epoch: 2758 \tTraining Loss: 0.035821\n",
      "Epoch: 2759 \tTraining Loss: 0.035531\n",
      "Epoch: 2760 \tTraining Loss: 0.035265\n",
      "Epoch: 2761 \tTraining Loss: 0.035445\n",
      "Epoch: 2762 \tTraining Loss: 0.035280\n",
      "Epoch: 2763 \tTraining Loss: 0.035300\n",
      "Epoch: 2764 \tTraining Loss: 0.037127\n",
      "Epoch: 2765 \tTraining Loss: 0.035053\n",
      "Epoch: 2766 \tTraining Loss: 0.035442\n",
      "Epoch: 2767 \tTraining Loss: 0.035309\n",
      "Epoch: 2768 \tTraining Loss: 0.034977\n",
      "Epoch: 2769 \tTraining Loss: 0.035166\n",
      "Epoch: 2770 \tTraining Loss: 0.034759\n",
      "Epoch: 2771 \tTraining Loss: 0.034634\n",
      "Epoch: 2772 \tTraining Loss: 0.034713\n",
      "Epoch: 2773 \tTraining Loss: 0.034627\n",
      "Epoch: 2774 \tTraining Loss: 0.034649\n",
      "Epoch: 2775 \tTraining Loss: 0.035269\n",
      "Epoch: 2776 \tTraining Loss: 0.035381\n",
      "Epoch: 2777 \tTraining Loss: 0.035452\n",
      "Epoch: 2778 \tTraining Loss: 0.035301\n",
      "Epoch: 2779 \tTraining Loss: 0.035170\n",
      "Epoch: 2780 \tTraining Loss: 0.035193\n",
      "Epoch: 2781 \tTraining Loss: 0.034609\n",
      "Epoch: 2782 \tTraining Loss: 0.034742\n",
      "Epoch: 2783 \tTraining Loss: 0.034513\n",
      "Epoch: 2784 \tTraining Loss: 0.034688\n",
      "Epoch: 2785 \tTraining Loss: 0.034428\n",
      "Epoch: 2786 \tTraining Loss: 0.034566\n",
      "Epoch: 2787 \tTraining Loss: 0.034581\n",
      "Epoch: 2788 \tTraining Loss: 0.034921\n",
      "Epoch: 2789 \tTraining Loss: 0.035243\n",
      "Epoch: 2790 \tTraining Loss: 0.034717\n",
      "Epoch: 2791 \tTraining Loss: 0.034579\n",
      "Epoch: 2792 \tTraining Loss: 0.034469\n",
      "Epoch: 2793 \tTraining Loss: 0.034473\n",
      "Epoch: 2794 \tTraining Loss: 0.034451\n",
      "Epoch: 2795 \tTraining Loss: 0.034538\n",
      "Epoch: 2796 \tTraining Loss: 0.034585\n",
      "Epoch: 2797 \tTraining Loss: 0.034643\n",
      "Epoch: 2798 \tTraining Loss: 0.034424\n",
      "Epoch: 2799 \tTraining Loss: 0.034498\n",
      "Epoch: 2800 \tTraining Loss: 0.034385\n",
      "Epoch: 2801 \tTraining Loss: 0.034776\n",
      "Epoch: 2802 \tTraining Loss: 0.034876\n",
      "Epoch: 2803 \tTraining Loss: 0.034600\n",
      "Epoch: 2804 \tTraining Loss: 0.035999\n",
      "Epoch: 2805 \tTraining Loss: 0.034729\n",
      "Epoch: 2806 \tTraining Loss: 0.035222\n",
      "Epoch: 2807 \tTraining Loss: 0.034795\n",
      "Epoch: 2808 \tTraining Loss: 0.034610\n",
      "Epoch: 2809 \tTraining Loss: 0.034575\n",
      "Epoch: 2810 \tTraining Loss: 0.034353\n",
      "Epoch: 2811 \tTraining Loss: 0.035055\n",
      "Epoch: 2812 \tTraining Loss: 0.035279\n",
      "Epoch: 2813 \tTraining Loss: 0.035168\n",
      "Epoch: 2814 \tTraining Loss: 0.034659\n",
      "Epoch: 2815 \tTraining Loss: 0.034463\n",
      "Epoch: 2816 \tTraining Loss: 0.034453\n",
      "Epoch: 2817 \tTraining Loss: 0.034660\n",
      "Epoch: 2818 \tTraining Loss: 0.034490\n",
      "Epoch: 2819 \tTraining Loss: 0.034265\n",
      "Epoch: 2820 \tTraining Loss: 0.034263\n",
      "Epoch: 2821 \tTraining Loss: 0.034249\n",
      "Epoch: 2822 \tTraining Loss: 0.034378\n",
      "Epoch: 2823 \tTraining Loss: 0.034563\n",
      "Epoch: 2824 \tTraining Loss: 0.034268\n",
      "Epoch: 2825 \tTraining Loss: 0.034420\n",
      "Epoch: 2826 \tTraining Loss: 0.037858\n",
      "Epoch: 2827 \tTraining Loss: 0.046873\n",
      "Epoch: 2828 \tTraining Loss: 0.082087\n",
      "Epoch: 2829 \tTraining Loss: 0.057406\n",
      "Epoch: 2830 \tTraining Loss: 0.069741\n",
      "Epoch: 2831 \tTraining Loss: 0.053099\n",
      "Epoch: 2832 \tTraining Loss: 0.062668\n",
      "Epoch: 2833 \tTraining Loss: 0.089587\n",
      "Epoch: 2834 \tTraining Loss: 0.050333\n",
      "Epoch: 2835 \tTraining Loss: 0.124465\n",
      "Epoch: 2836 \tTraining Loss: 0.194457\n",
      "Epoch: 2837 \tTraining Loss: 0.095897\n",
      "Epoch: 2838 \tTraining Loss: 0.082162\n",
      "Epoch: 2839 \tTraining Loss: 0.054391\n",
      "Epoch: 2840 \tTraining Loss: 0.045331\n",
      "Epoch: 2841 \tTraining Loss: 0.040606\n",
      "Epoch: 2842 \tTraining Loss: 0.039345\n",
      "Epoch: 2843 \tTraining Loss: 0.038964\n",
      "Epoch: 2844 \tTraining Loss: 0.039163\n",
      "Epoch: 2845 \tTraining Loss: 0.037573\n",
      "Epoch: 2846 \tTraining Loss: 0.038809\n",
      "Epoch: 2847 \tTraining Loss: 0.037998\n",
      "Epoch: 2848 \tTraining Loss: 0.038027\n",
      "Epoch: 2849 \tTraining Loss: 0.038017\n",
      "Epoch: 2850 \tTraining Loss: 0.037957\n",
      "Epoch: 2851 \tTraining Loss: 0.037715\n",
      "Epoch: 2852 \tTraining Loss: 0.037808\n",
      "Epoch: 2853 \tTraining Loss: 0.037835\n",
      "Epoch: 2854 \tTraining Loss: 0.037553\n",
      "Epoch: 2855 \tTraining Loss: 0.038333\n",
      "Epoch: 2856 \tTraining Loss: 0.038082\n",
      "Epoch: 2857 \tTraining Loss: 0.037573\n",
      "Epoch: 2858 \tTraining Loss: 0.037454\n",
      "Epoch: 2859 \tTraining Loss: 0.037821\n",
      "Epoch: 2860 \tTraining Loss: 0.038384\n",
      "Epoch: 2861 \tTraining Loss: 0.037721\n",
      "Epoch: 2862 \tTraining Loss: 0.037707\n",
      "Epoch: 2863 \tTraining Loss: 0.037277\n",
      "Epoch: 2864 \tTraining Loss: 0.037500\n",
      "Epoch: 2865 \tTraining Loss: 0.037164\n",
      "Epoch: 2866 \tTraining Loss: 0.037866\n",
      "Epoch: 2867 \tTraining Loss: 0.037637\n",
      "Epoch: 2868 \tTraining Loss: 0.037415\n",
      "Epoch: 2869 \tTraining Loss: 0.037410\n",
      "Epoch: 2870 \tTraining Loss: 0.040138\n",
      "Epoch: 2871 \tTraining Loss: 0.035502\n",
      "Epoch: 2872 \tTraining Loss: 0.034720\n",
      "Epoch: 2873 \tTraining Loss: 0.035350\n",
      "Epoch: 2874 \tTraining Loss: 0.034196\n",
      "Epoch: 2875 \tTraining Loss: 0.034613\n",
      "Epoch: 2876 \tTraining Loss: 0.033929\n",
      "Epoch: 2877 \tTraining Loss: 0.034001\n",
      "Epoch: 2878 \tTraining Loss: 0.034282\n",
      "Epoch: 2879 \tTraining Loss: 0.034510\n",
      "Epoch: 2880 \tTraining Loss: 0.034331\n",
      "Epoch: 2881 \tTraining Loss: 0.033956\n",
      "Epoch: 2882 \tTraining Loss: 0.033995\n",
      "Epoch: 2883 \tTraining Loss: 0.033869\n",
      "Epoch: 2884 \tTraining Loss: 0.033918\n",
      "Epoch: 2885 \tTraining Loss: 0.033571\n",
      "Epoch: 2886 \tTraining Loss: 0.033545\n",
      "Epoch: 2887 \tTraining Loss: 0.033704\n",
      "Epoch: 2888 \tTraining Loss: 0.033762\n",
      "Epoch: 2889 \tTraining Loss: 0.033817\n",
      "Epoch: 2890 \tTraining Loss: 0.033531\n",
      "Epoch: 2891 \tTraining Loss: 0.033570\n",
      "Epoch: 2892 \tTraining Loss: 0.033691\n",
      "Epoch: 2893 \tTraining Loss: 0.033561\n",
      "Epoch: 2894 \tTraining Loss: 0.033529\n",
      "Epoch: 2895 \tTraining Loss: 0.033726\n",
      "Epoch: 2896 \tTraining Loss: 0.034306\n",
      "Epoch: 2897 \tTraining Loss: 0.034915\n",
      "Epoch: 2898 \tTraining Loss: 0.040499\n",
      "Epoch: 2899 \tTraining Loss: 0.038103\n",
      "Epoch: 2900 \tTraining Loss: 0.038045\n",
      "Epoch: 2901 \tTraining Loss: 0.035287\n",
      "Epoch: 2902 \tTraining Loss: 0.038988\n",
      "Epoch: 2903 \tTraining Loss: 0.037868\n",
      "Epoch: 2904 \tTraining Loss: 0.050085\n",
      "Epoch: 2905 \tTraining Loss: 0.035100\n",
      "Epoch: 2906 \tTraining Loss: 0.035035\n",
      "Epoch: 2907 \tTraining Loss: 0.036496\n",
      "Epoch: 2908 \tTraining Loss: 0.042378\n",
      "Epoch: 2909 \tTraining Loss: 0.036623\n",
      "Epoch: 2910 \tTraining Loss: 0.034012\n",
      "Epoch: 2911 \tTraining Loss: 0.033935\n",
      "Epoch: 2912 \tTraining Loss: 0.033576\n",
      "Epoch: 2913 \tTraining Loss: 0.035555\n",
      "Epoch: 2914 \tTraining Loss: 0.033801\n",
      "Epoch: 2915 \tTraining Loss: 0.033929\n",
      "Epoch: 2916 \tTraining Loss: 0.033752\n",
      "Epoch: 2917 \tTraining Loss: 0.033894\n",
      "Epoch: 2918 \tTraining Loss: 0.034177\n",
      "Epoch: 2919 \tTraining Loss: 0.033703\n",
      "Epoch: 2920 \tTraining Loss: 0.033489\n",
      "Epoch: 2921 \tTraining Loss: 0.033442\n",
      "Epoch: 2922 \tTraining Loss: 0.034472\n",
      "Epoch: 2923 \tTraining Loss: 0.033486\n",
      "Epoch: 2924 \tTraining Loss: 0.033365\n",
      "Epoch: 2925 \tTraining Loss: 0.033392\n",
      "Epoch: 2926 \tTraining Loss: 0.033293\n",
      "Epoch: 2927 \tTraining Loss: 0.033535\n",
      "Epoch: 2928 \tTraining Loss: 0.033285\n",
      "Epoch: 2929 \tTraining Loss: 0.033463\n",
      "Epoch: 2930 \tTraining Loss: 0.033401\n",
      "Epoch: 2931 \tTraining Loss: 0.033330\n",
      "Epoch: 2932 \tTraining Loss: 0.034687\n",
      "Epoch: 2933 \tTraining Loss: 0.034822\n",
      "Epoch: 2934 \tTraining Loss: 0.035429\n",
      "Epoch: 2935 \tTraining Loss: 0.033749\n",
      "Epoch: 2936 \tTraining Loss: 0.033107\n",
      "Epoch: 2937 \tTraining Loss: 0.032918\n",
      "Epoch: 2938 \tTraining Loss: 0.034340\n",
      "Epoch: 2939 \tTraining Loss: 0.032735\n",
      "Epoch: 2940 \tTraining Loss: 0.032914\n",
      "Epoch: 2941 \tTraining Loss: 0.032629\n",
      "Epoch: 2942 \tTraining Loss: 0.032698\n",
      "Epoch: 2943 \tTraining Loss: 0.032795\n",
      "Epoch: 2944 \tTraining Loss: 0.032703\n",
      "Epoch: 2945 \tTraining Loss: 0.032670\n",
      "Epoch: 2946 \tTraining Loss: 0.032559\n",
      "Epoch: 2947 \tTraining Loss: 0.032612\n",
      "Epoch: 2948 \tTraining Loss: 0.035512\n",
      "Epoch: 2949 \tTraining Loss: 0.032632\n",
      "Epoch: 2950 \tTraining Loss: 0.032655\n",
      "Epoch: 2951 \tTraining Loss: 0.032868\n",
      "Epoch: 2952 \tTraining Loss: 0.032556\n",
      "Epoch: 2953 \tTraining Loss: 0.032764\n",
      "Epoch: 2954 \tTraining Loss: 0.033010\n",
      "Epoch: 2955 \tTraining Loss: 0.032893\n",
      "Epoch: 2956 \tTraining Loss: 0.032682\n",
      "Epoch: 2957 \tTraining Loss: 0.032536\n",
      "Epoch: 2958 \tTraining Loss: 0.032600\n",
      "Epoch: 2959 \tTraining Loss: 0.032530\n",
      "Epoch: 2960 \tTraining Loss: 0.032624\n",
      "Epoch: 2961 \tTraining Loss: 0.032927\n",
      "Epoch: 2962 \tTraining Loss: 0.033809\n",
      "Epoch: 2963 \tTraining Loss: 0.057180\n",
      "Epoch: 2964 \tTraining Loss: 0.148390\n",
      "Epoch: 2965 \tTraining Loss: 0.220619\n",
      "Epoch: 2966 \tTraining Loss: 0.137462\n",
      "Epoch: 2967 \tTraining Loss: 0.108321\n",
      "Epoch: 2968 \tTraining Loss: 0.066779\n",
      "Epoch: 2969 \tTraining Loss: 0.062463\n",
      "Epoch: 2970 \tTraining Loss: 0.043148\n",
      "Epoch: 2971 \tTraining Loss: 0.041633\n",
      "Epoch: 2972 \tTraining Loss: 0.044197\n",
      "Epoch: 2973 \tTraining Loss: 0.039813\n",
      "Epoch: 2974 \tTraining Loss: 0.043614\n",
      "Epoch: 2975 \tTraining Loss: 0.041630\n",
      "Epoch: 2976 \tTraining Loss: 0.041631\n",
      "Epoch: 2977 \tTraining Loss: 0.040209\n",
      "Epoch: 2978 \tTraining Loss: 0.041287\n",
      "Epoch: 2979 \tTraining Loss: 0.040008\n",
      "Epoch: 2980 \tTraining Loss: 0.039819\n",
      "Epoch: 2981 \tTraining Loss: 0.040437\n",
      "Epoch: 2982 \tTraining Loss: 0.040455\n",
      "Epoch: 2983 \tTraining Loss: 0.041737\n",
      "Epoch: 2984 \tTraining Loss: 0.042921\n",
      "Epoch: 2985 \tTraining Loss: 0.039424\n",
      "Epoch: 2986 \tTraining Loss: 0.040012\n",
      "Epoch: 2987 \tTraining Loss: 0.039032\n",
      "Epoch: 2988 \tTraining Loss: 0.039883\n",
      "Epoch: 2989 \tTraining Loss: 0.039714\n",
      "Epoch: 2990 \tTraining Loss: 0.039390\n",
      "Epoch: 2991 \tTraining Loss: 0.039551\n",
      "Epoch: 2992 \tTraining Loss: 0.039563\n",
      "Epoch: 2993 \tTraining Loss: 0.039680\n",
      "Epoch: 2994 \tTraining Loss: 0.039695\n",
      "Epoch: 2995 \tTraining Loss: 0.039311\n",
      "Epoch: 2996 \tTraining Loss: 0.040243\n",
      "Epoch: 2997 \tTraining Loss: 0.039393\n",
      "Epoch: 2998 \tTraining Loss: 0.039024\n",
      "Epoch: 2999 \tTraining Loss: 0.042253\n",
      "Epoch: 3000 \tTraining Loss: 0.038669\n",
      "Epoch: 3001 \tTraining Loss: 0.039215\n",
      "Epoch: 3002 \tTraining Loss: 0.038720\n",
      "Epoch: 3003 \tTraining Loss: 0.038882\n",
      "Epoch: 3004 \tTraining Loss: 0.038675\n",
      "Epoch: 3005 \tTraining Loss: 0.038689\n",
      "Epoch: 3006 \tTraining Loss: 0.038810\n",
      "Epoch: 3007 \tTraining Loss: 0.038644\n",
      "Epoch: 3008 \tTraining Loss: 0.038702\n",
      "Epoch: 3009 \tTraining Loss: 0.038720\n",
      "Epoch: 3010 \tTraining Loss: 0.038665\n",
      "Epoch: 3011 \tTraining Loss: 0.038557\n",
      "Epoch: 3012 \tTraining Loss: 0.038744\n",
      "Epoch: 3013 \tTraining Loss: 0.038664\n",
      "Epoch: 3014 \tTraining Loss: 0.038253\n",
      "Epoch: 3015 \tTraining Loss: 0.037408\n",
      "Epoch: 3016 \tTraining Loss: 0.037290\n",
      "Epoch: 3017 \tTraining Loss: 0.037052\n",
      "Epoch: 3018 \tTraining Loss: 0.037015\n",
      "Epoch: 3019 \tTraining Loss: 0.037375\n",
      "Epoch: 3020 \tTraining Loss: 0.037355\n",
      "Epoch: 3021 \tTraining Loss: 0.036946\n",
      "Epoch: 3022 \tTraining Loss: 0.036940\n",
      "Epoch: 3023 \tTraining Loss: 0.037001\n",
      "Epoch: 3024 \tTraining Loss: 0.036985\n",
      "Epoch: 3025 \tTraining Loss: 0.036825\n",
      "Epoch: 3026 \tTraining Loss: 0.036955\n",
      "Epoch: 3027 \tTraining Loss: 0.036937\n",
      "Epoch: 3028 \tTraining Loss: 0.036925\n",
      "Epoch: 3029 \tTraining Loss: 0.036909\n",
      "Epoch: 3030 \tTraining Loss: 0.036825\n",
      "Epoch: 3031 \tTraining Loss: 0.036916\n",
      "Epoch: 3032 \tTraining Loss: 0.036936\n",
      "Epoch: 3033 \tTraining Loss: 0.036767\n",
      "Epoch: 3034 \tTraining Loss: 0.036970\n",
      "Epoch: 3035 \tTraining Loss: 0.036833\n",
      "Epoch: 3036 \tTraining Loss: 0.037902\n",
      "Epoch: 3037 \tTraining Loss: 0.037521\n",
      "Epoch: 3038 \tTraining Loss: 0.036591\n",
      "Epoch: 3039 \tTraining Loss: 0.037011\n",
      "Epoch: 3040 \tTraining Loss: 0.036718\n",
      "Epoch: 3041 \tTraining Loss: 0.036785\n",
      "Epoch: 3042 \tTraining Loss: 0.036778\n",
      "Epoch: 3043 \tTraining Loss: 0.036719\n",
      "Epoch: 3044 \tTraining Loss: 0.036650\n",
      "Epoch: 3045 \tTraining Loss: 0.036717\n",
      "Epoch: 3046 \tTraining Loss: 0.036682\n",
      "Epoch: 3047 \tTraining Loss: 0.037439\n",
      "Epoch: 3048 \tTraining Loss: 0.036895\n",
      "Epoch: 3049 \tTraining Loss: 0.036793\n",
      "Epoch: 3050 \tTraining Loss: 0.036687\n",
      "Epoch: 3051 \tTraining Loss: 0.036680\n",
      "Epoch: 3052 \tTraining Loss: 0.036741\n",
      "Epoch: 3053 \tTraining Loss: 0.036681\n",
      "Epoch: 3054 \tTraining Loss: 0.036672\n",
      "Epoch: 3055 \tTraining Loss: 0.036637\n",
      "Epoch: 3056 \tTraining Loss: 0.036689\n",
      "Epoch: 3057 \tTraining Loss: 0.036598\n",
      "Epoch: 3058 \tTraining Loss: 0.036668\n",
      "Epoch: 3059 \tTraining Loss: 0.036550\n",
      "Epoch: 3060 \tTraining Loss: 0.036541\n",
      "Epoch: 3061 \tTraining Loss: 0.036601\n",
      "Epoch: 3062 \tTraining Loss: 0.036551\n",
      "Epoch: 3063 \tTraining Loss: 0.036630\n",
      "Epoch: 3064 \tTraining Loss: 0.036623\n",
      "Epoch: 3065 \tTraining Loss: 0.036580\n",
      "Epoch: 3066 \tTraining Loss: 0.036674\n",
      "Epoch: 3067 \tTraining Loss: 0.036691\n",
      "Epoch: 3068 \tTraining Loss: 0.036612\n",
      "Epoch: 3069 \tTraining Loss: 0.036549\n",
      "Epoch: 3070 \tTraining Loss: 0.036518\n",
      "Epoch: 3071 \tTraining Loss: 0.036589\n",
      "Epoch: 3072 \tTraining Loss: 0.036521\n",
      "Epoch: 3073 \tTraining Loss: 0.036501\n",
      "Epoch: 3074 \tTraining Loss: 0.036486\n",
      "Epoch: 3075 \tTraining Loss: 0.036541\n",
      "Epoch: 3076 \tTraining Loss: 0.036626\n",
      "Epoch: 3077 \tTraining Loss: 0.036453\n",
      "Epoch: 3078 \tTraining Loss: 0.036618\n",
      "Epoch: 3079 \tTraining Loss: 0.036584\n",
      "Epoch: 3080 \tTraining Loss: 0.037220\n",
      "Epoch: 3081 \tTraining Loss: 0.036616\n",
      "Epoch: 3082 \tTraining Loss: 0.036537\n",
      "Epoch: 3083 \tTraining Loss: 0.036504\n",
      "Epoch: 3084 \tTraining Loss: 0.036530\n",
      "Epoch: 3085 \tTraining Loss: 0.036522\n",
      "Epoch: 3086 \tTraining Loss: 0.036523\n",
      "Epoch: 3087 \tTraining Loss: 0.036508\n",
      "Epoch: 3088 \tTraining Loss: 0.036550\n",
      "Epoch: 3089 \tTraining Loss: 0.036476\n",
      "Epoch: 3090 \tTraining Loss: 0.036459\n",
      "Epoch: 3091 \tTraining Loss: 0.036434\n",
      "Epoch: 3092 \tTraining Loss: 0.036429\n",
      "Epoch: 3093 \tTraining Loss: 0.036608\n",
      "Epoch: 3094 \tTraining Loss: 0.036535\n",
      "Epoch: 3095 \tTraining Loss: 0.035757\n",
      "Epoch: 3096 \tTraining Loss: 0.035743\n",
      "Epoch: 3097 \tTraining Loss: 0.035807\n",
      "Epoch: 3098 \tTraining Loss: 0.035846\n",
      "Epoch: 3099 \tTraining Loss: 0.035686\n",
      "Epoch: 3100 \tTraining Loss: 0.035692\n",
      "Epoch: 3101 \tTraining Loss: 0.035676\n",
      "Epoch: 3102 \tTraining Loss: 0.035712\n",
      "Epoch: 3103 \tTraining Loss: 0.035604\n",
      "Epoch: 3104 \tTraining Loss: 0.035613\n",
      "Epoch: 3105 \tTraining Loss: 0.035589\n",
      "Epoch: 3106 \tTraining Loss: 0.035671\n",
      "Epoch: 3107 \tTraining Loss: 0.039416\n",
      "Epoch: 3108 \tTraining Loss: 0.094032\n",
      "Epoch: 3109 \tTraining Loss: 0.188577\n",
      "Epoch: 3110 \tTraining Loss: 0.494691\n",
      "Epoch: 3111 \tTraining Loss: 0.206506\n",
      "Epoch: 3112 \tTraining Loss: 0.229320\n",
      "Epoch: 3113 \tTraining Loss: 0.065803\n",
      "Epoch: 3114 \tTraining Loss: 0.069121\n",
      "Epoch: 3115 \tTraining Loss: 0.061397\n",
      "Epoch: 3116 \tTraining Loss: 0.052710\n",
      "Epoch: 3117 \tTraining Loss: 0.042408\n",
      "Epoch: 3118 \tTraining Loss: 0.040529\n",
      "Epoch: 3119 \tTraining Loss: 0.040468\n",
      "Epoch: 3120 \tTraining Loss: 0.039734\n",
      "Epoch: 3121 \tTraining Loss: 0.040504\n",
      "Epoch: 3122 \tTraining Loss: 0.040245\n",
      "Epoch: 3123 \tTraining Loss: 0.038520\n",
      "Epoch: 3124 \tTraining Loss: 0.039113\n",
      "Epoch: 3125 \tTraining Loss: 0.037857\n",
      "Epoch: 3126 \tTraining Loss: 0.037121\n",
      "Epoch: 3127 \tTraining Loss: 0.037442\n",
      "Epoch: 3128 \tTraining Loss: 0.037171\n",
      "Epoch: 3129 \tTraining Loss: 0.037168\n",
      "Epoch: 3130 \tTraining Loss: 0.036922\n",
      "Epoch: 3131 \tTraining Loss: 0.037136\n",
      "Epoch: 3132 \tTraining Loss: 0.037063\n",
      "Epoch: 3133 \tTraining Loss: 0.037712\n",
      "Epoch: 3134 \tTraining Loss: 0.037096\n",
      "Epoch: 3135 \tTraining Loss: 0.036552\n",
      "Epoch: 3136 \tTraining Loss: 0.036594\n",
      "Epoch: 3137 \tTraining Loss: 0.036607\n",
      "Epoch: 3138 \tTraining Loss: 0.036716\n",
      "Epoch: 3139 \tTraining Loss: 0.036730\n",
      "Epoch: 3140 \tTraining Loss: 0.036493\n",
      "Epoch: 3141 \tTraining Loss: 0.036465\n",
      "Epoch: 3142 \tTraining Loss: 0.036311\n",
      "Epoch: 3143 \tTraining Loss: 0.036290\n",
      "Epoch: 3144 \tTraining Loss: 0.036362\n",
      "Epoch: 3145 \tTraining Loss: 0.036142\n",
      "Epoch: 3146 \tTraining Loss: 0.036121\n",
      "Epoch: 3147 \tTraining Loss: 0.036075\n",
      "Epoch: 3148 \tTraining Loss: 0.036113\n",
      "Epoch: 3149 \tTraining Loss: 0.036142\n",
      "Epoch: 3150 \tTraining Loss: 0.036125\n",
      "Epoch: 3151 \tTraining Loss: 0.036204\n",
      "Epoch: 3152 \tTraining Loss: 0.036240\n",
      "Epoch: 3153 \tTraining Loss: 0.039854\n",
      "Epoch: 3154 \tTraining Loss: 0.036163\n",
      "Epoch: 3155 \tTraining Loss: 0.039182\n",
      "Epoch: 3156 \tTraining Loss: 0.036441\n",
      "Epoch: 3157 \tTraining Loss: 0.035955\n",
      "Epoch: 3158 \tTraining Loss: 0.036119\n",
      "Epoch: 3159 \tTraining Loss: 0.035849\n",
      "Epoch: 3160 \tTraining Loss: 0.035972\n",
      "Epoch: 3161 \tTraining Loss: 0.036033\n",
      "Epoch: 3162 \tTraining Loss: 0.036004\n",
      "Epoch: 3163 \tTraining Loss: 0.035945\n",
      "Epoch: 3164 \tTraining Loss: 0.035921\n",
      "Epoch: 3165 \tTraining Loss: 0.035928\n",
      "Epoch: 3166 \tTraining Loss: 0.035974\n",
      "Epoch: 3167 \tTraining Loss: 0.035972\n",
      "Epoch: 3168 \tTraining Loss: 0.035887\n",
      "Epoch: 3169 \tTraining Loss: 0.035884\n",
      "Epoch: 3170 \tTraining Loss: 0.035835\n",
      "Epoch: 3171 \tTraining Loss: 0.036102\n",
      "Epoch: 3172 \tTraining Loss: 0.035890\n",
      "Epoch: 3173 \tTraining Loss: 0.035853\n",
      "Epoch: 3174 \tTraining Loss: 0.035728\n",
      "Epoch: 3175 \tTraining Loss: 0.035913\n",
      "Epoch: 3176 \tTraining Loss: 0.035910\n",
      "Epoch: 3177 \tTraining Loss: 0.035747\n",
      "Epoch: 3178 \tTraining Loss: 0.035728\n",
      "Epoch: 3179 \tTraining Loss: 0.035706\n",
      "Epoch: 3180 \tTraining Loss: 0.035764\n",
      "Epoch: 3181 \tTraining Loss: 0.035840\n",
      "Epoch: 3182 \tTraining Loss: 0.035707\n",
      "Epoch: 3183 \tTraining Loss: 0.035769\n",
      "Epoch: 3184 \tTraining Loss: 0.035697\n",
      "Epoch: 3185 \tTraining Loss: 0.035751\n",
      "Epoch: 3186 \tTraining Loss: 0.035663\n",
      "Epoch: 3187 \tTraining Loss: 0.035655\n",
      "Epoch: 3188 \tTraining Loss: 0.035705\n",
      "Epoch: 3189 \tTraining Loss: 0.035774\n",
      "Epoch: 3190 \tTraining Loss: 0.035691\n",
      "Epoch: 3191 \tTraining Loss: 0.035679\n",
      "Epoch: 3192 \tTraining Loss: 0.035747\n",
      "Epoch: 3193 \tTraining Loss: 0.035621\n",
      "Epoch: 3194 \tTraining Loss: 0.035633\n",
      "Epoch: 3195 \tTraining Loss: 0.035618\n",
      "Epoch: 3196 \tTraining Loss: 0.035607\n",
      "Epoch: 3197 \tTraining Loss: 0.035714\n",
      "Epoch: 3198 \tTraining Loss: 0.035650\n",
      "Epoch: 3199 \tTraining Loss: 0.035617\n",
      "Epoch: 3200 \tTraining Loss: 0.035738\n",
      "Epoch: 3201 \tTraining Loss: 0.035652\n",
      "Epoch: 3202 \tTraining Loss: 0.036052\n",
      "Epoch: 3203 \tTraining Loss: 0.035660\n",
      "Epoch: 3204 \tTraining Loss: 0.035687\n",
      "Epoch: 3205 \tTraining Loss: 0.035663\n",
      "Epoch: 3206 \tTraining Loss: 0.035681\n",
      "Epoch: 3207 \tTraining Loss: 0.037559\n",
      "Epoch: 3208 \tTraining Loss: 0.047446\n",
      "Epoch: 3209 \tTraining Loss: 0.042737\n",
      "Epoch: 3210 \tTraining Loss: 0.044220\n",
      "Epoch: 3211 \tTraining Loss: 0.042244\n",
      "Epoch: 3212 \tTraining Loss: 0.099531\n",
      "Epoch: 3213 \tTraining Loss: 0.044072\n",
      "Epoch: 3214 \tTraining Loss: 0.062563\n",
      "Epoch: 3215 \tTraining Loss: 0.047155\n",
      "Epoch: 3216 \tTraining Loss: 0.062420\n",
      "Epoch: 3217 \tTraining Loss: 0.055289\n",
      "Epoch: 3218 \tTraining Loss: 0.043229\n",
      "Epoch: 3219 \tTraining Loss: 0.045352\n",
      "Epoch: 3220 \tTraining Loss: 0.038324\n",
      "Epoch: 3221 \tTraining Loss: 0.037629\n",
      "Epoch: 3222 \tTraining Loss: 0.035543\n",
      "Epoch: 3223 \tTraining Loss: 0.035423\n",
      "Epoch: 3224 \tTraining Loss: 0.035279\n",
      "Epoch: 3225 \tTraining Loss: 0.035171\n",
      "Epoch: 3226 \tTraining Loss: 0.035038\n",
      "Epoch: 3227 \tTraining Loss: 0.035128\n",
      "Epoch: 3228 \tTraining Loss: 0.035110\n",
      "Epoch: 3229 \tTraining Loss: 0.035077\n",
      "Epoch: 3230 \tTraining Loss: 0.035166\n",
      "Epoch: 3231 \tTraining Loss: 0.036509\n",
      "Epoch: 3232 \tTraining Loss: 0.039162\n",
      "Epoch: 3233 \tTraining Loss: 0.036679\n",
      "Epoch: 3234 \tTraining Loss: 0.037216\n",
      "Epoch: 3235 \tTraining Loss: 0.036413\n",
      "Epoch: 3236 \tTraining Loss: 0.036097\n",
      "Epoch: 3237 \tTraining Loss: 0.035628\n",
      "Epoch: 3238 \tTraining Loss: 0.035808\n",
      "Epoch: 3239 \tTraining Loss: 0.035740\n",
      "Epoch: 3240 \tTraining Loss: 0.035744\n",
      "Epoch: 3241 \tTraining Loss: 0.035766\n",
      "Epoch: 3242 \tTraining Loss: 0.035852\n",
      "Epoch: 3243 \tTraining Loss: 0.036034\n",
      "Epoch: 3244 \tTraining Loss: 0.035680\n",
      "Epoch: 3245 \tTraining Loss: 0.035956\n",
      "Epoch: 3246 \tTraining Loss: 0.035862\n",
      "Epoch: 3247 \tTraining Loss: 0.035072\n",
      "Epoch: 3248 \tTraining Loss: 0.035353\n",
      "Epoch: 3249 \tTraining Loss: 0.037011\n",
      "Epoch: 3250 \tTraining Loss: 0.036821\n",
      "Epoch: 3251 \tTraining Loss: 0.037261\n",
      "Epoch: 3252 \tTraining Loss: 0.036779\n",
      "Epoch: 3253 \tTraining Loss: 0.036648\n",
      "Epoch: 3254 \tTraining Loss: 0.036727\n",
      "Epoch: 3255 \tTraining Loss: 0.036556\n",
      "Epoch: 3256 \tTraining Loss: 0.036590\n",
      "Epoch: 3257 \tTraining Loss: 0.036537\n",
      "Epoch: 3258 \tTraining Loss: 0.036612\n",
      "Epoch: 3259 \tTraining Loss: 0.036565\n",
      "Epoch: 3260 \tTraining Loss: 0.036539\n",
      "Epoch: 3261 \tTraining Loss: 0.036593\n",
      "Epoch: 3262 \tTraining Loss: 0.036666\n",
      "Epoch: 3263 \tTraining Loss: 0.036619\n",
      "Epoch: 3264 \tTraining Loss: 0.036437\n",
      "Epoch: 3265 \tTraining Loss: 0.036497\n",
      "Epoch: 3266 \tTraining Loss: 0.036477\n",
      "Epoch: 3267 \tTraining Loss: 0.036492\n",
      "Epoch: 3268 \tTraining Loss: 0.036497\n",
      "Epoch: 3269 \tTraining Loss: 0.036613\n",
      "Epoch: 3270 \tTraining Loss: 0.036422\n",
      "Epoch: 3271 \tTraining Loss: 0.036456\n",
      "Epoch: 3272 \tTraining Loss: 0.036505\n",
      "Epoch: 3273 \tTraining Loss: 0.036526\n",
      "Epoch: 3274 \tTraining Loss: 0.036519\n",
      "Epoch: 3275 \tTraining Loss: 0.036428\n",
      "Epoch: 3276 \tTraining Loss: 0.036407\n",
      "Epoch: 3277 \tTraining Loss: 0.036427\n",
      "Epoch: 3278 \tTraining Loss: 0.036526\n",
      "Epoch: 3279 \tTraining Loss: 0.036379\n",
      "Epoch: 3280 \tTraining Loss: 0.036418\n",
      "Epoch: 3281 \tTraining Loss: 0.036372\n",
      "Epoch: 3282 \tTraining Loss: 0.036472\n",
      "Epoch: 3283 \tTraining Loss: 0.036452\n",
      "Epoch: 3284 \tTraining Loss: 0.036440\n",
      "Epoch: 3285 \tTraining Loss: 0.036428\n",
      "Epoch: 3286 \tTraining Loss: 0.036378\n",
      "Epoch: 3287 \tTraining Loss: 0.036402\n",
      "Epoch: 3288 \tTraining Loss: 0.036388\n",
      "Epoch: 3289 \tTraining Loss: 0.036429\n",
      "Epoch: 3290 \tTraining Loss: 0.036361\n",
      "Epoch: 3291 \tTraining Loss: 0.036417\n",
      "Epoch: 3292 \tTraining Loss: 0.036393\n",
      "Epoch: 3293 \tTraining Loss: 0.036362\n",
      "Epoch: 3294 \tTraining Loss: 0.036403\n",
      "Epoch: 3295 \tTraining Loss: 0.036321\n",
      "Epoch: 3296 \tTraining Loss: 0.036397\n",
      "Epoch: 3297 \tTraining Loss: 0.036303\n",
      "Epoch: 3298 \tTraining Loss: 0.036418\n",
      "Epoch: 3299 \tTraining Loss: 0.036450\n",
      "Epoch: 3300 \tTraining Loss: 0.036371\n",
      "Epoch: 3301 \tTraining Loss: 0.036335\n",
      "Epoch: 3302 \tTraining Loss: 0.036347\n",
      "Epoch: 3303 \tTraining Loss: 0.036336\n",
      "Epoch: 3304 \tTraining Loss: 0.036335\n",
      "Epoch: 3305 \tTraining Loss: 0.036331\n",
      "Epoch: 3306 \tTraining Loss: 0.036362\n",
      "Epoch: 3307 \tTraining Loss: 0.036352\n",
      "Epoch: 3308 \tTraining Loss: 0.036388\n",
      "Epoch: 3309 \tTraining Loss: 0.036383\n",
      "Epoch: 3310 \tTraining Loss: 0.036380\n",
      "Epoch: 3311 \tTraining Loss: 0.036366\n",
      "Epoch: 3312 \tTraining Loss: 0.036338\n",
      "Epoch: 3313 \tTraining Loss: 0.036358\n",
      "Epoch: 3314 \tTraining Loss: 0.036418\n",
      "Epoch: 3315 \tTraining Loss: 0.036381\n",
      "Epoch: 3316 \tTraining Loss: 0.036369\n",
      "Epoch: 3317 \tTraining Loss: 0.037236\n",
      "Epoch: 3318 \tTraining Loss: 0.037426\n",
      "Epoch: 3319 \tTraining Loss: 0.037282\n",
      "Epoch: 3320 \tTraining Loss: 0.036863\n",
      "Epoch: 3321 \tTraining Loss: 0.036349\n",
      "Epoch: 3322 \tTraining Loss: 0.036362\n",
      "Epoch: 3323 \tTraining Loss: 0.036300\n",
      "Epoch: 3324 \tTraining Loss: 0.036496\n",
      "Epoch: 3325 \tTraining Loss: 0.036317\n",
      "Epoch: 3326 \tTraining Loss: 0.036310\n",
      "Epoch: 3327 \tTraining Loss: 0.036345\n",
      "Epoch: 3328 \tTraining Loss: 0.036305\n",
      "Epoch: 3329 \tTraining Loss: 0.036317\n",
      "Epoch: 3330 \tTraining Loss: 0.036330\n",
      "Epoch: 3331 \tTraining Loss: 0.036337\n",
      "Epoch: 3332 \tTraining Loss: 0.036289\n",
      "Epoch: 3333 \tTraining Loss: 0.036331\n",
      "Epoch: 3334 \tTraining Loss: 0.036263\n",
      "Epoch: 3335 \tTraining Loss: 0.036259\n",
      "Epoch: 3336 \tTraining Loss: 0.036282\n",
      "Epoch: 3337 \tTraining Loss: 0.036313\n",
      "Epoch: 3338 \tTraining Loss: 0.036276\n",
      "Epoch: 3339 \tTraining Loss: 0.036370\n",
      "Epoch: 3340 \tTraining Loss: 0.036306\n",
      "Epoch: 3341 \tTraining Loss: 0.036288\n",
      "Epoch: 3342 \tTraining Loss: 0.036263\n",
      "Epoch: 3343 \tTraining Loss: 0.036251\n",
      "Epoch: 3344 \tTraining Loss: 0.036405\n",
      "Epoch: 3345 \tTraining Loss: 0.088424\n",
      "Epoch: 3346 \tTraining Loss: 0.276188\n",
      "Epoch: 3347 \tTraining Loss: 0.157185\n",
      "Epoch: 3348 \tTraining Loss: 0.233152\n",
      "Epoch: 3349 \tTraining Loss: 0.090045\n",
      "Epoch: 3350 \tTraining Loss: 0.088432\n",
      "Epoch: 3351 \tTraining Loss: 0.060341\n",
      "Epoch: 3352 \tTraining Loss: 0.058747\n",
      "Epoch: 3353 \tTraining Loss: 0.056740\n",
      "Epoch: 3354 \tTraining Loss: 0.055674\n",
      "Epoch: 3355 \tTraining Loss: 0.055187\n",
      "Epoch: 3356 \tTraining Loss: 0.054897\n",
      "Epoch: 3357 \tTraining Loss: 0.054850\n",
      "Epoch: 3358 \tTraining Loss: 0.054556\n",
      "Epoch: 3359 \tTraining Loss: 0.055192\n",
      "Epoch: 3360 \tTraining Loss: 0.054423\n",
      "Epoch: 3361 \tTraining Loss: 0.054476\n",
      "Epoch: 3362 \tTraining Loss: 0.054393\n",
      "Epoch: 3363 \tTraining Loss: 0.054510\n",
      "Epoch: 3364 \tTraining Loss: 0.054146\n",
      "Epoch: 3365 \tTraining Loss: 0.054065\n",
      "Epoch: 3366 \tTraining Loss: 0.054080\n",
      "Epoch: 3367 \tTraining Loss: 0.053948\n",
      "Epoch: 3368 \tTraining Loss: 0.054369\n",
      "Epoch: 3369 \tTraining Loss: 0.052858\n",
      "Epoch: 3370 \tTraining Loss: 0.052212\n",
      "Epoch: 3371 \tTraining Loss: 0.052564\n",
      "Epoch: 3372 \tTraining Loss: 0.049985\n",
      "Epoch: 3373 \tTraining Loss: 0.065410\n",
      "Epoch: 3374 \tTraining Loss: 0.082982\n",
      "Epoch: 3375 \tTraining Loss: 0.105323\n",
      "Epoch: 3376 \tTraining Loss: 0.059165\n",
      "Epoch: 3377 \tTraining Loss: 0.054343\n",
      "Epoch: 3378 \tTraining Loss: 0.046361\n",
      "Epoch: 3379 \tTraining Loss: 0.049011\n",
      "Epoch: 3380 \tTraining Loss: 0.046268\n",
      "Epoch: 3381 \tTraining Loss: 0.045784\n",
      "Epoch: 3382 \tTraining Loss: 0.045858\n",
      "Epoch: 3383 \tTraining Loss: 0.045811\n",
      "Epoch: 3384 \tTraining Loss: 0.045691\n",
      "Epoch: 3385 \tTraining Loss: 0.047559\n",
      "Epoch: 3386 \tTraining Loss: 0.043970\n",
      "Epoch: 3387 \tTraining Loss: 0.044197\n",
      "Epoch: 3388 \tTraining Loss: 0.043996\n",
      "Epoch: 3389 \tTraining Loss: 0.043967\n",
      "Epoch: 3390 \tTraining Loss: 0.044604\n",
      "Epoch: 3391 \tTraining Loss: 0.043954\n",
      "Epoch: 3392 \tTraining Loss: 0.044036\n",
      "Epoch: 3393 \tTraining Loss: 0.043937\n",
      "Epoch: 3394 \tTraining Loss: 0.043884\n",
      "Epoch: 3395 \tTraining Loss: 0.044018\n",
      "Epoch: 3396 \tTraining Loss: 0.044674\n",
      "Epoch: 3397 \tTraining Loss: 0.044687\n",
      "Epoch: 3398 \tTraining Loss: 0.044083\n",
      "Epoch: 3399 \tTraining Loss: 0.043843\n",
      "Epoch: 3400 \tTraining Loss: 0.043909\n",
      "Epoch: 3401 \tTraining Loss: 0.043822\n",
      "Epoch: 3402 \tTraining Loss: 0.043930\n",
      "Epoch: 3403 \tTraining Loss: 0.043956\n",
      "Epoch: 3404 \tTraining Loss: 0.043948\n",
      "Epoch: 3405 \tTraining Loss: 0.043787\n",
      "Epoch: 3406 \tTraining Loss: 0.043952\n",
      "Epoch: 3407 \tTraining Loss: 0.043896\n",
      "Epoch: 3408 \tTraining Loss: 0.043862\n",
      "Epoch: 3409 \tTraining Loss: 0.043910\n",
      "Epoch: 3410 \tTraining Loss: 0.043871\n",
      "Epoch: 3411 \tTraining Loss: 0.043898\n",
      "Epoch: 3412 \tTraining Loss: 0.043887\n",
      "Epoch: 3413 \tTraining Loss: 0.043866\n",
      "Epoch: 3414 \tTraining Loss: 0.043696\n",
      "Epoch: 3415 \tTraining Loss: 0.043812\n",
      "Epoch: 3416 \tTraining Loss: 0.043858\n",
      "Epoch: 3417 \tTraining Loss: 0.043645\n",
      "Epoch: 3418 \tTraining Loss: 0.043852\n",
      "Epoch: 3419 \tTraining Loss: 0.043857\n",
      "Epoch: 3420 \tTraining Loss: 0.043799\n",
      "Epoch: 3421 \tTraining Loss: 0.044001\n",
      "Epoch: 3422 \tTraining Loss: 0.044108\n",
      "Epoch: 3423 \tTraining Loss: 0.043963\n",
      "Epoch: 3424 \tTraining Loss: 0.043903\n",
      "Epoch: 3425 \tTraining Loss: 0.043744\n",
      "Epoch: 3426 \tTraining Loss: 0.043768\n",
      "Epoch: 3427 \tTraining Loss: 0.044111\n",
      "Epoch: 3428 \tTraining Loss: 0.044434\n",
      "Epoch: 3429 \tTraining Loss: 0.044378\n",
      "Epoch: 3430 \tTraining Loss: 0.044404\n",
      "Epoch: 3431 \tTraining Loss: 0.044394\n",
      "Epoch: 3432 \tTraining Loss: 0.044428\n",
      "Epoch: 3433 \tTraining Loss: 0.044604\n",
      "Epoch: 3434 \tTraining Loss: 0.044469\n",
      "Epoch: 3435 \tTraining Loss: 0.044409\n",
      "Epoch: 3436 \tTraining Loss: 0.044447\n",
      "Epoch: 3437 \tTraining Loss: 0.044351\n",
      "Epoch: 3438 \tTraining Loss: 0.044352\n",
      "Epoch: 3439 \tTraining Loss: 0.044401\n",
      "Epoch: 3440 \tTraining Loss: 0.044367\n",
      "Epoch: 3441 \tTraining Loss: 0.044383\n",
      "Epoch: 3442 \tTraining Loss: 0.044382\n",
      "Epoch: 3443 \tTraining Loss: 0.044370\n",
      "Epoch: 3444 \tTraining Loss: 0.044327\n",
      "Epoch: 3445 \tTraining Loss: 0.044328\n",
      "Epoch: 3446 \tTraining Loss: 0.044356\n",
      "Epoch: 3447 \tTraining Loss: 0.044361\n",
      "Epoch: 3448 \tTraining Loss: 0.044271\n",
      "Epoch: 3449 \tTraining Loss: 0.044319\n",
      "Epoch: 3450 \tTraining Loss: 0.044341\n",
      "Epoch: 3451 \tTraining Loss: 0.044392\n",
      "Epoch: 3452 \tTraining Loss: 0.044474\n",
      "Epoch: 3453 \tTraining Loss: 0.044294\n",
      "Epoch: 3454 \tTraining Loss: 0.044388\n",
      "Epoch: 3455 \tTraining Loss: 0.044334\n",
      "Epoch: 3456 \tTraining Loss: 0.044298\n",
      "Epoch: 3457 \tTraining Loss: 0.044291\n",
      "Epoch: 3458 \tTraining Loss: 0.044310\n",
      "Epoch: 3459 \tTraining Loss: 0.044346\n",
      "Epoch: 3460 \tTraining Loss: 0.044253\n",
      "Epoch: 3461 \tTraining Loss: 0.044278\n",
      "Epoch: 3462 \tTraining Loss: 0.044349\n",
      "Epoch: 3463 \tTraining Loss: 0.044387\n",
      "Epoch: 3464 \tTraining Loss: 0.044321\n",
      "Epoch: 3465 \tTraining Loss: 0.044345\n",
      "Epoch: 3466 \tTraining Loss: 0.044388\n",
      "Epoch: 3467 \tTraining Loss: 0.044492\n",
      "Epoch: 3468 \tTraining Loss: 0.044304\n",
      "Epoch: 3469 \tTraining Loss: 0.044301\n",
      "Epoch: 3470 \tTraining Loss: 0.044364\n",
      "Epoch: 3471 \tTraining Loss: 0.044306\n",
      "Epoch: 3472 \tTraining Loss: 0.044277\n",
      "Epoch: 3473 \tTraining Loss: 0.044338\n",
      "Epoch: 3474 \tTraining Loss: 0.044323\n",
      "Epoch: 3475 \tTraining Loss: 0.044343\n",
      "Epoch: 3476 \tTraining Loss: 0.044310\n",
      "Epoch: 3477 \tTraining Loss: 0.044307\n",
      "Epoch: 3478 \tTraining Loss: 0.044326\n",
      "Epoch: 3479 \tTraining Loss: 0.044283\n",
      "Epoch: 3480 \tTraining Loss: 0.044287\n",
      "Epoch: 3481 \tTraining Loss: 0.044265\n",
      "Epoch: 3482 \tTraining Loss: 0.044245\n",
      "Epoch: 3483 \tTraining Loss: 0.044263\n",
      "Epoch: 3484 \tTraining Loss: 0.044257\n",
      "Epoch: 3485 \tTraining Loss: 0.044243\n",
      "Epoch: 3486 \tTraining Loss: 0.044321\n",
      "Epoch: 3487 \tTraining Loss: 0.044235\n",
      "Epoch: 3488 \tTraining Loss: 0.044272\n",
      "Epoch: 3489 \tTraining Loss: 0.044274\n",
      "Epoch: 3490 \tTraining Loss: 0.044336\n",
      "Epoch: 3491 \tTraining Loss: 0.044401\n",
      "Epoch: 3492 \tTraining Loss: 0.044310\n",
      "Epoch: 3493 \tTraining Loss: 0.044236\n",
      "Epoch: 3494 \tTraining Loss: 0.044234\n",
      "Epoch: 3495 \tTraining Loss: 0.044267\n",
      "Epoch: 3496 \tTraining Loss: 0.044220\n",
      "Epoch: 3497 \tTraining Loss: 0.044224\n",
      "Epoch: 3498 \tTraining Loss: 0.044339\n",
      "Epoch: 3499 \tTraining Loss: 0.044259\n",
      "Epoch: 3500 \tTraining Loss: 0.044219\n",
      "Epoch: 3501 \tTraining Loss: 0.044347\n",
      "Epoch: 3502 \tTraining Loss: 0.044438\n",
      "Epoch: 3503 \tTraining Loss: 0.044225\n",
      "Epoch: 3504 \tTraining Loss: 0.044239\n",
      "Epoch: 3505 \tTraining Loss: 0.044305\n",
      "Epoch: 3506 \tTraining Loss: 0.044232\n",
      "Epoch: 3507 \tTraining Loss: 0.044278\n",
      "Epoch: 3508 \tTraining Loss: 0.044325\n",
      "Epoch: 3509 \tTraining Loss: 0.044220\n",
      "Epoch: 3510 \tTraining Loss: 0.044333\n",
      "Epoch: 3511 \tTraining Loss: 0.044235\n",
      "Epoch: 3512 \tTraining Loss: 0.044284\n",
      "Epoch: 3513 \tTraining Loss: 0.047689\n",
      "Epoch: 3514 \tTraining Loss: 0.047778\n",
      "Epoch: 3515 \tTraining Loss: 0.052288\n",
      "Epoch: 3516 \tTraining Loss: 0.058275\n",
      "Epoch: 3517 \tTraining Loss: 0.176386\n",
      "Epoch: 3518 \tTraining Loss: 0.189437\n",
      "Epoch: 3519 \tTraining Loss: 0.193475\n",
      "Epoch: 3520 \tTraining Loss: 0.097189\n",
      "Epoch: 3521 \tTraining Loss: 0.110439\n",
      "Epoch: 3522 \tTraining Loss: 0.060077\n",
      "Epoch: 3523 \tTraining Loss: 0.054394\n",
      "Epoch: 3524 \tTraining Loss: 0.046834\n",
      "Epoch: 3525 \tTraining Loss: 0.048462\n",
      "Epoch: 3526 \tTraining Loss: 0.043856\n",
      "Epoch: 3527 \tTraining Loss: 0.044009\n",
      "Epoch: 3528 \tTraining Loss: 0.041454\n",
      "Epoch: 3529 \tTraining Loss: 0.039586\n",
      "Epoch: 3530 \tTraining Loss: 0.039030\n",
      "Epoch: 3531 \tTraining Loss: 0.038538\n",
      "Epoch: 3532 \tTraining Loss: 0.038450\n",
      "Epoch: 3533 \tTraining Loss: 0.038419\n",
      "Epoch: 3534 \tTraining Loss: 0.038446\n",
      "Epoch: 3535 \tTraining Loss: 0.038427\n",
      "Epoch: 3536 \tTraining Loss: 0.038480\n",
      "Epoch: 3537 \tTraining Loss: 0.038434\n",
      "Epoch: 3538 \tTraining Loss: 0.038309\n",
      "Epoch: 3539 \tTraining Loss: 0.038321\n",
      "Epoch: 3540 \tTraining Loss: 0.038253\n",
      "Epoch: 3541 \tTraining Loss: 0.038287\n",
      "Epoch: 3542 \tTraining Loss: 0.038186\n",
      "Epoch: 3543 \tTraining Loss: 0.038215\n",
      "Epoch: 3544 \tTraining Loss: 0.038229\n",
      "Epoch: 3545 \tTraining Loss: 0.038196\n",
      "Epoch: 3546 \tTraining Loss: 0.038190\n",
      "Epoch: 3547 \tTraining Loss: 0.038209\n",
      "Epoch: 3548 \tTraining Loss: 0.038159\n",
      "Epoch: 3549 \tTraining Loss: 0.038214\n",
      "Epoch: 3550 \tTraining Loss: 0.038147\n",
      "Epoch: 3551 \tTraining Loss: 0.038199\n",
      "Epoch: 3552 \tTraining Loss: 0.038125\n",
      "Epoch: 3553 \tTraining Loss: 0.038148\n",
      "Epoch: 3554 \tTraining Loss: 0.038048\n",
      "Epoch: 3555 \tTraining Loss: 0.037140\n",
      "Epoch: 3556 \tTraining Loss: 0.036099\n",
      "Epoch: 3557 \tTraining Loss: 0.048028\n",
      "Epoch: 3558 \tTraining Loss: 0.035135\n",
      "Epoch: 3559 \tTraining Loss: 0.034954\n",
      "Epoch: 3560 \tTraining Loss: 0.034942\n",
      "Epoch: 3561 \tTraining Loss: 0.034852\n",
      "Epoch: 3562 \tTraining Loss: 0.034869\n",
      "Epoch: 3563 \tTraining Loss: 0.034916\n",
      "Epoch: 3564 \tTraining Loss: 0.034822\n",
      "Epoch: 3565 \tTraining Loss: 0.034844\n",
      "Epoch: 3566 \tTraining Loss: 0.034855\n",
      "Epoch: 3567 \tTraining Loss: 0.034800\n",
      "Epoch: 3568 \tTraining Loss: 0.034862\n",
      "Epoch: 3569 \tTraining Loss: 0.034776\n",
      "Epoch: 3570 \tTraining Loss: 0.034846\n",
      "Epoch: 3571 \tTraining Loss: 0.034812\n",
      "Epoch: 3572 \tTraining Loss: 0.034790\n",
      "Epoch: 3573 \tTraining Loss: 0.034759\n",
      "Epoch: 3574 \tTraining Loss: 0.034775\n",
      "Epoch: 3575 \tTraining Loss: 0.034766\n",
      "Epoch: 3576 \tTraining Loss: 0.034914\n",
      "Epoch: 3577 \tTraining Loss: 0.034694\n",
      "Epoch: 3578 \tTraining Loss: 0.034728\n",
      "Epoch: 3579 \tTraining Loss: 0.034800\n",
      "Epoch: 3580 \tTraining Loss: 0.034837\n",
      "Epoch: 3581 \tTraining Loss: 0.034776\n",
      "Epoch: 3582 \tTraining Loss: 0.034955\n",
      "Epoch: 3583 \tTraining Loss: 0.034740\n",
      "Epoch: 3584 \tTraining Loss: 0.034728\n",
      "Epoch: 3585 \tTraining Loss: 0.034709\n",
      "Epoch: 3586 \tTraining Loss: 0.034746\n",
      "Epoch: 3587 \tTraining Loss: 0.034659\n",
      "Epoch: 3588 \tTraining Loss: 0.034706\n",
      "Epoch: 3589 \tTraining Loss: 0.034657\n",
      "Epoch: 3590 \tTraining Loss: 0.034669\n",
      "Epoch: 3591 \tTraining Loss: 0.034745\n",
      "Epoch: 3592 \tTraining Loss: 0.034648\n",
      "Epoch: 3593 \tTraining Loss: 0.034676\n",
      "Epoch: 3594 \tTraining Loss: 0.034625\n",
      "Epoch: 3595 \tTraining Loss: 0.034667\n",
      "Epoch: 3596 \tTraining Loss: 0.034704\n",
      "Epoch: 3597 \tTraining Loss: 0.034728\n",
      "Epoch: 3598 \tTraining Loss: 0.034646\n",
      "Epoch: 3599 \tTraining Loss: 0.034699\n",
      "Epoch: 3600 \tTraining Loss: 0.034626\n",
      "Epoch: 3601 \tTraining Loss: 0.034644\n",
      "Epoch: 3602 \tTraining Loss: 0.034676\n",
      "Epoch: 3603 \tTraining Loss: 0.034611\n",
      "Epoch: 3604 \tTraining Loss: 0.034622\n",
      "Epoch: 3605 \tTraining Loss: 0.034632\n",
      "Epoch: 3606 \tTraining Loss: 0.034650\n",
      "Epoch: 3607 \tTraining Loss: 0.034673\n",
      "Epoch: 3608 \tTraining Loss: 0.034823\n",
      "Epoch: 3609 \tTraining Loss: 0.034651\n",
      "Epoch: 3610 \tTraining Loss: 0.034620\n",
      "Epoch: 3611 \tTraining Loss: 0.034667\n",
      "Epoch: 3612 \tTraining Loss: 0.034576\n",
      "Epoch: 3613 \tTraining Loss: 0.034627\n",
      "Epoch: 3614 \tTraining Loss: 0.034592\n",
      "Epoch: 3615 \tTraining Loss: 0.034623\n",
      "Epoch: 3616 \tTraining Loss: 0.034603\n",
      "Epoch: 3617 \tTraining Loss: 0.034628\n",
      "Epoch: 3618 \tTraining Loss: 0.034583\n",
      "Epoch: 3619 \tTraining Loss: 0.034640\n",
      "Epoch: 3620 \tTraining Loss: 0.034589\n",
      "Epoch: 3621 \tTraining Loss: 0.034593\n",
      "Epoch: 3622 \tTraining Loss: 0.034675\n",
      "Epoch: 3623 \tTraining Loss: 0.034635\n",
      "Epoch: 3624 \tTraining Loss: 0.034542\n",
      "Epoch: 3625 \tTraining Loss: 0.034597\n",
      "Epoch: 3626 \tTraining Loss: 0.034869\n",
      "Epoch: 3627 \tTraining Loss: 0.056044\n",
      "Epoch: 3628 \tTraining Loss: 0.048722\n",
      "Epoch: 3629 \tTraining Loss: 0.047135\n",
      "Epoch: 3630 \tTraining Loss: 0.061924\n",
      "Epoch: 3631 \tTraining Loss: 0.061730\n",
      "Epoch: 3632 \tTraining Loss: 0.063837\n",
      "Epoch: 3633 \tTraining Loss: 0.049279\n",
      "Epoch: 3634 \tTraining Loss: 0.054263\n",
      "Epoch: 3635 \tTraining Loss: 0.044345\n",
      "Epoch: 3636 \tTraining Loss: 0.107727\n",
      "Epoch: 3637 \tTraining Loss: 0.071107\n",
      "Epoch: 3638 \tTraining Loss: 0.108935\n",
      "Epoch: 3639 \tTraining Loss: 0.094698\n",
      "Epoch: 3640 \tTraining Loss: 0.110910\n",
      "Epoch: 3641 \tTraining Loss: 0.075578\n",
      "Epoch: 3642 \tTraining Loss: 0.055855\n",
      "Epoch: 3643 \tTraining Loss: 0.050863\n",
      "Epoch: 3644 \tTraining Loss: 0.046566\n",
      "Epoch: 3645 \tTraining Loss: 0.048473\n",
      "Epoch: 3646 \tTraining Loss: 0.045804\n",
      "Epoch: 3647 \tTraining Loss: 0.045116\n",
      "Epoch: 3648 \tTraining Loss: 0.045153\n",
      "Epoch: 3649 \tTraining Loss: 0.044904\n",
      "Epoch: 3650 \tTraining Loss: 0.044859\n",
      "Epoch: 3651 \tTraining Loss: 0.044563\n",
      "Epoch: 3652 \tTraining Loss: 0.040881\n",
      "Epoch: 3653 \tTraining Loss: 0.043665\n",
      "Epoch: 3654 \tTraining Loss: 0.046398\n",
      "Epoch: 3655 \tTraining Loss: 0.041155\n",
      "Epoch: 3656 \tTraining Loss: 0.038918\n",
      "Epoch: 3657 \tTraining Loss: 0.038873\n",
      "Epoch: 3658 \tTraining Loss: 0.038095\n",
      "Epoch: 3659 \tTraining Loss: 0.038339\n",
      "Epoch: 3660 \tTraining Loss: 0.037945\n",
      "Epoch: 3661 \tTraining Loss: 0.037854\n",
      "Epoch: 3662 \tTraining Loss: 0.037911\n",
      "Epoch: 3663 \tTraining Loss: 0.037721\n",
      "Epoch: 3664 \tTraining Loss: 0.037652\n",
      "Epoch: 3665 \tTraining Loss: 0.037671\n",
      "Epoch: 3666 \tTraining Loss: 0.037652\n",
      "Epoch: 3667 \tTraining Loss: 0.037677\n",
      "Epoch: 3668 \tTraining Loss: 0.037517\n",
      "Epoch: 3669 \tTraining Loss: 0.037532\n",
      "Epoch: 3670 \tTraining Loss: 0.037507\n",
      "Epoch: 3671 \tTraining Loss: 0.038141\n",
      "Epoch: 3672 \tTraining Loss: 0.037677\n",
      "Epoch: 3673 \tTraining Loss: 0.037402\n",
      "Epoch: 3674 \tTraining Loss: 0.037408\n",
      "Epoch: 3675 \tTraining Loss: 0.037359\n",
      "Epoch: 3676 \tTraining Loss: 0.037372\n",
      "Epoch: 3677 \tTraining Loss: 0.037488\n",
      "Epoch: 3678 \tTraining Loss: 0.037362\n",
      "Epoch: 3679 \tTraining Loss: 0.037401\n",
      "Epoch: 3680 \tTraining Loss: 0.035974\n",
      "Epoch: 3681 \tTraining Loss: 0.035698\n",
      "Epoch: 3682 \tTraining Loss: 0.035676\n",
      "Epoch: 3683 \tTraining Loss: 0.035677\n",
      "Epoch: 3684 \tTraining Loss: 0.035680\n",
      "Epoch: 3685 \tTraining Loss: 0.035654\n",
      "Epoch: 3686 \tTraining Loss: 0.035510\n",
      "Epoch: 3687 \tTraining Loss: 0.042922\n",
      "Epoch: 3688 \tTraining Loss: 0.035896\n",
      "Epoch: 3689 \tTraining Loss: 0.035639\n",
      "Epoch: 3690 \tTraining Loss: 0.035656\n",
      "Epoch: 3691 \tTraining Loss: 0.035629\n",
      "Epoch: 3692 \tTraining Loss: 0.035633\n",
      "Epoch: 3693 \tTraining Loss: 0.035626\n",
      "Epoch: 3694 \tTraining Loss: 0.035605\n",
      "Epoch: 3695 \tTraining Loss: 0.035694\n",
      "Epoch: 3696 \tTraining Loss: 0.035715\n",
      "Epoch: 3697 \tTraining Loss: 0.035608\n",
      "Epoch: 3698 \tTraining Loss: 0.035617\n",
      "Epoch: 3699 \tTraining Loss: 0.035656\n",
      "Epoch: 3700 \tTraining Loss: 0.035556\n",
      "Epoch: 3701 \tTraining Loss: 0.035608\n",
      "Epoch: 3702 \tTraining Loss: 0.035598\n",
      "Epoch: 3703 \tTraining Loss: 0.035632\n",
      "Epoch: 3704 \tTraining Loss: 0.035637\n",
      "Epoch: 3705 \tTraining Loss: 0.035561\n",
      "Epoch: 3706 \tTraining Loss: 0.035543\n",
      "Epoch: 3707 \tTraining Loss: 0.035534\n",
      "Epoch: 3708 \tTraining Loss: 0.035568\n",
      "Epoch: 3709 \tTraining Loss: 0.035538\n",
      "Epoch: 3710 \tTraining Loss: 0.035544\n",
      "Epoch: 3711 \tTraining Loss: 0.035532\n",
      "Epoch: 3712 \tTraining Loss: 0.035525\n",
      "Epoch: 3713 \tTraining Loss: 0.035536\n",
      "Epoch: 3714 \tTraining Loss: 0.035558\n",
      "Epoch: 3715 \tTraining Loss: 0.035538\n",
      "Epoch: 3716 \tTraining Loss: 0.035557\n",
      "Epoch: 3717 \tTraining Loss: 0.035634\n",
      "Epoch: 3718 \tTraining Loss: 0.035541\n",
      "Epoch: 3719 \tTraining Loss: 0.035531\n",
      "Epoch: 3720 \tTraining Loss: 0.035531\n",
      "Epoch: 3721 \tTraining Loss: 0.035513\n",
      "Epoch: 3722 \tTraining Loss: 0.035543\n",
      "Epoch: 3723 \tTraining Loss: 0.035596\n",
      "Epoch: 3724 \tTraining Loss: 0.035546\n",
      "Epoch: 3725 \tTraining Loss: 0.035601\n",
      "Epoch: 3726 \tTraining Loss: 0.035529\n",
      "Epoch: 3727 \tTraining Loss: 0.035545\n",
      "Epoch: 3728 \tTraining Loss: 0.035517\n",
      "Epoch: 3729 \tTraining Loss: 0.035515\n",
      "Epoch: 3730 \tTraining Loss: 0.035492\n",
      "Epoch: 3731 \tTraining Loss: 0.035477\n",
      "Epoch: 3732 \tTraining Loss: 0.035517\n",
      "Epoch: 3733 \tTraining Loss: 0.035615\n",
      "Epoch: 3734 \tTraining Loss: 0.035509\n",
      "Epoch: 3735 \tTraining Loss: 0.035493\n",
      "Epoch: 3736 \tTraining Loss: 0.035486\n",
      "Epoch: 3737 \tTraining Loss: 0.035507\n",
      "Epoch: 3738 \tTraining Loss: 0.035581\n",
      "Epoch: 3739 \tTraining Loss: 0.035471\n",
      "Epoch: 3740 \tTraining Loss: 0.035471\n",
      "Epoch: 3741 \tTraining Loss: 0.035502\n",
      "Epoch: 3742 \tTraining Loss: 0.035492\n",
      "Epoch: 3743 \tTraining Loss: 0.035477\n",
      "Epoch: 3744 \tTraining Loss: 0.035502\n",
      "Epoch: 3745 \tTraining Loss: 0.035466\n",
      "Epoch: 3746 \tTraining Loss: 0.035462\n",
      "Epoch: 3747 \tTraining Loss: 0.035457\n",
      "Epoch: 3748 \tTraining Loss: 0.035462\n",
      "Epoch: 3749 \tTraining Loss: 0.035454\n",
      "Epoch: 3750 \tTraining Loss: 0.035484\n",
      "Epoch: 3751 \tTraining Loss: 0.035436\n",
      "Epoch: 3752 \tTraining Loss: 0.035461\n",
      "Epoch: 3753 \tTraining Loss: 0.035460\n",
      "Epoch: 3754 \tTraining Loss: 0.035431\n",
      "Epoch: 3755 \tTraining Loss: 0.035453\n",
      "Epoch: 3756 \tTraining Loss: 0.035514\n",
      "Epoch: 3757 \tTraining Loss: 0.035473\n",
      "Epoch: 3758 \tTraining Loss: 0.035489\n",
      "Epoch: 3759 \tTraining Loss: 0.035501\n",
      "Epoch: 3760 \tTraining Loss: 0.035523\n",
      "Epoch: 3761 \tTraining Loss: 0.035468\n",
      "Epoch: 3762 \tTraining Loss: 0.035522\n",
      "Epoch: 3763 \tTraining Loss: 0.035445\n",
      "Epoch: 3764 \tTraining Loss: 0.035450\n",
      "Epoch: 3765 \tTraining Loss: 0.035430\n",
      "Epoch: 3766 \tTraining Loss: 0.035440\n",
      "Epoch: 3767 \tTraining Loss: 0.035461\n",
      "Epoch: 3768 \tTraining Loss: 0.035434\n",
      "Epoch: 3769 \tTraining Loss: 0.035483\n",
      "Epoch: 3770 \tTraining Loss: 0.035437\n",
      "Epoch: 3771 \tTraining Loss: 0.035419\n",
      "Epoch: 3772 \tTraining Loss: 0.035520\n",
      "Epoch: 3773 \tTraining Loss: 0.035443\n",
      "Epoch: 3774 \tTraining Loss: 0.035500\n",
      "Epoch: 3775 \tTraining Loss: 0.035498\n",
      "Epoch: 3776 \tTraining Loss: 0.035454\n",
      "Epoch: 3777 \tTraining Loss: 0.035412\n",
      "Epoch: 3778 \tTraining Loss: 0.035437\n",
      "Epoch: 3779 \tTraining Loss: 0.035463\n",
      "Epoch: 3780 \tTraining Loss: 0.035414\n",
      "Epoch: 3781 \tTraining Loss: 0.035416\n",
      "Epoch: 3782 \tTraining Loss: 0.035402\n",
      "Epoch: 3783 \tTraining Loss: 0.035448\n",
      "Epoch: 3784 \tTraining Loss: 0.035430\n",
      "Epoch: 3785 \tTraining Loss: 0.035455\n",
      "Epoch: 3786 \tTraining Loss: 0.035414\n",
      "Epoch: 3787 \tTraining Loss: 0.035438\n",
      "Epoch: 3788 \tTraining Loss: 0.035430\n",
      "Epoch: 3789 \tTraining Loss: 0.065982\n",
      "Epoch: 3790 \tTraining Loss: 0.217349\n",
      "Epoch: 3791 \tTraining Loss: 0.228127\n",
      "Epoch: 3792 \tTraining Loss: 0.342840\n",
      "Epoch: 3793 \tTraining Loss: 0.176031\n",
      "Epoch: 3794 \tTraining Loss: 0.101244\n",
      "Epoch: 3795 \tTraining Loss: 0.121864\n",
      "Epoch: 3796 \tTraining Loss: 0.047709\n",
      "Epoch: 3797 \tTraining Loss: 0.040845\n",
      "Epoch: 3798 \tTraining Loss: 0.046679\n",
      "Epoch: 3799 \tTraining Loss: 0.037110\n",
      "Epoch: 3800 \tTraining Loss: 0.036978\n",
      "Epoch: 3801 \tTraining Loss: 0.038747\n",
      "Epoch: 3802 \tTraining Loss: 0.036579\n",
      "Epoch: 3803 \tTraining Loss: 0.036244\n",
      "Epoch: 3804 \tTraining Loss: 0.034810\n",
      "Epoch: 3805 \tTraining Loss: 0.034550\n",
      "Epoch: 3806 \tTraining Loss: 0.034408\n",
      "Epoch: 3807 \tTraining Loss: 0.034448\n",
      "Epoch: 3808 \tTraining Loss: 0.034360\n",
      "Epoch: 3809 \tTraining Loss: 0.034224\n",
      "Epoch: 3810 \tTraining Loss: 0.034131\n",
      "Epoch: 3811 \tTraining Loss: 0.033827\n",
      "Epoch: 3812 \tTraining Loss: 0.033990\n",
      "Epoch: 3813 \tTraining Loss: 0.033742\n",
      "Epoch: 3814 \tTraining Loss: 0.033698\n",
      "Epoch: 3815 \tTraining Loss: 0.033478\n",
      "Epoch: 3816 \tTraining Loss: 0.033534\n",
      "Epoch: 3817 \tTraining Loss: 0.033411\n",
      "Epoch: 3818 \tTraining Loss: 0.033427\n",
      "Epoch: 3819 \tTraining Loss: 0.033471\n",
      "Epoch: 3820 \tTraining Loss: 0.033397\n",
      "Epoch: 3821 \tTraining Loss: 0.033399\n",
      "Epoch: 3822 \tTraining Loss: 0.033449\n",
      "Epoch: 3823 \tTraining Loss: 0.033347\n",
      "Epoch: 3824 \tTraining Loss: 0.033376\n",
      "Epoch: 3825 \tTraining Loss: 0.033343\n",
      "Epoch: 3826 \tTraining Loss: 0.033399\n",
      "Epoch: 3827 \tTraining Loss: 0.033280\n",
      "Epoch: 3828 \tTraining Loss: 0.033354\n",
      "Epoch: 3829 \tTraining Loss: 0.033341\n",
      "Epoch: 3830 \tTraining Loss: 0.033357\n",
      "Epoch: 3831 \tTraining Loss: 0.033291\n",
      "Epoch: 3832 \tTraining Loss: 0.033264\n",
      "Epoch: 3833 \tTraining Loss: 0.033360\n",
      "Epoch: 3834 \tTraining Loss: 0.033369\n",
      "Epoch: 3835 \tTraining Loss: 0.033253\n",
      "Epoch: 3836 \tTraining Loss: 0.033276\n",
      "Epoch: 3837 \tTraining Loss: 0.033359\n",
      "Epoch: 3838 \tTraining Loss: 0.033244\n",
      "Epoch: 3839 \tTraining Loss: 0.033303\n",
      "Epoch: 3840 \tTraining Loss: 0.033260\n",
      "Epoch: 3841 \tTraining Loss: 0.033259\n",
      "Epoch: 3842 \tTraining Loss: 0.033259\n",
      "Epoch: 3843 \tTraining Loss: 0.033260\n",
      "Epoch: 3844 \tTraining Loss: 0.033222\n",
      "Epoch: 3845 \tTraining Loss: 0.033306\n",
      "Epoch: 3846 \tTraining Loss: 0.033204\n",
      "Epoch: 3847 \tTraining Loss: 0.033280\n",
      "Epoch: 3848 \tTraining Loss: 0.033274\n",
      "Epoch: 3849 \tTraining Loss: 0.033414\n",
      "Epoch: 3850 \tTraining Loss: 0.033260\n",
      "Epoch: 3851 \tTraining Loss: 0.033134\n",
      "Epoch: 3852 \tTraining Loss: 0.033164\n",
      "Epoch: 3853 \tTraining Loss: 0.033173\n",
      "Epoch: 3854 \tTraining Loss: 0.033195\n",
      "Epoch: 3855 \tTraining Loss: 0.033207\n",
      "Epoch: 3856 \tTraining Loss: 0.033193\n",
      "Epoch: 3857 \tTraining Loss: 0.033141\n",
      "Epoch: 3858 \tTraining Loss: 0.033162\n",
      "Epoch: 3859 \tTraining Loss: 0.033240\n",
      "Epoch: 3860 \tTraining Loss: 0.033112\n",
      "Epoch: 3861 \tTraining Loss: 0.033149\n",
      "Epoch: 3862 \tTraining Loss: 0.033150\n",
      "Epoch: 3863 \tTraining Loss: 0.033129\n",
      "Epoch: 3864 \tTraining Loss: 0.033122\n",
      "Epoch: 3865 \tTraining Loss: 0.033148\n",
      "Epoch: 3866 \tTraining Loss: 0.033121\n",
      "Epoch: 3867 \tTraining Loss: 0.033156\n",
      "Epoch: 3868 \tTraining Loss: 0.033102\n",
      "Epoch: 3869 \tTraining Loss: 0.033126\n",
      "Epoch: 3870 \tTraining Loss: 0.033081\n",
      "Epoch: 3871 \tTraining Loss: 0.033205\n",
      "Epoch: 3872 \tTraining Loss: 0.033084\n",
      "Epoch: 3873 \tTraining Loss: 0.033139\n",
      "Epoch: 3874 \tTraining Loss: 0.033185\n",
      "Epoch: 3875 \tTraining Loss: 0.033089\n",
      "Epoch: 3876 \tTraining Loss: 0.033095\n",
      "Epoch: 3877 \tTraining Loss: 0.033102\n",
      "Epoch: 3878 \tTraining Loss: 0.033158\n",
      "Epoch: 3879 \tTraining Loss: 0.033092\n",
      "Epoch: 3880 \tTraining Loss: 0.033113\n",
      "Epoch: 3881 \tTraining Loss: 0.033201\n",
      "Epoch: 3882 \tTraining Loss: 0.033104\n",
      "Epoch: 3883 \tTraining Loss: 0.033095\n",
      "Epoch: 3884 \tTraining Loss: 0.033085\n",
      "Epoch: 3885 \tTraining Loss: 0.033110\n",
      "Epoch: 3886 \tTraining Loss: 0.037255\n",
      "Epoch: 3887 \tTraining Loss: 0.033433\n",
      "Epoch: 3888 \tTraining Loss: 0.033256\n",
      "Epoch: 3889 \tTraining Loss: 0.033150\n",
      "Epoch: 3890 \tTraining Loss: 0.033086\n",
      "Epoch: 3891 \tTraining Loss: 0.033177\n",
      "Epoch: 3892 \tTraining Loss: 0.033102\n",
      "Epoch: 3893 \tTraining Loss: 0.033128\n",
      "Epoch: 3894 \tTraining Loss: 0.033169\n",
      "Epoch: 3895 \tTraining Loss: 0.033096\n",
      "Epoch: 3896 \tTraining Loss: 0.033088\n",
      "Epoch: 3897 \tTraining Loss: 0.033102\n",
      "Epoch: 3898 \tTraining Loss: 0.033084\n",
      "Epoch: 3899 \tTraining Loss: 0.033077\n",
      "Epoch: 3900 \tTraining Loss: 0.033073\n",
      "Epoch: 3901 \tTraining Loss: 0.033076\n",
      "Epoch: 3902 \tTraining Loss: 0.033094\n",
      "Epoch: 3903 \tTraining Loss: 0.033050\n",
      "Epoch: 3904 \tTraining Loss: 0.033047\n",
      "Epoch: 3905 \tTraining Loss: 0.033288\n",
      "Epoch: 3906 \tTraining Loss: 0.033183\n",
      "Epoch: 3907 \tTraining Loss: 0.033061\n",
      "Epoch: 3908 \tTraining Loss: 0.033081\n",
      "Epoch: 3909 \tTraining Loss: 0.033024\n",
      "Epoch: 3910 \tTraining Loss: 0.033046\n",
      "Epoch: 3911 \tTraining Loss: 0.033042\n",
      "Epoch: 3912 \tTraining Loss: 0.033039\n",
      "Epoch: 3913 \tTraining Loss: 0.033022\n",
      "Epoch: 3914 \tTraining Loss: 0.033098\n",
      "Epoch: 3915 \tTraining Loss: 0.033023\n",
      "Epoch: 3916 \tTraining Loss: 0.033027\n",
      "Epoch: 3917 \tTraining Loss: 0.033007\n",
      "Epoch: 3918 \tTraining Loss: 0.033028\n",
      "Epoch: 3919 \tTraining Loss: 0.033055\n",
      "Epoch: 3920 \tTraining Loss: 0.033060\n",
      "Epoch: 3921 \tTraining Loss: 0.033009\n",
      "Epoch: 3922 \tTraining Loss: 0.033056\n",
      "Epoch: 3923 \tTraining Loss: 0.033040\n",
      "Epoch: 3924 \tTraining Loss: 0.033027\n",
      "Epoch: 3925 \tTraining Loss: 0.033000\n",
      "Epoch: 3926 \tTraining Loss: 0.033032\n",
      "Epoch: 3927 \tTraining Loss: 0.032983\n",
      "Epoch: 3928 \tTraining Loss: 0.033052\n",
      "Epoch: 3929 \tTraining Loss: 0.033083\n",
      "Epoch: 3930 \tTraining Loss: 0.032975\n",
      "Epoch: 3931 \tTraining Loss: 0.033035\n",
      "Epoch: 3932 \tTraining Loss: 0.033030\n",
      "Epoch: 3933 \tTraining Loss: 0.032974\n",
      "Epoch: 3934 \tTraining Loss: 0.033015\n",
      "Epoch: 3935 \tTraining Loss: 0.033123\n",
      "Epoch: 3936 \tTraining Loss: 0.032963\n",
      "Epoch: 3937 \tTraining Loss: 0.032984\n",
      "Epoch: 3938 \tTraining Loss: 0.033015\n",
      "Epoch: 3939 \tTraining Loss: 0.032986\n",
      "Epoch: 3940 \tTraining Loss: 0.032995\n",
      "Epoch: 3941 \tTraining Loss: 0.032978\n",
      "Epoch: 3942 \tTraining Loss: 0.032986\n",
      "Epoch: 3943 \tTraining Loss: 0.033033\n",
      "Epoch: 3944 \tTraining Loss: 0.033016\n",
      "Epoch: 3945 \tTraining Loss: 0.032979\n",
      "Epoch: 3946 \tTraining Loss: 0.032972\n",
      "Epoch: 3947 \tTraining Loss: 0.033082\n",
      "Epoch: 3948 \tTraining Loss: 0.033125\n",
      "Epoch: 3949 \tTraining Loss: 0.032988\n",
      "Epoch: 3950 \tTraining Loss: 0.033112\n",
      "Epoch: 3951 \tTraining Loss: 0.032980\n",
      "Epoch: 3952 \tTraining Loss: 0.040761\n",
      "Epoch: 3953 \tTraining Loss: 0.041405\n",
      "Epoch: 3954 \tTraining Loss: 0.084573\n",
      "Epoch: 3955 \tTraining Loss: 0.103208\n",
      "Epoch: 3956 \tTraining Loss: 0.231873\n",
      "Epoch: 3957 \tTraining Loss: 0.254061\n",
      "Epoch: 3958 \tTraining Loss: 0.184335\n",
      "Epoch: 3959 \tTraining Loss: 0.078676\n",
      "Epoch: 3960 \tTraining Loss: 0.067017\n",
      "Epoch: 3961 \tTraining Loss: 0.051258\n",
      "Epoch: 3962 \tTraining Loss: 0.057787\n",
      "Epoch: 3963 \tTraining Loss: 0.052439\n",
      "Epoch: 3964 \tTraining Loss: 0.050288\n",
      "Epoch: 3965 \tTraining Loss: 0.045479\n",
      "Epoch: 3966 \tTraining Loss: 0.044654\n",
      "Epoch: 3967 \tTraining Loss: 0.043804\n",
      "Epoch: 3968 \tTraining Loss: 0.060852\n",
      "Epoch: 3969 \tTraining Loss: 0.048497\n",
      "Epoch: 3970 \tTraining Loss: 0.047758\n",
      "Epoch: 3971 \tTraining Loss: 0.042121\n",
      "Epoch: 3972 \tTraining Loss: 0.042457\n",
      "Epoch: 3973 \tTraining Loss: 0.041524\n",
      "Epoch: 3974 \tTraining Loss: 0.041497\n",
      "Epoch: 3975 \tTraining Loss: 0.041419\n",
      "Epoch: 3976 \tTraining Loss: 0.041372\n",
      "Epoch: 3977 \tTraining Loss: 0.041299\n",
      "Epoch: 3978 \tTraining Loss: 0.041104\n",
      "Epoch: 3979 \tTraining Loss: 0.041093\n",
      "Epoch: 3980 \tTraining Loss: 0.041129\n",
      "Epoch: 3981 \tTraining Loss: 0.042290\n",
      "Epoch: 3982 \tTraining Loss: 0.041197\n",
      "Epoch: 3983 \tTraining Loss: 0.040970\n",
      "Epoch: 3984 \tTraining Loss: 0.041009\n",
      "Epoch: 3985 \tTraining Loss: 0.041139\n",
      "Epoch: 3986 \tTraining Loss: 0.040823\n",
      "Epoch: 3987 \tTraining Loss: 0.040884\n",
      "Epoch: 3988 \tTraining Loss: 0.040814\n",
      "Epoch: 3989 \tTraining Loss: 0.040823\n",
      "Epoch: 3990 \tTraining Loss: 0.040878\n",
      "Epoch: 3991 \tTraining Loss: 0.040835\n",
      "Epoch: 3992 \tTraining Loss: 0.040742\n",
      "Epoch: 3993 \tTraining Loss: 0.040790\n",
      "Epoch: 3994 \tTraining Loss: 0.040731\n",
      "Epoch: 3995 \tTraining Loss: 0.040780\n",
      "Epoch: 3996 \tTraining Loss: 0.040749\n",
      "Epoch: 3997 \tTraining Loss: 0.040799\n",
      "Epoch: 3998 \tTraining Loss: 0.040790\n",
      "Epoch: 3999 \tTraining Loss: 0.040696\n",
      "Epoch: 4000 \tTraining Loss: 0.040764\n",
      "Epoch: 4001 \tTraining Loss: 0.040701\n",
      "Epoch: 4002 \tTraining Loss: 0.040663\n",
      "Epoch: 4003 \tTraining Loss: 0.040761\n",
      "Epoch: 4004 \tTraining Loss: 0.040681\n",
      "Epoch: 4005 \tTraining Loss: 0.040660\n",
      "Epoch: 4006 \tTraining Loss: 0.040847\n",
      "Epoch: 4007 \tTraining Loss: 0.040990\n",
      "Epoch: 4008 \tTraining Loss: 0.040857\n",
      "Epoch: 4009 \tTraining Loss: 0.041668\n",
      "Epoch: 4010 \tTraining Loss: 0.040664\n",
      "Epoch: 4011 \tTraining Loss: 0.040625\n",
      "Epoch: 4012 \tTraining Loss: 0.040728\n",
      "Epoch: 4013 \tTraining Loss: 0.040625\n",
      "Epoch: 4014 \tTraining Loss: 0.040834\n",
      "Epoch: 4015 \tTraining Loss: 0.040604\n",
      "Epoch: 4016 \tTraining Loss: 0.040605\n",
      "Epoch: 4017 \tTraining Loss: 0.040539\n",
      "Epoch: 4018 \tTraining Loss: 0.040663\n",
      "Epoch: 4019 \tTraining Loss: 0.040562\n",
      "Epoch: 4020 \tTraining Loss: 0.040533\n",
      "Epoch: 4021 \tTraining Loss: 0.040566\n",
      "Epoch: 4022 \tTraining Loss: 0.040519\n",
      "Epoch: 4023 \tTraining Loss: 0.040524\n",
      "Epoch: 4024 \tTraining Loss: 0.040553\n",
      "Epoch: 4025 \tTraining Loss: 0.040572\n",
      "Epoch: 4026 \tTraining Loss: 0.040520\n",
      "Epoch: 4027 \tTraining Loss: 0.040498\n",
      "Epoch: 4028 \tTraining Loss: 0.040550\n",
      "Epoch: 4029 \tTraining Loss: 0.040493\n",
      "Epoch: 4030 \tTraining Loss: 0.040513\n",
      "Epoch: 4031 \tTraining Loss: 0.040492\n",
      "Epoch: 4032 \tTraining Loss: 0.040522\n",
      "Epoch: 4033 \tTraining Loss: 0.040479\n",
      "Epoch: 4034 \tTraining Loss: 0.040497\n",
      "Epoch: 4035 \tTraining Loss: 0.040500\n",
      "Epoch: 4036 \tTraining Loss: 0.040480\n",
      "Epoch: 4037 \tTraining Loss: 0.040498\n",
      "Epoch: 4038 \tTraining Loss: 0.040504\n",
      "Epoch: 4039 \tTraining Loss: 0.040502\n",
      "Epoch: 4040 \tTraining Loss: 0.040506\n",
      "Epoch: 4041 \tTraining Loss: 0.040452\n",
      "Epoch: 4042 \tTraining Loss: 0.040532\n",
      "Epoch: 4043 \tTraining Loss: 0.040455\n",
      "Epoch: 4044 \tTraining Loss: 0.040715\n",
      "Epoch: 4045 \tTraining Loss: 0.040603\n",
      "Epoch: 4046 \tTraining Loss: 0.040467\n",
      "Epoch: 4047 \tTraining Loss: 0.040483\n",
      "Epoch: 4048 \tTraining Loss: 0.040383\n",
      "Epoch: 4049 \tTraining Loss: 0.040564\n",
      "Epoch: 4050 \tTraining Loss: 0.040528\n",
      "Epoch: 4051 \tTraining Loss: 0.040474\n",
      "Epoch: 4052 \tTraining Loss: 0.040470\n",
      "Epoch: 4053 \tTraining Loss: 0.040664\n",
      "Epoch: 4054 \tTraining Loss: 0.042317\n",
      "Epoch: 4055 \tTraining Loss: 0.041116\n",
      "Epoch: 4056 \tTraining Loss: 0.040439\n",
      "Epoch: 4057 \tTraining Loss: 0.040420\n",
      "Epoch: 4058 \tTraining Loss: 0.040400\n",
      "Epoch: 4059 \tTraining Loss: 0.040375\n",
      "Epoch: 4060 \tTraining Loss: 0.040367\n",
      "Epoch: 4061 \tTraining Loss: 0.040418\n",
      "Epoch: 4062 \tTraining Loss: 0.040346\n",
      "Epoch: 4063 \tTraining Loss: 0.040378\n",
      "Epoch: 4064 \tTraining Loss: 0.040445\n",
      "Epoch: 4065 \tTraining Loss: 0.040411\n",
      "Epoch: 4066 \tTraining Loss: 0.040403\n",
      "Epoch: 4067 \tTraining Loss: 0.040401\n",
      "Epoch: 4068 \tTraining Loss: 0.040371\n",
      "Epoch: 4069 \tTraining Loss: 0.040413\n",
      "Epoch: 4070 \tTraining Loss: 0.040348\n",
      "Epoch: 4071 \tTraining Loss: 0.040360\n",
      "Epoch: 4072 \tTraining Loss: 0.040397\n",
      "Epoch: 4073 \tTraining Loss: 0.040333\n",
      "Epoch: 4074 \tTraining Loss: 0.040396\n",
      "Epoch: 4075 \tTraining Loss: 0.040337\n",
      "Epoch: 4076 \tTraining Loss: 0.040368\n",
      "Epoch: 4077 \tTraining Loss: 0.040360\n",
      "Epoch: 4078 \tTraining Loss: 0.040405\n",
      "Epoch: 4079 \tTraining Loss: 0.040368\n",
      "Epoch: 4080 \tTraining Loss: 0.040388\n",
      "Epoch: 4081 \tTraining Loss: 0.040349\n",
      "Epoch: 4082 \tTraining Loss: 0.040549\n",
      "Epoch: 4083 \tTraining Loss: 0.040323\n",
      "Epoch: 4084 \tTraining Loss: 0.040357\n",
      "Epoch: 4085 \tTraining Loss: 0.040379\n",
      "Epoch: 4086 \tTraining Loss: 0.040402\n",
      "Epoch: 4087 \tTraining Loss: 0.040375\n",
      "Epoch: 4088 \tTraining Loss: 0.040344\n",
      "Epoch: 4089 \tTraining Loss: 0.040386\n",
      "Epoch: 4090 \tTraining Loss: 0.040356\n",
      "Epoch: 4091 \tTraining Loss: 0.040347\n",
      "Epoch: 4092 \tTraining Loss: 0.040332\n",
      "Epoch: 4093 \tTraining Loss: 0.040387\n",
      "Epoch: 4094 \tTraining Loss: 0.040328\n",
      "Epoch: 4095 \tTraining Loss: 0.040387\n",
      "Epoch: 4096 \tTraining Loss: 0.040363\n",
      "Epoch: 4097 \tTraining Loss: 0.040331\n",
      "Epoch: 4098 \tTraining Loss: 0.040322\n",
      "Epoch: 4099 \tTraining Loss: 0.040326\n",
      "Epoch: 4100 \tTraining Loss: 0.040321\n",
      "Epoch: 4101 \tTraining Loss: 0.040303\n",
      "Epoch: 4102 \tTraining Loss: 0.040330\n",
      "Epoch: 4103 \tTraining Loss: 0.040330\n",
      "Epoch: 4104 \tTraining Loss: 0.041293\n",
      "Epoch: 4105 \tTraining Loss: 0.039611\n",
      "Epoch: 4106 \tTraining Loss: 0.071070\n",
      "Epoch: 4107 \tTraining Loss: 0.127930\n",
      "Epoch: 4108 \tTraining Loss: 0.179783\n",
      "Epoch: 4109 \tTraining Loss: 0.178144\n",
      "Epoch: 4110 \tTraining Loss: 0.232732\n",
      "Epoch: 4111 \tTraining Loss: 0.081396\n",
      "Epoch: 4112 \tTraining Loss: 0.092418\n",
      "Epoch: 4113 \tTraining Loss: 0.064366\n",
      "Epoch: 4114 \tTraining Loss: 0.047977\n",
      "Epoch: 4115 \tTraining Loss: 0.054251\n",
      "Epoch: 4116 \tTraining Loss: 0.051096\n",
      "Epoch: 4117 \tTraining Loss: 0.044689\n",
      "Epoch: 4118 \tTraining Loss: 0.042041\n",
      "Epoch: 4119 \tTraining Loss: 0.041732\n",
      "Epoch: 4120 \tTraining Loss: 0.041855\n",
      "Epoch: 4121 \tTraining Loss: 0.041644\n",
      "Epoch: 4122 \tTraining Loss: 0.041406\n",
      "Epoch: 4123 \tTraining Loss: 0.042095\n",
      "Epoch: 4124 \tTraining Loss: 0.042091\n",
      "Epoch: 4125 \tTraining Loss: 0.041438\n",
      "Epoch: 4126 \tTraining Loss: 0.041874\n",
      "Epoch: 4127 \tTraining Loss: 0.041650\n",
      "Epoch: 4128 \tTraining Loss: 0.041479\n",
      "Epoch: 4129 \tTraining Loss: 0.041592\n",
      "Epoch: 4130 \tTraining Loss: 0.041845\n",
      "Epoch: 4131 \tTraining Loss: 0.042249\n",
      "Epoch: 4132 \tTraining Loss: 0.048561\n",
      "Epoch: 4133 \tTraining Loss: 0.042769\n",
      "Epoch: 4134 \tTraining Loss: 0.045832\n",
      "Epoch: 4135 \tTraining Loss: 0.042321\n",
      "Epoch: 4136 \tTraining Loss: 0.041165\n",
      "Epoch: 4137 \tTraining Loss: 0.041255\n",
      "Epoch: 4138 \tTraining Loss: 0.041411\n",
      "Epoch: 4139 \tTraining Loss: 0.041032\n",
      "Epoch: 4140 \tTraining Loss: 0.041041\n",
      "Epoch: 4141 \tTraining Loss: 0.041094\n",
      "Epoch: 4142 \tTraining Loss: 0.041180\n",
      "Epoch: 4143 \tTraining Loss: 0.041026\n",
      "Epoch: 4144 \tTraining Loss: 0.041632\n",
      "Epoch: 4145 \tTraining Loss: 0.040936\n",
      "Epoch: 4146 \tTraining Loss: 0.041206\n",
      "Epoch: 4147 \tTraining Loss: 0.041263\n",
      "Epoch: 4148 \tTraining Loss: 0.040887\n",
      "Epoch: 4149 \tTraining Loss: 0.040894\n",
      "Epoch: 4150 \tTraining Loss: 0.041065\n",
      "Epoch: 4151 \tTraining Loss: 0.040835\n",
      "Epoch: 4152 \tTraining Loss: 0.040855\n",
      "Epoch: 4153 \tTraining Loss: 0.040972\n",
      "Epoch: 4154 \tTraining Loss: 0.041079\n",
      "Epoch: 4155 \tTraining Loss: 0.040960\n",
      "Epoch: 4156 \tTraining Loss: 0.040877\n",
      "Epoch: 4157 \tTraining Loss: 0.041096\n",
      "Epoch: 4158 \tTraining Loss: 0.040765\n",
      "Epoch: 4159 \tTraining Loss: 0.040921\n",
      "Epoch: 4160 \tTraining Loss: 0.040821\n",
      "Epoch: 4161 \tTraining Loss: 0.040957\n",
      "Epoch: 4162 \tTraining Loss: 0.040866\n",
      "Epoch: 4163 \tTraining Loss: 0.041078\n",
      "Epoch: 4164 \tTraining Loss: 0.040768\n",
      "Epoch: 4165 \tTraining Loss: 0.040844\n",
      "Epoch: 4166 \tTraining Loss: 0.040760\n",
      "Epoch: 4167 \tTraining Loss: 0.040728\n",
      "Epoch: 4168 \tTraining Loss: 0.040723\n",
      "Epoch: 4169 \tTraining Loss: 0.040807\n",
      "Epoch: 4170 \tTraining Loss: 0.040644\n",
      "Epoch: 4171 \tTraining Loss: 0.040661\n",
      "Epoch: 4172 \tTraining Loss: 0.040794\n",
      "Epoch: 4173 \tTraining Loss: 0.040699\n",
      "Epoch: 4174 \tTraining Loss: 0.040640\n",
      "Epoch: 4175 \tTraining Loss: 0.040727\n",
      "Epoch: 4176 \tTraining Loss: 0.040674\n",
      "Epoch: 4177 \tTraining Loss: 0.040649\n",
      "Epoch: 4178 \tTraining Loss: 0.040674\n",
      "Epoch: 4179 \tTraining Loss: 0.041011\n",
      "Epoch: 4180 \tTraining Loss: 0.040667\n",
      "Epoch: 4181 \tTraining Loss: 0.040661\n",
      "Epoch: 4182 \tTraining Loss: 0.040636\n",
      "Epoch: 4183 \tTraining Loss: 0.040625\n",
      "Epoch: 4184 \tTraining Loss: 0.040617\n",
      "Epoch: 4185 \tTraining Loss: 0.040655\n",
      "Epoch: 4186 \tTraining Loss: 0.040640\n",
      "Epoch: 4187 \tTraining Loss: 0.040619\n",
      "Epoch: 4188 \tTraining Loss: 0.040635\n",
      "Epoch: 4189 \tTraining Loss: 0.040577\n",
      "Epoch: 4190 \tTraining Loss: 0.040576\n",
      "Epoch: 4191 \tTraining Loss: 0.040588\n",
      "Epoch: 4192 \tTraining Loss: 0.040622\n",
      "Epoch: 4193 \tTraining Loss: 0.040479\n",
      "Epoch: 4194 \tTraining Loss: 0.040786\n",
      "Epoch: 4195 \tTraining Loss: 0.040511\n",
      "Epoch: 4196 \tTraining Loss: 0.040717\n",
      "Epoch: 4197 \tTraining Loss: 0.040596\n",
      "Epoch: 4198 \tTraining Loss: 0.040525\n",
      "Epoch: 4199 \tTraining Loss: 0.040602\n",
      "Epoch: 4200 \tTraining Loss: 0.040529\n",
      "Epoch: 4201 \tTraining Loss: 0.040573\n",
      "Epoch: 4202 \tTraining Loss: 0.040507\n",
      "Epoch: 4203 \tTraining Loss: 0.040560\n",
      "Epoch: 4204 \tTraining Loss: 0.040555\n",
      "Epoch: 4205 \tTraining Loss: 0.040547\n",
      "Epoch: 4206 \tTraining Loss: 0.040553\n",
      "Epoch: 4207 \tTraining Loss: 0.040612\n",
      "Epoch: 4208 \tTraining Loss: 0.040589\n",
      "Epoch: 4209 \tTraining Loss: 0.040417\n",
      "Epoch: 4210 \tTraining Loss: 0.040402\n",
      "Epoch: 4211 \tTraining Loss: 0.040464\n",
      "Epoch: 4212 \tTraining Loss: 0.040418\n",
      "Epoch: 4213 \tTraining Loss: 0.040411\n",
      "Epoch: 4214 \tTraining Loss: 0.040396\n",
      "Epoch: 4215 \tTraining Loss: 0.040371\n",
      "Epoch: 4216 \tTraining Loss: 0.040410\n",
      "Epoch: 4217 \tTraining Loss: 0.040444\n",
      "Epoch: 4218 \tTraining Loss: 0.040366\n",
      "Epoch: 4219 \tTraining Loss: 0.040356\n",
      "Epoch: 4220 \tTraining Loss: 0.040338\n",
      "Epoch: 4221 \tTraining Loss: 0.040382\n",
      "Epoch: 4222 \tTraining Loss: 0.040376\n",
      "Epoch: 4223 \tTraining Loss: 0.040350\n",
      "Epoch: 4224 \tTraining Loss: 0.040352\n",
      "Epoch: 4225 \tTraining Loss: 0.040356\n",
      "Epoch: 4226 \tTraining Loss: 0.040460\n",
      "Epoch: 4227 \tTraining Loss: 0.040362\n",
      "Epoch: 4228 \tTraining Loss: 0.040337\n",
      "Epoch: 4229 \tTraining Loss: 0.040349\n",
      "Epoch: 4230 \tTraining Loss: 0.040351\n",
      "Epoch: 4231 \tTraining Loss: 0.040316\n",
      "Epoch: 4232 \tTraining Loss: 0.040316\n",
      "Epoch: 4233 \tTraining Loss: 0.040336\n",
      "Epoch: 4234 \tTraining Loss: 0.040374\n",
      "Epoch: 4235 \tTraining Loss: 0.040367\n",
      "Epoch: 4236 \tTraining Loss: 0.040386\n",
      "Epoch: 4237 \tTraining Loss: 0.040303\n",
      "Epoch: 4238 \tTraining Loss: 0.040338\n",
      "Epoch: 4239 \tTraining Loss: 0.040322\n",
      "Epoch: 4240 \tTraining Loss: 0.040319\n",
      "Epoch: 4241 \tTraining Loss: 0.040338\n",
      "Epoch: 4242 \tTraining Loss: 0.040378\n",
      "Epoch: 4243 \tTraining Loss: 0.040306\n",
      "Epoch: 4244 \tTraining Loss: 0.040336\n",
      "Epoch: 4245 \tTraining Loss: 0.040341\n",
      "Epoch: 4246 \tTraining Loss: 0.040312\n",
      "Epoch: 4247 \tTraining Loss: 0.040302\n",
      "Epoch: 4248 \tTraining Loss: 0.040348\n",
      "Epoch: 4249 \tTraining Loss: 0.040292\n",
      "Epoch: 4250 \tTraining Loss: 0.040366\n",
      "Epoch: 4251 \tTraining Loss: 0.040317\n",
      "Epoch: 4252 \tTraining Loss: 0.040321\n",
      "Epoch: 4253 \tTraining Loss: 0.040352\n",
      "Epoch: 4254 \tTraining Loss: 0.040308\n",
      "Epoch: 4255 \tTraining Loss: 0.040298\n",
      "Epoch: 4256 \tTraining Loss: 0.040349\n",
      "Epoch: 4257 \tTraining Loss: 0.042698\n",
      "Epoch: 4258 \tTraining Loss: 0.053145\n",
      "Epoch: 4259 \tTraining Loss: 0.120385\n",
      "Epoch: 4260 \tTraining Loss: 0.336179\n",
      "Epoch: 4261 \tTraining Loss: 0.274677\n",
      "Epoch: 4262 \tTraining Loss: 0.185904\n",
      "Epoch: 4263 \tTraining Loss: 0.111789\n",
      "Epoch: 4264 \tTraining Loss: 0.066676\n",
      "Epoch: 4265 \tTraining Loss: 0.079691\n",
      "Epoch: 4266 \tTraining Loss: 0.058312\n",
      "Epoch: 4267 \tTraining Loss: 0.054109\n",
      "Epoch: 4268 \tTraining Loss: 0.050328\n",
      "Epoch: 4269 \tTraining Loss: 0.049871\n",
      "Epoch: 4270 \tTraining Loss: 0.048829\n",
      "Epoch: 4271 \tTraining Loss: 0.049491\n",
      "Epoch: 4272 \tTraining Loss: 0.048840\n",
      "Epoch: 4273 \tTraining Loss: 0.048702\n",
      "Epoch: 4274 \tTraining Loss: 0.048545\n",
      "Epoch: 4275 \tTraining Loss: 0.048302\n",
      "Epoch: 4276 \tTraining Loss: 0.048530\n",
      "Epoch: 4277 \tTraining Loss: 0.048330\n",
      "Epoch: 4278 \tTraining Loss: 0.048420\n",
      "Epoch: 4279 \tTraining Loss: 0.048481\n",
      "Epoch: 4280 \tTraining Loss: 0.067117\n",
      "Epoch: 4281 \tTraining Loss: 0.050134\n",
      "Epoch: 4282 \tTraining Loss: 0.048090\n",
      "Epoch: 4283 \tTraining Loss: 0.047279\n",
      "Epoch: 4284 \tTraining Loss: 0.047344\n",
      "Epoch: 4285 \tTraining Loss: 0.047048\n",
      "Epoch: 4286 \tTraining Loss: 0.047006\n",
      "Epoch: 4287 \tTraining Loss: 0.046921\n",
      "Epoch: 4288 \tTraining Loss: 0.046931\n",
      "Epoch: 4289 \tTraining Loss: 0.047005\n",
      "Epoch: 4290 \tTraining Loss: 0.046957\n",
      "Epoch: 4291 \tTraining Loss: 0.047164\n",
      "Epoch: 4292 \tTraining Loss: 0.046936\n",
      "Epoch: 4293 \tTraining Loss: 0.046859\n",
      "Epoch: 4294 \tTraining Loss: 0.046838\n",
      "Epoch: 4295 \tTraining Loss: 0.046752\n",
      "Epoch: 4296 \tTraining Loss: 0.046775\n",
      "Epoch: 4297 \tTraining Loss: 0.046727\n",
      "Epoch: 4298 \tTraining Loss: 0.046693\n",
      "Epoch: 4299 \tTraining Loss: 0.046740\n",
      "Epoch: 4300 \tTraining Loss: 0.046729\n",
      "Epoch: 4301 \tTraining Loss: 0.046723\n",
      "Epoch: 4302 \tTraining Loss: 0.046730\n",
      "Epoch: 4303 \tTraining Loss: 0.046639\n",
      "Epoch: 4304 \tTraining Loss: 0.046286\n",
      "Epoch: 4305 \tTraining Loss: 0.045649\n",
      "Epoch: 4306 \tTraining Loss: 0.045603\n",
      "Epoch: 4307 \tTraining Loss: 0.045533\n",
      "Epoch: 4308 \tTraining Loss: 0.045581\n",
      "Epoch: 4309 \tTraining Loss: 0.045454\n",
      "Epoch: 4310 \tTraining Loss: 0.045390\n",
      "Epoch: 4311 \tTraining Loss: 0.045401\n",
      "Epoch: 4312 \tTraining Loss: 0.045417\n",
      "Epoch: 4313 \tTraining Loss: 0.045379\n",
      "Epoch: 4314 \tTraining Loss: 0.045368\n",
      "Epoch: 4315 \tTraining Loss: 0.045331\n",
      "Epoch: 4316 \tTraining Loss: 0.045322\n",
      "Epoch: 4317 \tTraining Loss: 0.045369\n",
      "Epoch: 4318 \tTraining Loss: 0.045299\n",
      "Epoch: 4319 \tTraining Loss: 0.045345\n",
      "Epoch: 4320 \tTraining Loss: 0.045338\n",
      "Epoch: 4321 \tTraining Loss: 0.045315\n",
      "Epoch: 4322 \tTraining Loss: 0.045397\n",
      "Epoch: 4323 \tTraining Loss: 0.045272\n",
      "Epoch: 4324 \tTraining Loss: 0.045398\n",
      "Epoch: 4325 \tTraining Loss: 0.045284\n",
      "Epoch: 4326 \tTraining Loss: 0.045247\n",
      "Epoch: 4327 \tTraining Loss: 0.045286\n",
      "Epoch: 4328 \tTraining Loss: 0.045328\n",
      "Epoch: 4329 \tTraining Loss: 0.045123\n",
      "Epoch: 4330 \tTraining Loss: 0.046193\n",
      "Epoch: 4331 \tTraining Loss: 0.045387\n",
      "Epoch: 4332 \tTraining Loss: 0.045302\n",
      "Epoch: 4333 \tTraining Loss: 0.045269\n",
      "Epoch: 4334 \tTraining Loss: 0.045348\n",
      "Epoch: 4335 \tTraining Loss: 0.045219\n",
      "Epoch: 4336 \tTraining Loss: 0.045299\n",
      "Epoch: 4337 \tTraining Loss: 0.045356\n",
      "Epoch: 4338 \tTraining Loss: 0.045379\n",
      "Epoch: 4339 \tTraining Loss: 0.045242\n",
      "Epoch: 4340 \tTraining Loss: 0.045279\n",
      "Epoch: 4341 \tTraining Loss: 0.045262\n",
      "Epoch: 4342 \tTraining Loss: 0.045250\n",
      "Epoch: 4343 \tTraining Loss: 0.045438\n",
      "Epoch: 4344 \tTraining Loss: 0.045262\n",
      "Epoch: 4345 \tTraining Loss: 0.045265\n",
      "Epoch: 4346 \tTraining Loss: 0.045212\n",
      "Epoch: 4347 \tTraining Loss: 0.045287\n",
      "Epoch: 4348 \tTraining Loss: 0.045222\n",
      "Epoch: 4349 \tTraining Loss: 0.045157\n",
      "Epoch: 4350 \tTraining Loss: 0.045178\n",
      "Epoch: 4351 \tTraining Loss: 0.045173\n",
      "Epoch: 4352 \tTraining Loss: 0.045172\n",
      "Epoch: 4353 \tTraining Loss: 0.045145\n",
      "Epoch: 4354 \tTraining Loss: 0.045216\n",
      "Epoch: 4355 \tTraining Loss: 0.045155\n",
      "Epoch: 4356 \tTraining Loss: 0.045131\n",
      "Epoch: 4357 \tTraining Loss: 0.045117\n",
      "Epoch: 4358 \tTraining Loss: 0.045132\n",
      "Epoch: 4359 \tTraining Loss: 0.045166\n",
      "Epoch: 4360 \tTraining Loss: 0.045066\n",
      "Epoch: 4361 \tTraining Loss: 0.045061\n",
      "Epoch: 4362 \tTraining Loss: 0.045075\n",
      "Epoch: 4363 \tTraining Loss: 0.045081\n",
      "Epoch: 4364 \tTraining Loss: 0.045068\n",
      "Epoch: 4365 \tTraining Loss: 0.045081\n",
      "Epoch: 4366 \tTraining Loss: 0.045119\n",
      "Epoch: 4367 \tTraining Loss: 0.045062\n",
      "Epoch: 4368 \tTraining Loss: 0.045203\n",
      "Epoch: 4369 \tTraining Loss: 0.045045\n",
      "Epoch: 4370 \tTraining Loss: 0.044977\n",
      "Epoch: 4371 \tTraining Loss: 0.045030\n",
      "Epoch: 4372 \tTraining Loss: 0.045053\n",
      "Epoch: 4373 \tTraining Loss: 0.044974\n",
      "Epoch: 4374 \tTraining Loss: 0.045013\n",
      "Epoch: 4375 \tTraining Loss: 0.044982\n",
      "Epoch: 4376 \tTraining Loss: 0.045006\n",
      "Epoch: 4377 \tTraining Loss: 0.044989\n",
      "Epoch: 4378 \tTraining Loss: 0.045006\n",
      "Epoch: 4379 \tTraining Loss: 0.044978\n",
      "Epoch: 4380 \tTraining Loss: 0.044942\n",
      "Epoch: 4381 \tTraining Loss: 0.044978\n",
      "Epoch: 4382 \tTraining Loss: 0.045054\n",
      "Epoch: 4383 \tTraining Loss: 0.045139\n",
      "Epoch: 4384 \tTraining Loss: 0.061409\n",
      "Epoch: 4385 \tTraining Loss: 0.113294\n",
      "Epoch: 4386 \tTraining Loss: 0.157282\n",
      "Epoch: 4387 \tTraining Loss: 0.136436\n",
      "Epoch: 4388 \tTraining Loss: 0.159789\n",
      "Epoch: 4389 \tTraining Loss: 0.063041\n",
      "Epoch: 4390 \tTraining Loss: 0.061859\n",
      "Epoch: 4391 \tTraining Loss: 0.048253\n",
      "Epoch: 4392 \tTraining Loss: 0.048596\n",
      "Epoch: 4393 \tTraining Loss: 0.049753\n",
      "Epoch: 4394 \tTraining Loss: 0.045802\n",
      "Epoch: 4395 \tTraining Loss: 0.045475\n",
      "Epoch: 4396 \tTraining Loss: 0.045434\n",
      "Epoch: 4397 \tTraining Loss: 0.045027\n",
      "Epoch: 4398 \tTraining Loss: 0.044319\n",
      "Epoch: 4399 \tTraining Loss: 0.053202\n",
      "Epoch: 4400 \tTraining Loss: 0.045461\n",
      "Epoch: 4401 \tTraining Loss: 0.046568\n",
      "Epoch: 4402 \tTraining Loss: 0.046527\n",
      "Epoch: 4403 \tTraining Loss: 0.046420\n",
      "Epoch: 4404 \tTraining Loss: 0.046394\n",
      "Epoch: 4405 \tTraining Loss: 0.046280\n",
      "Epoch: 4406 \tTraining Loss: 0.045253\n",
      "Epoch: 4407 \tTraining Loss: 0.045273\n",
      "Epoch: 4408 \tTraining Loss: 0.045210\n",
      "Epoch: 4409 \tTraining Loss: 0.044824\n",
      "Epoch: 4410 \tTraining Loss: 0.044496\n",
      "Epoch: 4411 \tTraining Loss: 0.043858\n",
      "Epoch: 4412 \tTraining Loss: 0.051361\n",
      "Epoch: 4413 \tTraining Loss: 0.047237\n",
      "Epoch: 4414 \tTraining Loss: 0.044177\n",
      "Epoch: 4415 \tTraining Loss: 0.044175\n",
      "Epoch: 4416 \tTraining Loss: 0.043859\n",
      "Epoch: 4417 \tTraining Loss: 0.043838\n",
      "Epoch: 4418 \tTraining Loss: 0.043835\n",
      "Epoch: 4419 \tTraining Loss: 0.043837\n",
      "Epoch: 4420 \tTraining Loss: 0.043849\n",
      "Epoch: 4421 \tTraining Loss: 0.043841\n",
      "Epoch: 4422 \tTraining Loss: 0.043820\n",
      "Epoch: 4423 \tTraining Loss: 0.043762\n",
      "Epoch: 4424 \tTraining Loss: 0.043781\n",
      "Epoch: 4425 \tTraining Loss: 0.043795\n",
      "Epoch: 4426 \tTraining Loss: 0.043829\n",
      "Epoch: 4427 \tTraining Loss: 0.043751\n",
      "Epoch: 4428 \tTraining Loss: 0.043735\n",
      "Epoch: 4429 \tTraining Loss: 0.043739\n",
      "Epoch: 4430 \tTraining Loss: 0.043753\n",
      "Epoch: 4431 \tTraining Loss: 0.043791\n",
      "Epoch: 4432 \tTraining Loss: 0.043752\n",
      "Epoch: 4433 \tTraining Loss: 0.043765\n",
      "Epoch: 4434 \tTraining Loss: 0.043742\n",
      "Epoch: 4435 \tTraining Loss: 0.043710\n",
      "Epoch: 4436 \tTraining Loss: 0.043716\n",
      "Epoch: 4437 \tTraining Loss: 0.043768\n",
      "Epoch: 4438 \tTraining Loss: 0.043796\n",
      "Epoch: 4439 \tTraining Loss: 0.043863\n",
      "Epoch: 4440 \tTraining Loss: 0.043889\n",
      "Epoch: 4441 \tTraining Loss: 0.043801\n",
      "Epoch: 4442 \tTraining Loss: 0.043832\n",
      "Epoch: 4443 \tTraining Loss: 0.043843\n",
      "Epoch: 4444 \tTraining Loss: 0.043812\n",
      "Epoch: 4445 \tTraining Loss: 0.043737\n",
      "Epoch: 4446 \tTraining Loss: 0.043736\n",
      "Epoch: 4447 \tTraining Loss: 0.043767\n",
      "Epoch: 4448 \tTraining Loss: 0.043722\n",
      "Epoch: 4449 \tTraining Loss: 0.043742\n",
      "Epoch: 4450 \tTraining Loss: 0.043839\n",
      "Epoch: 4451 \tTraining Loss: 0.043713\n",
      "Epoch: 4452 \tTraining Loss: 0.043730\n",
      "Epoch: 4453 \tTraining Loss: 0.043711\n",
      "Epoch: 4454 \tTraining Loss: 0.043764\n",
      "Epoch: 4455 \tTraining Loss: 0.043686\n",
      "Epoch: 4456 \tTraining Loss: 0.043736\n",
      "Epoch: 4457 \tTraining Loss: 0.043743\n",
      "Epoch: 4458 \tTraining Loss: 0.043701\n",
      "Epoch: 4459 \tTraining Loss: 0.043701\n",
      "Epoch: 4460 \tTraining Loss: 0.043696\n",
      "Epoch: 4461 \tTraining Loss: 0.043734\n",
      "Epoch: 4462 \tTraining Loss: 0.043699\n",
      "Epoch: 4463 \tTraining Loss: 0.043661\n",
      "Epoch: 4464 \tTraining Loss: 0.043810\n",
      "Epoch: 4465 \tTraining Loss: 0.043699\n",
      "Epoch: 4466 \tTraining Loss: 0.043724\n",
      "Epoch: 4467 \tTraining Loss: 0.043704\n",
      "Epoch: 4468 \tTraining Loss: 0.043661\n",
      "Epoch: 4469 \tTraining Loss: 0.043657\n",
      "Epoch: 4470 \tTraining Loss: 0.043693\n",
      "Epoch: 4471 \tTraining Loss: 0.043645\n",
      "Epoch: 4472 \tTraining Loss: 0.043705\n",
      "Epoch: 4473 \tTraining Loss: 0.043684\n",
      "Epoch: 4474 \tTraining Loss: 0.043648\n",
      "Epoch: 4475 \tTraining Loss: 0.043678\n",
      "Epoch: 4476 \tTraining Loss: 0.043663\n",
      "Epoch: 4477 \tTraining Loss: 0.043664\n",
      "Epoch: 4478 \tTraining Loss: 0.043793\n",
      "Epoch: 4479 \tTraining Loss: 0.043659\n",
      "Epoch: 4480 \tTraining Loss: 0.044494\n",
      "Epoch: 4481 \tTraining Loss: 0.049421\n",
      "Epoch: 4482 \tTraining Loss: 0.044685\n",
      "Epoch: 4483 \tTraining Loss: 0.044178\n",
      "Epoch: 4484 \tTraining Loss: 0.043296\n",
      "Epoch: 4485 \tTraining Loss: 0.042774\n",
      "Epoch: 4486 \tTraining Loss: 0.042783\n",
      "Epoch: 4487 \tTraining Loss: 0.042752\n",
      "Epoch: 4488 \tTraining Loss: 0.042766\n",
      "Epoch: 4489 \tTraining Loss: 0.042741\n",
      "Epoch: 4490 \tTraining Loss: 0.042865\n",
      "Epoch: 4491 \tTraining Loss: 0.042761\n",
      "Epoch: 4492 \tTraining Loss: 0.042955\n",
      "Epoch: 4493 \tTraining Loss: 0.042689\n",
      "Epoch: 4494 \tTraining Loss: 0.042662\n",
      "Epoch: 4495 \tTraining Loss: 0.042634\n",
      "Epoch: 4496 \tTraining Loss: 0.042649\n",
      "Epoch: 4497 \tTraining Loss: 0.042648\n",
      "Epoch: 4498 \tTraining Loss: 0.042658\n",
      "Epoch: 4499 \tTraining Loss: 0.042604\n",
      "Epoch: 4500 \tTraining Loss: 0.042653\n",
      "Epoch: 4501 \tTraining Loss: 0.042592\n",
      "Epoch: 4502 \tTraining Loss: 0.042597\n",
      "Epoch: 4503 \tTraining Loss: 0.042591\n",
      "Epoch: 4504 \tTraining Loss: 0.042731\n",
      "Epoch: 4505 \tTraining Loss: 0.042599\n",
      "Epoch: 4506 \tTraining Loss: 0.042593\n",
      "Epoch: 4507 \tTraining Loss: 0.042591\n",
      "Epoch: 4508 \tTraining Loss: 0.042682\n",
      "Epoch: 4509 \tTraining Loss: 0.042598\n",
      "Epoch: 4510 \tTraining Loss: 0.042658\n",
      "Epoch: 4511 \tTraining Loss: 0.042574\n",
      "Epoch: 4512 \tTraining Loss: 0.042576\n",
      "Epoch: 4513 \tTraining Loss: 0.042602\n",
      "Epoch: 4514 \tTraining Loss: 0.042564\n",
      "Epoch: 4515 \tTraining Loss: 0.042620\n",
      "Epoch: 4516 \tTraining Loss: 0.042558\n",
      "Epoch: 4517 \tTraining Loss: 0.042588\n",
      "Epoch: 4518 \tTraining Loss: 0.042587\n",
      "Epoch: 4519 \tTraining Loss: 0.042651\n",
      "Epoch: 4520 \tTraining Loss: 0.042554\n",
      "Epoch: 4521 \tTraining Loss: 0.042657\n",
      "Epoch: 4522 \tTraining Loss: 0.042505\n",
      "Epoch: 4523 \tTraining Loss: 0.042588\n",
      "Epoch: 4524 \tTraining Loss: 0.042609\n",
      "Epoch: 4525 \tTraining Loss: 0.042603\n",
      "Epoch: 4526 \tTraining Loss: 0.042556\n",
      "Epoch: 4527 \tTraining Loss: 0.042562\n",
      "Epoch: 4528 \tTraining Loss: 0.042567\n",
      "Epoch: 4529 \tTraining Loss: 0.042680\n",
      "Epoch: 4530 \tTraining Loss: 0.042564\n",
      "Epoch: 4531 \tTraining Loss: 0.042538\n",
      "Epoch: 4532 \tTraining Loss: 0.042539\n",
      "Epoch: 4533 \tTraining Loss: 0.042546\n",
      "Epoch: 4534 \tTraining Loss: 0.042569\n",
      "Epoch: 4535 \tTraining Loss: 0.042605\n",
      "Epoch: 4536 \tTraining Loss: 0.042554\n",
      "Epoch: 4537 \tTraining Loss: 0.042580\n",
      "Epoch: 4538 \tTraining Loss: 0.042555\n",
      "Epoch: 4539 \tTraining Loss: 0.042537\n",
      "Epoch: 4540 \tTraining Loss: 0.042539\n",
      "Epoch: 4541 \tTraining Loss: 0.042552\n",
      "Epoch: 4542 \tTraining Loss: 0.042539\n",
      "Epoch: 4543 \tTraining Loss: 0.042578\n",
      "Epoch: 4544 \tTraining Loss: 0.042508\n",
      "Epoch: 4545 \tTraining Loss: 0.042517\n",
      "Epoch: 4546 \tTraining Loss: 0.042530\n",
      "Epoch: 4547 \tTraining Loss: 0.042534\n",
      "Epoch: 4548 \tTraining Loss: 0.042525\n",
      "Epoch: 4549 \tTraining Loss: 0.042534\n",
      "Epoch: 4550 \tTraining Loss: 0.042606\n",
      "Epoch: 4551 \tTraining Loss: 0.042546\n",
      "Epoch: 4552 \tTraining Loss: 0.042494\n",
      "Epoch: 4553 \tTraining Loss: 0.042607\n",
      "Epoch: 4554 \tTraining Loss: 0.042525\n",
      "Epoch: 4555 \tTraining Loss: 0.042522\n",
      "Epoch: 4556 \tTraining Loss: 0.042488\n",
      "Epoch: 4557 \tTraining Loss: 0.042562\n",
      "Epoch: 4558 \tTraining Loss: 0.042519\n",
      "Epoch: 4559 \tTraining Loss: 0.042537\n",
      "Epoch: 4560 \tTraining Loss: 0.042510\n",
      "Epoch: 4561 \tTraining Loss: 0.042533\n",
      "Epoch: 4562 \tTraining Loss: 0.042524\n",
      "Epoch: 4563 \tTraining Loss: 0.042574\n",
      "Epoch: 4564 \tTraining Loss: 0.042546\n",
      "Epoch: 4565 \tTraining Loss: 0.042520\n",
      "Epoch: 4566 \tTraining Loss: 0.042523\n",
      "Epoch: 4567 \tTraining Loss: 0.042503\n",
      "Epoch: 4568 \tTraining Loss: 0.042531\n",
      "Epoch: 4569 \tTraining Loss: 0.042509\n",
      "Epoch: 4570 \tTraining Loss: 0.042509\n",
      "Epoch: 4571 \tTraining Loss: 0.042521\n",
      "Epoch: 4572 \tTraining Loss: 0.042529\n",
      "Epoch: 4573 \tTraining Loss: 0.042508\n",
      "Epoch: 4574 \tTraining Loss: 0.042527\n",
      "Epoch: 4575 \tTraining Loss: 0.042504\n",
      "Epoch: 4576 \tTraining Loss: 0.042517\n",
      "Epoch: 4577 \tTraining Loss: 0.042530\n",
      "Epoch: 4578 \tTraining Loss: 0.042541\n",
      "Epoch: 4579 \tTraining Loss: 0.042521\n",
      "Epoch: 4580 \tTraining Loss: 0.042587\n",
      "Epoch: 4581 \tTraining Loss: 0.042501\n",
      "Epoch: 4582 \tTraining Loss: 0.041499\n",
      "Epoch: 4583 \tTraining Loss: 0.197900\n",
      "Epoch: 4584 \tTraining Loss: 0.286030\n",
      "Epoch: 4585 \tTraining Loss: 0.201868\n",
      "Epoch: 4586 \tTraining Loss: 0.136263\n",
      "Epoch: 4587 \tTraining Loss: 0.098335\n",
      "Epoch: 4588 \tTraining Loss: 0.079308\n",
      "Epoch: 4589 \tTraining Loss: 0.044956\n",
      "Epoch: 4590 \tTraining Loss: 0.042289\n",
      "Epoch: 4591 \tTraining Loss: 0.041985\n",
      "Epoch: 4592 \tTraining Loss: 0.041867\n",
      "Epoch: 4593 \tTraining Loss: 0.041807\n",
      "Epoch: 4594 \tTraining Loss: 0.041774\n",
      "Epoch: 4595 \tTraining Loss: 0.041778\n",
      "Epoch: 4596 \tTraining Loss: 0.041728\n",
      "Epoch: 4597 \tTraining Loss: 0.041682\n",
      "Epoch: 4598 \tTraining Loss: 0.041682\n",
      "Epoch: 4599 \tTraining Loss: 0.041695\n",
      "Epoch: 4600 \tTraining Loss: 0.041669\n",
      "Epoch: 4601 \tTraining Loss: 0.041698\n",
      "Epoch: 4602 \tTraining Loss: 0.041610\n",
      "Epoch: 4603 \tTraining Loss: 0.041661\n",
      "Epoch: 4604 \tTraining Loss: 0.041609\n",
      "Epoch: 4605 \tTraining Loss: 0.041584\n",
      "Epoch: 4606 \tTraining Loss: 0.041585\n",
      "Epoch: 4607 \tTraining Loss: 0.041645\n",
      "Epoch: 4608 \tTraining Loss: 0.041712\n",
      "Epoch: 4609 \tTraining Loss: 0.041652\n",
      "Epoch: 4610 \tTraining Loss: 0.041639\n",
      "Epoch: 4611 \tTraining Loss: 0.041694\n",
      "Epoch: 4612 \tTraining Loss: 0.041572\n",
      "Epoch: 4613 \tTraining Loss: 0.041633\n",
      "Epoch: 4614 \tTraining Loss: 0.041555\n",
      "Epoch: 4615 \tTraining Loss: 0.041570\n",
      "Epoch: 4616 \tTraining Loss: 0.041570\n",
      "Epoch: 4617 \tTraining Loss: 0.041728\n",
      "Epoch: 4618 \tTraining Loss: 0.041542\n",
      "Epoch: 4619 \tTraining Loss: 0.041576\n",
      "Epoch: 4620 \tTraining Loss: 0.041552\n",
      "Epoch: 4621 \tTraining Loss: 0.041550\n",
      "Epoch: 4622 \tTraining Loss: 0.041545\n",
      "Epoch: 4623 \tTraining Loss: 0.041538\n",
      "Epoch: 4624 \tTraining Loss: 0.041663\n",
      "Epoch: 4625 \tTraining Loss: 0.041583\n",
      "Epoch: 4626 \tTraining Loss: 0.041577\n",
      "Epoch: 4627 \tTraining Loss: 0.041588\n",
      "Epoch: 4628 \tTraining Loss: 0.041618\n",
      "Epoch: 4629 \tTraining Loss: 0.041564\n",
      "Epoch: 4630 \tTraining Loss: 0.041534\n",
      "Epoch: 4631 \tTraining Loss: 0.041555\n",
      "Epoch: 4632 \tTraining Loss: 0.042176\n",
      "Epoch: 4633 \tTraining Loss: 0.041662\n",
      "Epoch: 4634 \tTraining Loss: 0.041778\n",
      "Epoch: 4635 \tTraining Loss: 0.038309\n",
      "Epoch: 4636 \tTraining Loss: 0.044423\n",
      "Epoch: 4637 \tTraining Loss: 0.040494\n",
      "Epoch: 4638 \tTraining Loss: 0.040523\n",
      "Epoch: 4639 \tTraining Loss: 0.039414\n",
      "Epoch: 4640 \tTraining Loss: 0.041407\n",
      "Epoch: 4641 \tTraining Loss: 0.039254\n",
      "Epoch: 4642 \tTraining Loss: 0.039273\n",
      "Epoch: 4643 \tTraining Loss: 0.039134\n",
      "Epoch: 4644 \tTraining Loss: 0.039107\n",
      "Epoch: 4645 \tTraining Loss: 0.039035\n",
      "Epoch: 4646 \tTraining Loss: 0.039096\n",
      "Epoch: 4647 \tTraining Loss: 0.039193\n",
      "Epoch: 4648 \tTraining Loss: 0.039016\n",
      "Epoch: 4649 \tTraining Loss: 0.039057\n",
      "Epoch: 4650 \tTraining Loss: 0.039185\n",
      "Epoch: 4651 \tTraining Loss: 0.046417\n",
      "Epoch: 4652 \tTraining Loss: 0.049252\n",
      "Epoch: 4653 \tTraining Loss: 0.045038\n",
      "Epoch: 4654 \tTraining Loss: 0.044536\n",
      "Epoch: 4655 \tTraining Loss: 0.043135\n",
      "Epoch: 4656 \tTraining Loss: 0.041078\n",
      "Epoch: 4657 \tTraining Loss: 0.040180\n",
      "Epoch: 4658 \tTraining Loss: 0.048099\n",
      "Epoch: 4659 \tTraining Loss: 0.044437\n",
      "Epoch: 4660 \tTraining Loss: 0.046021\n",
      "Epoch: 4661 \tTraining Loss: 0.045328\n",
      "Epoch: 4662 \tTraining Loss: 0.045279\n",
      "Epoch: 4663 \tTraining Loss: 0.045155\n",
      "Epoch: 4664 \tTraining Loss: 0.045130\n",
      "Epoch: 4665 \tTraining Loss: 0.040577\n",
      "Epoch: 4666 \tTraining Loss: 0.037289\n",
      "Epoch: 4667 \tTraining Loss: 0.036761\n",
      "Epoch: 4668 \tTraining Loss: 0.036629\n",
      "Epoch: 4669 \tTraining Loss: 0.036563\n",
      "Epoch: 4670 \tTraining Loss: 0.036507\n",
      "Epoch: 4671 \tTraining Loss: 0.036405\n",
      "Epoch: 4672 \tTraining Loss: 0.036335\n",
      "Epoch: 4673 \tTraining Loss: 0.036312\n",
      "Epoch: 4674 \tTraining Loss: 0.036274\n",
      "Epoch: 4675 \tTraining Loss: 0.036189\n",
      "Epoch: 4676 \tTraining Loss: 0.036161\n",
      "Epoch: 4677 \tTraining Loss: 0.036169\n",
      "Epoch: 4678 \tTraining Loss: 0.036112\n",
      "Epoch: 4679 \tTraining Loss: 0.036169\n",
      "Epoch: 4680 \tTraining Loss: 0.036100\n",
      "Epoch: 4681 \tTraining Loss: 0.036093\n",
      "Epoch: 4682 \tTraining Loss: 0.036093\n",
      "Epoch: 4683 \tTraining Loss: 0.036131\n",
      "Epoch: 4684 \tTraining Loss: 0.036078\n",
      "Epoch: 4685 \tTraining Loss: 0.036075\n",
      "Epoch: 4686 \tTraining Loss: 0.036114\n",
      "Epoch: 4687 \tTraining Loss: 0.036099\n",
      "Epoch: 4688 \tTraining Loss: 0.036084\n",
      "Epoch: 4689 \tTraining Loss: 0.036099\n",
      "Epoch: 4690 \tTraining Loss: 0.036152\n",
      "Epoch: 4691 \tTraining Loss: 0.036119\n",
      "Epoch: 4692 \tTraining Loss: 0.036088\n",
      "Epoch: 4693 \tTraining Loss: 0.036089\n",
      "Epoch: 4694 \tTraining Loss: 0.036075\n",
      "Epoch: 4695 \tTraining Loss: 0.036082\n",
      "Epoch: 4696 \tTraining Loss: 0.036080\n",
      "Epoch: 4697 \tTraining Loss: 0.036098\n",
      "Epoch: 4698 \tTraining Loss: 0.036085\n",
      "Epoch: 4699 \tTraining Loss: 0.036136\n",
      "Epoch: 4700 \tTraining Loss: 0.036158\n",
      "Epoch: 4701 \tTraining Loss: 0.037333\n",
      "Epoch: 4702 \tTraining Loss: 0.037382\n",
      "Epoch: 4703 \tTraining Loss: 0.036119\n",
      "Epoch: 4704 \tTraining Loss: 0.036112\n",
      "Epoch: 4705 \tTraining Loss: 0.036092\n",
      "Epoch: 4706 \tTraining Loss: 0.036084\n",
      "Epoch: 4707 \tTraining Loss: 0.036146\n",
      "Epoch: 4708 \tTraining Loss: 0.036077\n",
      "Epoch: 4709 \tTraining Loss: 0.036087\n",
      "Epoch: 4710 \tTraining Loss: 0.036135\n",
      "Epoch: 4711 \tTraining Loss: 0.036069\n",
      "Epoch: 4712 \tTraining Loss: 0.036095\n",
      "Epoch: 4713 \tTraining Loss: 0.036157\n",
      "Epoch: 4714 \tTraining Loss: 0.036143\n",
      "Epoch: 4715 \tTraining Loss: 0.036072\n",
      "Epoch: 4716 \tTraining Loss: 0.036107\n",
      "Epoch: 4717 \tTraining Loss: 0.036064\n",
      "Epoch: 4718 \tTraining Loss: 0.036086\n",
      "Epoch: 4719 \tTraining Loss: 0.036100\n",
      "Epoch: 4720 \tTraining Loss: 0.036060\n",
      "Epoch: 4721 \tTraining Loss: 0.036059\n",
      "Epoch: 4722 \tTraining Loss: 0.036051\n",
      "Epoch: 4723 \tTraining Loss: 0.036069\n",
      "Epoch: 4724 \tTraining Loss: 0.036126\n",
      "Epoch: 4725 \tTraining Loss: 0.036106\n",
      "Epoch: 4726 \tTraining Loss: 0.036089\n",
      "Epoch: 4727 \tTraining Loss: 0.036181\n",
      "Epoch: 4728 \tTraining Loss: 0.036406\n",
      "Epoch: 4729 \tTraining Loss: 0.036318\n",
      "Epoch: 4730 \tTraining Loss: 0.036033\n",
      "Epoch: 4731 \tTraining Loss: 0.036048\n",
      "Epoch: 4732 \tTraining Loss: 0.036131\n",
      "Epoch: 4733 \tTraining Loss: 0.036057\n",
      "Epoch: 4734 \tTraining Loss: 0.036044\n",
      "Epoch: 4735 \tTraining Loss: 0.036049\n",
      "Epoch: 4736 \tTraining Loss: 0.036168\n",
      "Epoch: 4737 \tTraining Loss: 0.036042\n",
      "Epoch: 4738 \tTraining Loss: 0.036005\n",
      "Epoch: 4739 \tTraining Loss: 0.035972\n",
      "Epoch: 4740 \tTraining Loss: 0.046422\n",
      "Epoch: 4741 \tTraining Loss: 0.083661\n",
      "Epoch: 4742 \tTraining Loss: 0.109260\n",
      "Epoch: 4743 \tTraining Loss: 0.148024\n",
      "Epoch: 4744 \tTraining Loss: 0.104658\n",
      "Epoch: 4745 \tTraining Loss: 0.126914\n",
      "Epoch: 4746 \tTraining Loss: 0.066074\n",
      "Epoch: 4747 \tTraining Loss: 0.053359\n",
      "Epoch: 4748 \tTraining Loss: 0.059313\n",
      "Epoch: 4749 \tTraining Loss: 0.046668\n",
      "Epoch: 4750 \tTraining Loss: 0.044451\n",
      "Epoch: 4751 \tTraining Loss: 0.043984\n",
      "Epoch: 4752 \tTraining Loss: 0.041360\n",
      "Epoch: 4753 \tTraining Loss: 0.041290\n",
      "Epoch: 4754 \tTraining Loss: 0.041235\n",
      "Epoch: 4755 \tTraining Loss: 0.041296\n",
      "Epoch: 4756 \tTraining Loss: 0.041182\n",
      "Epoch: 4757 \tTraining Loss: 0.041214\n",
      "Epoch: 4758 \tTraining Loss: 0.041166\n",
      "Epoch: 4759 \tTraining Loss: 0.041195\n",
      "Epoch: 4760 \tTraining Loss: 0.041092\n",
      "Epoch: 4761 \tTraining Loss: 0.041127\n",
      "Epoch: 4762 \tTraining Loss: 0.041636\n",
      "Epoch: 4763 \tTraining Loss: 0.041160\n",
      "Epoch: 4764 \tTraining Loss: 0.041065\n",
      "Epoch: 4765 \tTraining Loss: 0.041061\n",
      "Epoch: 4766 \tTraining Loss: 0.041095\n",
      "Epoch: 4767 \tTraining Loss: 0.041029\n",
      "Epoch: 4768 \tTraining Loss: 0.041024\n",
      "Epoch: 4769 \tTraining Loss: 0.041051\n",
      "Epoch: 4770 \tTraining Loss: 0.041019\n",
      "Epoch: 4771 \tTraining Loss: 0.040981\n",
      "Epoch: 4772 \tTraining Loss: 0.041044\n",
      "Epoch: 4773 \tTraining Loss: 0.041014\n",
      "Epoch: 4774 \tTraining Loss: 0.041006\n",
      "Epoch: 4775 \tTraining Loss: 0.041032\n",
      "Epoch: 4776 \tTraining Loss: 0.041952\n",
      "Epoch: 4777 \tTraining Loss: 0.040114\n",
      "Epoch: 4778 \tTraining Loss: 0.040148\n",
      "Epoch: 4779 \tTraining Loss: 0.040062\n",
      "Epoch: 4780 \tTraining Loss: 0.040060\n",
      "Epoch: 4781 \tTraining Loss: 0.040054\n",
      "Epoch: 4782 \tTraining Loss: 0.039988\n",
      "Epoch: 4783 \tTraining Loss: 0.040019\n",
      "Epoch: 4784 \tTraining Loss: 0.039957\n",
      "Epoch: 4785 \tTraining Loss: 0.039979\n",
      "Epoch: 4786 \tTraining Loss: 0.039960\n",
      "Epoch: 4787 \tTraining Loss: 0.040088\n",
      "Epoch: 4788 \tTraining Loss: 0.039979\n",
      "Epoch: 4789 \tTraining Loss: 0.039926\n",
      "Epoch: 4790 \tTraining Loss: 0.039993\n",
      "Epoch: 4791 \tTraining Loss: 0.039988\n",
      "Epoch: 4792 \tTraining Loss: 0.039947\n",
      "Epoch: 4793 \tTraining Loss: 0.039931\n",
      "Epoch: 4794 \tTraining Loss: 0.039917\n",
      "Epoch: 4795 \tTraining Loss: 0.040004\n",
      "Epoch: 4796 \tTraining Loss: 0.039936\n",
      "Epoch: 4797 \tTraining Loss: 0.039918\n",
      "Epoch: 4798 \tTraining Loss: 0.039928\n",
      "Epoch: 4799 \tTraining Loss: 0.039917\n",
      "Epoch: 4800 \tTraining Loss: 0.039904\n",
      "Epoch: 4801 \tTraining Loss: 0.039933\n",
      "Epoch: 4802 \tTraining Loss: 0.040003\n",
      "Epoch: 4803 \tTraining Loss: 0.039888\n",
      "Epoch: 4804 \tTraining Loss: 0.039910\n",
      "Epoch: 4805 \tTraining Loss: 0.039894\n",
      "Epoch: 4806 \tTraining Loss: 0.039899\n",
      "Epoch: 4807 \tTraining Loss: 0.039916\n",
      "Epoch: 4808 \tTraining Loss: 0.039905\n",
      "Epoch: 4809 \tTraining Loss: 0.039978\n",
      "Epoch: 4810 \tTraining Loss: 0.039880\n",
      "Epoch: 4811 \tTraining Loss: 0.039912\n",
      "Epoch: 4812 \tTraining Loss: 0.039884\n",
      "Epoch: 4813 \tTraining Loss: 0.039898\n",
      "Epoch: 4814 \tTraining Loss: 0.039949\n",
      "Epoch: 4815 \tTraining Loss: 0.054583\n",
      "Epoch: 4816 \tTraining Loss: 0.042224\n",
      "Epoch: 4817 \tTraining Loss: 0.042602\n",
      "Epoch: 4818 \tTraining Loss: 0.042148\n",
      "Epoch: 4819 \tTraining Loss: 0.042162\n",
      "Epoch: 4820 \tTraining Loss: 0.042955\n",
      "Epoch: 4821 \tTraining Loss: 0.041805\n",
      "Epoch: 4822 \tTraining Loss: 0.041658\n",
      "Epoch: 4823 \tTraining Loss: 0.041458\n",
      "Epoch: 4824 \tTraining Loss: 0.041397\n",
      "Epoch: 4825 \tTraining Loss: 0.041379\n",
      "Epoch: 4826 \tTraining Loss: 0.041389\n",
      "Epoch: 4827 \tTraining Loss: 0.041414\n",
      "Epoch: 4828 \tTraining Loss: 0.041356\n",
      "Epoch: 4829 \tTraining Loss: 0.041316\n",
      "Epoch: 4830 \tTraining Loss: 0.041338\n",
      "Epoch: 4831 \tTraining Loss: 0.041375\n",
      "Epoch: 4832 \tTraining Loss: 0.041318\n",
      "Epoch: 4833 \tTraining Loss: 0.041311\n",
      "Epoch: 4834 \tTraining Loss: 0.041384\n",
      "Epoch: 4835 \tTraining Loss: 0.041340\n",
      "Epoch: 4836 \tTraining Loss: 0.041279\n",
      "Epoch: 4837 \tTraining Loss: 0.041280\n",
      "Epoch: 4838 \tTraining Loss: 0.041291\n",
      "Epoch: 4839 \tTraining Loss: 0.041300\n",
      "Epoch: 4840 \tTraining Loss: 0.041298\n",
      "Epoch: 4841 \tTraining Loss: 0.041271\n",
      "Epoch: 4842 \tTraining Loss: 0.041317\n",
      "Epoch: 4843 \tTraining Loss: 0.041338\n",
      "Epoch: 4844 \tTraining Loss: 0.041297\n",
      "Epoch: 4845 \tTraining Loss: 0.041246\n",
      "Epoch: 4846 \tTraining Loss: 0.041283\n",
      "Epoch: 4847 \tTraining Loss: 0.041283\n",
      "Epoch: 4848 \tTraining Loss: 0.041255\n",
      "Epoch: 4849 \tTraining Loss: 0.041241\n",
      "Epoch: 4850 \tTraining Loss: 0.041308\n",
      "Epoch: 4851 \tTraining Loss: 0.041274\n",
      "Epoch: 4852 \tTraining Loss: 0.041279\n",
      "Epoch: 4853 \tTraining Loss: 0.041291\n",
      "Epoch: 4854 \tTraining Loss: 0.041244\n",
      "Epoch: 4855 \tTraining Loss: 0.041265\n",
      "Epoch: 4856 \tTraining Loss: 0.041382\n",
      "Epoch: 4857 \tTraining Loss: 0.041236\n",
      "Epoch: 4858 \tTraining Loss: 0.041244\n",
      "Epoch: 4859 \tTraining Loss: 0.041233\n",
      "Epoch: 4860 \tTraining Loss: 0.041223\n",
      "Epoch: 4861 \tTraining Loss: 0.041258\n",
      "Epoch: 4862 \tTraining Loss: 0.041261\n",
      "Epoch: 4863 \tTraining Loss: 0.041232\n",
      "Epoch: 4864 \tTraining Loss: 0.041265\n",
      "Epoch: 4865 \tTraining Loss: 0.041225\n",
      "Epoch: 4866 \tTraining Loss: 0.041267\n",
      "Epoch: 4867 \tTraining Loss: 0.041269\n",
      "Epoch: 4868 \tTraining Loss: 0.041245\n",
      "Epoch: 4869 \tTraining Loss: 0.041388\n",
      "Epoch: 4870 \tTraining Loss: 0.041266\n",
      "Epoch: 4871 \tTraining Loss: 0.041219\n",
      "Epoch: 4872 \tTraining Loss: 0.041228\n",
      "Epoch: 4873 \tTraining Loss: 0.041249\n",
      "Epoch: 4874 \tTraining Loss: 0.041243\n",
      "Epoch: 4875 \tTraining Loss: 0.041237\n",
      "Epoch: 4876 \tTraining Loss: 0.041213\n",
      "Epoch: 4877 \tTraining Loss: 0.041228\n",
      "Epoch: 4878 \tTraining Loss: 0.041251\n",
      "Epoch: 4879 \tTraining Loss: 0.041230\n",
      "Epoch: 4880 \tTraining Loss: 0.041231\n",
      "Epoch: 4881 \tTraining Loss: 0.041231\n",
      "Epoch: 4882 \tTraining Loss: 0.041230\n",
      "Epoch: 4883 \tTraining Loss: 0.041315\n",
      "Epoch: 4884 \tTraining Loss: 0.041196\n",
      "Epoch: 4885 \tTraining Loss: 0.041223\n",
      "Epoch: 4886 \tTraining Loss: 0.041265\n",
      "Epoch: 4887 \tTraining Loss: 0.041265\n",
      "Epoch: 4888 \tTraining Loss: 0.041261\n",
      "Epoch: 4889 \tTraining Loss: 0.041233\n",
      "Epoch: 4890 \tTraining Loss: 0.041257\n",
      "Epoch: 4891 \tTraining Loss: 0.041230\n",
      "Epoch: 4892 \tTraining Loss: 0.041286\n",
      "Epoch: 4893 \tTraining Loss: 0.041214\n",
      "Epoch: 4894 \tTraining Loss: 0.041231\n",
      "Epoch: 4895 \tTraining Loss: 0.041205\n",
      "Epoch: 4896 \tTraining Loss: 0.041338\n",
      "Epoch: 4897 \tTraining Loss: 0.041219\n",
      "Epoch: 4898 \tTraining Loss: 0.041220\n",
      "Epoch: 4899 \tTraining Loss: 0.064229\n",
      "Epoch: 4900 \tTraining Loss: 0.066607\n",
      "Epoch: 4901 \tTraining Loss: 0.169515\n",
      "Epoch: 4902 \tTraining Loss: 0.332853\n",
      "Epoch: 4903 \tTraining Loss: 0.135441\n",
      "Epoch: 4904 \tTraining Loss: 0.136874\n",
      "Epoch: 4905 \tTraining Loss: 0.092207\n",
      "Epoch: 4906 \tTraining Loss: 0.088029\n",
      "Epoch: 4907 \tTraining Loss: 0.054665\n",
      "Epoch: 4908 \tTraining Loss: 0.054059\n",
      "Epoch: 4909 \tTraining Loss: 0.050924\n",
      "Epoch: 4910 \tTraining Loss: 0.050368\n",
      "Epoch: 4911 \tTraining Loss: 0.049997\n",
      "Epoch: 4912 \tTraining Loss: 0.049806\n",
      "Epoch: 4913 \tTraining Loss: 0.049059\n",
      "Epoch: 4914 \tTraining Loss: 0.048133\n",
      "Epoch: 4915 \tTraining Loss: 0.048026\n",
      "Epoch: 4916 \tTraining Loss: 0.048022\n",
      "Epoch: 4917 \tTraining Loss: 0.048055\n",
      "Epoch: 4918 \tTraining Loss: 0.048050\n",
      "Epoch: 4919 \tTraining Loss: 0.048088\n",
      "Epoch: 4920 \tTraining Loss: 0.048090\n",
      "Epoch: 4921 \tTraining Loss: 0.048017\n",
      "Epoch: 4922 \tTraining Loss: 0.062512\n",
      "Epoch: 4923 \tTraining Loss: 0.046031\n",
      "Epoch: 4924 \tTraining Loss: 0.047890\n",
      "Epoch: 4925 \tTraining Loss: 0.047959\n",
      "Epoch: 4926 \tTraining Loss: 0.047905\n",
      "Epoch: 4927 \tTraining Loss: 0.048003\n",
      "Epoch: 4928 \tTraining Loss: 0.047873\n",
      "Epoch: 4929 \tTraining Loss: 0.047880\n",
      "Epoch: 4930 \tTraining Loss: 0.047940\n",
      "Epoch: 4931 \tTraining Loss: 0.047858\n",
      "Epoch: 4932 \tTraining Loss: 0.047845\n",
      "Epoch: 4933 \tTraining Loss: 0.047886\n",
      "Epoch: 4934 \tTraining Loss: 0.047992\n",
      "Epoch: 4935 \tTraining Loss: 0.047865\n",
      "Epoch: 4936 \tTraining Loss: 0.047847\n",
      "Epoch: 4937 \tTraining Loss: 0.047872\n",
      "Epoch: 4938 \tTraining Loss: 0.047903\n",
      "Epoch: 4939 \tTraining Loss: 0.047926\n",
      "Epoch: 4940 \tTraining Loss: 0.047835\n",
      "Epoch: 4941 \tTraining Loss: 0.047878\n",
      "Epoch: 4942 \tTraining Loss: 0.047887\n",
      "Epoch: 4943 \tTraining Loss: 0.047816\n",
      "Epoch: 4944 \tTraining Loss: 0.047851\n",
      "Epoch: 4945 \tTraining Loss: 0.047820\n",
      "Epoch: 4946 \tTraining Loss: 0.047903\n",
      "Epoch: 4947 \tTraining Loss: 0.047842\n",
      "Epoch: 4948 \tTraining Loss: 0.047908\n",
      "Epoch: 4949 \tTraining Loss: 0.047921\n",
      "Epoch: 4950 \tTraining Loss: 0.047806\n",
      "Epoch: 4951 \tTraining Loss: 0.047817\n",
      "Epoch: 4952 \tTraining Loss: 0.055166\n",
      "Epoch: 4953 \tTraining Loss: 0.047842\n",
      "Epoch: 4954 \tTraining Loss: 0.047930\n",
      "Epoch: 4955 \tTraining Loss: 0.047807\n",
      "Epoch: 4956 \tTraining Loss: 0.047814\n",
      "Epoch: 4957 \tTraining Loss: 0.047794\n",
      "Epoch: 4958 \tTraining Loss: 0.047826\n",
      "Epoch: 4959 \tTraining Loss: 0.047847\n",
      "Epoch: 4960 \tTraining Loss: 0.047809\n",
      "Epoch: 4961 \tTraining Loss: 0.046736\n",
      "Epoch: 4962 \tTraining Loss: 0.046470\n",
      "Epoch: 4963 \tTraining Loss: 0.045922\n",
      "Epoch: 4964 \tTraining Loss: 0.045998\n",
      "Epoch: 4965 \tTraining Loss: 0.045897\n",
      "Epoch: 4966 \tTraining Loss: 0.045929\n",
      "Epoch: 4967 \tTraining Loss: 0.045899\n",
      "Epoch: 4968 \tTraining Loss: 0.045880\n",
      "Epoch: 4969 \tTraining Loss: 0.045897\n",
      "Epoch: 4970 \tTraining Loss: 0.045843\n",
      "Epoch: 4971 \tTraining Loss: 0.045879\n",
      "Epoch: 4972 \tTraining Loss: 0.045846\n",
      "Epoch: 4973 \tTraining Loss: 0.045813\n",
      "Epoch: 4974 \tTraining Loss: 0.045821\n",
      "Epoch: 4975 \tTraining Loss: 0.045763\n",
      "Epoch: 4976 \tTraining Loss: 0.043228\n",
      "Epoch: 4977 \tTraining Loss: 0.043043\n",
      "Epoch: 4978 \tTraining Loss: 0.042997\n",
      "Epoch: 4979 \tTraining Loss: 0.042962\n",
      "Epoch: 4980 \tTraining Loss: 0.042917\n",
      "Epoch: 4981 \tTraining Loss: 0.042909\n",
      "Epoch: 4982 \tTraining Loss: 0.042850\n",
      "Epoch: 4983 \tTraining Loss: 0.042867\n",
      "Epoch: 4984 \tTraining Loss: 0.042837\n",
      "Epoch: 4985 \tTraining Loss: 0.042831\n",
      "Epoch: 4986 \tTraining Loss: 0.042839\n",
      "Epoch: 4987 \tTraining Loss: 0.042846\n",
      "Epoch: 4988 \tTraining Loss: 0.042824\n",
      "Epoch: 4989 \tTraining Loss: 0.042847\n",
      "Epoch: 4990 \tTraining Loss: 0.042806\n",
      "Epoch: 4991 \tTraining Loss: 0.042840\n",
      "Epoch: 4992 \tTraining Loss: 0.042800\n",
      "Epoch: 4993 \tTraining Loss: 0.042889\n",
      "Epoch: 4994 \tTraining Loss: 0.042842\n",
      "Epoch: 4995 \tTraining Loss: 0.042904\n",
      "Epoch: 4996 \tTraining Loss: 0.042802\n",
      "Epoch: 4997 \tTraining Loss: 0.042855\n",
      "Epoch: 4998 \tTraining Loss: 0.042794\n",
      "Epoch: 4999 \tTraining Loss: 0.042829\n",
      "Epoch: 5000 \tTraining Loss: 0.042828\n",
      "Epoch: 5001 \tTraining Loss: 0.042776\n",
      "Epoch: 5002 \tTraining Loss: 0.042777\n",
      "Epoch: 5003 \tTraining Loss: 0.042775\n",
      "Epoch: 5004 \tTraining Loss: 0.042782\n",
      "Epoch: 5005 \tTraining Loss: 0.042828\n",
      "Epoch: 5006 \tTraining Loss: 0.042786\n",
      "Epoch: 5007 \tTraining Loss: 0.042833\n",
      "Epoch: 5008 \tTraining Loss: 0.042789\n",
      "Epoch: 5009 \tTraining Loss: 0.042786\n",
      "Epoch: 5010 \tTraining Loss: 0.042789\n",
      "Epoch: 5011 \tTraining Loss: 0.042838\n",
      "Epoch: 5012 \tTraining Loss: 0.042758\n",
      "Epoch: 5013 \tTraining Loss: 0.041602\n",
      "Epoch: 5014 \tTraining Loss: 0.045217\n",
      "Epoch: 5015 \tTraining Loss: 0.052961\n",
      "Epoch: 5016 \tTraining Loss: 0.048835\n",
      "Epoch: 5017 \tTraining Loss: 0.083501\n",
      "Epoch: 5018 \tTraining Loss: 0.077947\n",
      "Epoch: 5019 \tTraining Loss: 0.072723\n",
      "Epoch: 5020 \tTraining Loss: 0.113321\n",
      "Epoch: 5021 \tTraining Loss: 0.064521\n",
      "Epoch: 5022 \tTraining Loss: 0.058554\n",
      "Epoch: 5023 \tTraining Loss: 0.079109\n",
      "Epoch: 5024 \tTraining Loss: 0.050372\n",
      "Epoch: 5025 \tTraining Loss: 0.042749\n",
      "Epoch: 5026 \tTraining Loss: 0.041588\n",
      "Epoch: 5027 \tTraining Loss: 0.041701\n",
      "Epoch: 5028 \tTraining Loss: 0.041395\n",
      "Epoch: 5029 \tTraining Loss: 0.041275\n",
      "Epoch: 5030 \tTraining Loss: 0.041186\n",
      "Epoch: 5031 \tTraining Loss: 0.041190\n",
      "Epoch: 5032 \tTraining Loss: 0.041208\n",
      "Epoch: 5033 \tTraining Loss: 0.041282\n",
      "Epoch: 5034 \tTraining Loss: 0.041158\n",
      "Epoch: 5035 \tTraining Loss: 0.041091\n",
      "Epoch: 5036 \tTraining Loss: 0.041007\n",
      "Epoch: 5037 \tTraining Loss: 0.040169\n",
      "Epoch: 5038 \tTraining Loss: 0.040142\n",
      "Epoch: 5039 \tTraining Loss: 0.040115\n",
      "Epoch: 5040 \tTraining Loss: 0.040081\n",
      "Epoch: 5041 \tTraining Loss: 0.040117\n",
      "Epoch: 5042 \tTraining Loss: 0.040041\n",
      "Epoch: 5043 \tTraining Loss: 0.040024\n",
      "Epoch: 5044 \tTraining Loss: 0.040048\n",
      "Epoch: 5045 \tTraining Loss: 0.040094\n",
      "Epoch: 5046 \tTraining Loss: 0.040049\n",
      "Epoch: 5047 \tTraining Loss: 0.040027\n",
      "Epoch: 5048 \tTraining Loss: 0.040020\n",
      "Epoch: 5049 \tTraining Loss: 0.039979\n",
      "Epoch: 5050 \tTraining Loss: 0.039988\n",
      "Epoch: 5051 \tTraining Loss: 0.039967\n",
      "Epoch: 5052 \tTraining Loss: 0.039973\n",
      "Epoch: 5053 \tTraining Loss: 0.039979\n",
      "Epoch: 5054 \tTraining Loss: 0.039967\n",
      "Epoch: 5055 \tTraining Loss: 0.055184\n",
      "Epoch: 5056 \tTraining Loss: 0.038299\n",
      "Epoch: 5057 \tTraining Loss: 0.040805\n",
      "Epoch: 5058 \tTraining Loss: 0.038320\n",
      "Epoch: 5059 \tTraining Loss: 0.038390\n",
      "Epoch: 5060 \tTraining Loss: 0.037978\n",
      "Epoch: 5061 \tTraining Loss: 0.037981\n",
      "Epoch: 5062 \tTraining Loss: 0.037954\n",
      "Epoch: 5063 \tTraining Loss: 0.037948\n",
      "Epoch: 5064 \tTraining Loss: 0.037953\n",
      "Epoch: 5065 \tTraining Loss: 0.037953\n",
      "Epoch: 5066 \tTraining Loss: 0.037934\n",
      "Epoch: 5067 \tTraining Loss: 0.037922\n",
      "Epoch: 5068 \tTraining Loss: 0.037928\n",
      "Epoch: 5069 \tTraining Loss: 0.037967\n",
      "Epoch: 5070 \tTraining Loss: 0.037912\n",
      "Epoch: 5071 \tTraining Loss: 0.037914\n",
      "Epoch: 5072 \tTraining Loss: 0.037882\n",
      "Epoch: 5073 \tTraining Loss: 0.037886\n",
      "Epoch: 5074 \tTraining Loss: 0.037943\n",
      "Epoch: 5075 \tTraining Loss: 0.037890\n",
      "Epoch: 5076 \tTraining Loss: 0.037891\n",
      "Epoch: 5077 \tTraining Loss: 0.037870\n",
      "Epoch: 5078 \tTraining Loss: 0.037866\n",
      "Epoch: 5079 \tTraining Loss: 0.037877\n",
      "Epoch: 5080 \tTraining Loss: 0.037875\n",
      "Epoch: 5081 \tTraining Loss: 0.037867\n",
      "Epoch: 5082 \tTraining Loss: 0.037899\n",
      "Epoch: 5083 \tTraining Loss: 0.037865\n",
      "Epoch: 5084 \tTraining Loss: 0.037882\n",
      "Epoch: 5085 \tTraining Loss: 0.037861\n",
      "Epoch: 5086 \tTraining Loss: 0.037869\n",
      "Epoch: 5087 \tTraining Loss: 0.037851\n",
      "Epoch: 5088 \tTraining Loss: 0.037871\n",
      "Epoch: 5089 \tTraining Loss: 0.037849\n",
      "Epoch: 5090 \tTraining Loss: 0.037843\n",
      "Epoch: 5091 \tTraining Loss: 0.037897\n",
      "Epoch: 5092 \tTraining Loss: 0.037859\n",
      "Epoch: 5093 \tTraining Loss: 0.037842\n",
      "Epoch: 5094 \tTraining Loss: 0.037845\n",
      "Epoch: 5095 \tTraining Loss: 0.037834\n",
      "Epoch: 5096 \tTraining Loss: 0.037876\n",
      "Epoch: 5097 \tTraining Loss: 0.037854\n",
      "Epoch: 5098 \tTraining Loss: 0.037821\n",
      "Epoch: 5099 \tTraining Loss: 0.037855\n",
      "Epoch: 5100 \tTraining Loss: 0.037862\n",
      "Epoch: 5101 \tTraining Loss: 0.037807\n",
      "Epoch: 5102 \tTraining Loss: 0.037873\n",
      "Epoch: 5103 \tTraining Loss: 0.037811\n",
      "Epoch: 5104 \tTraining Loss: 0.037863\n",
      "Epoch: 5105 \tTraining Loss: 0.037868\n",
      "Epoch: 5106 \tTraining Loss: 0.037875\n",
      "Epoch: 5107 \tTraining Loss: 0.037872\n",
      "Epoch: 5108 \tTraining Loss: 0.037823\n",
      "Epoch: 5109 \tTraining Loss: 0.037851\n",
      "Epoch: 5110 \tTraining Loss: 0.037855\n",
      "Epoch: 5111 \tTraining Loss: 0.037830\n",
      "Epoch: 5112 \tTraining Loss: 0.037885\n",
      "Epoch: 5113 \tTraining Loss: 0.037858\n",
      "Epoch: 5114 \tTraining Loss: 0.037817\n",
      "Epoch: 5115 \tTraining Loss: 0.037789\n",
      "Epoch: 5116 \tTraining Loss: 0.037818\n",
      "Epoch: 5117 \tTraining Loss: 0.037838\n",
      "Epoch: 5118 \tTraining Loss: 0.037809\n",
      "Epoch: 5119 \tTraining Loss: 0.037832\n",
      "Epoch: 5120 \tTraining Loss: 0.037813\n",
      "Epoch: 5121 \tTraining Loss: 0.037815\n",
      "Epoch: 5122 \tTraining Loss: 0.037902\n",
      "Epoch: 5123 \tTraining Loss: 0.037787\n",
      "Epoch: 5124 \tTraining Loss: 0.037873\n",
      "Epoch: 5125 \tTraining Loss: 0.037858\n",
      "Epoch: 5126 \tTraining Loss: 0.037787\n",
      "Epoch: 5127 \tTraining Loss: 0.037795\n",
      "Epoch: 5128 \tTraining Loss: 0.037804\n",
      "Epoch: 5129 \tTraining Loss: 0.037805\n",
      "Epoch: 5130 \tTraining Loss: 0.037806\n",
      "Epoch: 5131 \tTraining Loss: 0.037860\n",
      "Epoch: 5132 \tTraining Loss: 0.037761\n",
      "Epoch: 5133 \tTraining Loss: 0.038735\n",
      "Epoch: 5134 \tTraining Loss: 0.037869\n",
      "Epoch: 5135 \tTraining Loss: 0.037934\n",
      "Epoch: 5136 \tTraining Loss: 0.037808\n",
      "Epoch: 5137 \tTraining Loss: 0.037784\n",
      "Epoch: 5138 \tTraining Loss: 0.037832\n",
      "Epoch: 5139 \tTraining Loss: 0.037837\n",
      "Epoch: 5140 \tTraining Loss: 0.037807\n",
      "Epoch: 5141 \tTraining Loss: 0.037805\n",
      "Epoch: 5142 \tTraining Loss: 0.037815\n",
      "Epoch: 5143 \tTraining Loss: 0.037781\n",
      "Epoch: 5144 \tTraining Loss: 0.037798\n",
      "Epoch: 5145 \tTraining Loss: 0.037797\n",
      "Epoch: 5146 \tTraining Loss: 0.037805\n",
      "Epoch: 5147 \tTraining Loss: 0.037830\n",
      "Epoch: 5148 \tTraining Loss: 0.037805\n",
      "Epoch: 5149 \tTraining Loss: 0.037815\n",
      "Epoch: 5150 \tTraining Loss: 0.037840\n",
      "Epoch: 5151 \tTraining Loss: 0.037775\n",
      "Epoch: 5152 \tTraining Loss: 0.037793\n",
      "Epoch: 5153 \tTraining Loss: 0.037797\n",
      "Epoch: 5154 \tTraining Loss: 0.037818\n",
      "Epoch: 5155 \tTraining Loss: 0.037782\n",
      "Epoch: 5156 \tTraining Loss: 0.037805\n",
      "Epoch: 5157 \tTraining Loss: 0.037919\n",
      "Epoch: 5158 \tTraining Loss: 0.037768\n",
      "Epoch: 5159 \tTraining Loss: 0.037802\n",
      "Epoch: 5160 \tTraining Loss: 0.037767\n",
      "Epoch: 5161 \tTraining Loss: 0.037844\n",
      "Epoch: 5162 \tTraining Loss: 0.037774\n",
      "Epoch: 5163 \tTraining Loss: 0.037763\n",
      "Epoch: 5164 \tTraining Loss: 0.037785\n",
      "Epoch: 5165 \tTraining Loss: 0.037767\n",
      "Epoch: 5166 \tTraining Loss: 0.037764\n",
      "Epoch: 5167 \tTraining Loss: 0.037799\n",
      "Epoch: 5168 \tTraining Loss: 0.037798\n",
      "Epoch: 5169 \tTraining Loss: 0.037765\n",
      "Epoch: 5170 \tTraining Loss: 0.037777\n",
      "Epoch: 5171 \tTraining Loss: 0.037768\n",
      "Epoch: 5172 \tTraining Loss: 0.037774\n",
      "Epoch: 5173 \tTraining Loss: 0.037762\n",
      "Epoch: 5174 \tTraining Loss: 0.037765\n",
      "Epoch: 5175 \tTraining Loss: 0.037770\n",
      "Epoch: 5176 \tTraining Loss: 0.037761\n",
      "Epoch: 5177 \tTraining Loss: 0.037808\n",
      "Epoch: 5178 \tTraining Loss: 0.039267\n",
      "Epoch: 5179 \tTraining Loss: 0.039114\n",
      "Epoch: 5180 \tTraining Loss: 0.038730\n",
      "Epoch: 5181 \tTraining Loss: 0.058429\n",
      "Epoch: 5182 \tTraining Loss: 0.180085\n",
      "Epoch: 5183 \tTraining Loss: 0.147091\n",
      "Epoch: 5184 \tTraining Loss: 0.143037\n",
      "Epoch: 5185 \tTraining Loss: 0.184213\n",
      "Epoch: 5186 \tTraining Loss: 0.118576\n",
      "Epoch: 5187 \tTraining Loss: 0.100635\n",
      "Epoch: 5188 \tTraining Loss: 0.075084\n",
      "Epoch: 5189 \tTraining Loss: 0.077168\n",
      "Epoch: 5190 \tTraining Loss: 0.110489\n",
      "Epoch: 5191 \tTraining Loss: 0.063338\n",
      "Epoch: 5192 \tTraining Loss: 0.056931\n",
      "Epoch: 5193 \tTraining Loss: 0.054760\n",
      "Epoch: 5194 \tTraining Loss: 0.052500\n",
      "Epoch: 5195 \tTraining Loss: 0.052803\n",
      "Epoch: 5196 \tTraining Loss: 0.052137\n",
      "Epoch: 5197 \tTraining Loss: 0.052131\n",
      "Epoch: 5198 \tTraining Loss: 0.051903\n",
      "Epoch: 5199 \tTraining Loss: 0.051769\n",
      "Epoch: 5200 \tTraining Loss: 0.051774\n",
      "Epoch: 5201 \tTraining Loss: 0.051734\n",
      "Epoch: 5202 \tTraining Loss: 0.051733\n",
      "Epoch: 5203 \tTraining Loss: 0.051712\n",
      "Epoch: 5204 \tTraining Loss: 0.051674\n",
      "Epoch: 5205 \tTraining Loss: 0.051617\n",
      "Epoch: 5206 \tTraining Loss: 0.050702\n",
      "Epoch: 5207 \tTraining Loss: 0.050628\n",
      "Epoch: 5208 \tTraining Loss: 0.050681\n",
      "Epoch: 5209 \tTraining Loss: 0.050592\n",
      "Epoch: 5210 \tTraining Loss: 0.050591\n",
      "Epoch: 5211 \tTraining Loss: 0.050559\n",
      "Epoch: 5212 \tTraining Loss: 0.050610\n",
      "Epoch: 5213 \tTraining Loss: 0.050603\n",
      "Epoch: 5214 \tTraining Loss: 0.050569\n",
      "Epoch: 5215 \tTraining Loss: 0.050631\n",
      "Epoch: 5216 \tTraining Loss: 0.050542\n",
      "Epoch: 5217 \tTraining Loss: 0.050538\n",
      "Epoch: 5218 \tTraining Loss: 0.050550\n",
      "Epoch: 5219 \tTraining Loss: 0.050594\n",
      "Epoch: 5220 \tTraining Loss: 0.050505\n",
      "Epoch: 5221 \tTraining Loss: 0.050652\n",
      "Epoch: 5222 \tTraining Loss: 0.050521\n",
      "Epoch: 5223 \tTraining Loss: 0.050599\n",
      "Epoch: 5224 \tTraining Loss: 0.050499\n",
      "Epoch: 5225 \tTraining Loss: 0.050586\n",
      "Epoch: 5226 \tTraining Loss: 0.050588\n",
      "Epoch: 5227 \tTraining Loss: 0.050514\n",
      "Epoch: 5228 \tTraining Loss: 0.049206\n",
      "Epoch: 5229 \tTraining Loss: 0.049138\n",
      "Epoch: 5230 \tTraining Loss: 0.049160\n",
      "Epoch: 5231 \tTraining Loss: 0.049097\n",
      "Epoch: 5232 \tTraining Loss: 0.049218\n",
      "Epoch: 5233 \tTraining Loss: 0.049068\n",
      "Epoch: 5234 \tTraining Loss: 0.049062\n",
      "Epoch: 5235 \tTraining Loss: 0.049097\n",
      "Epoch: 5236 \tTraining Loss: 0.049048\n",
      "Epoch: 5237 \tTraining Loss: 0.049049\n",
      "Epoch: 5238 \tTraining Loss: 0.049053\n",
      "Epoch: 5239 \tTraining Loss: 0.049045\n",
      "Epoch: 5240 \tTraining Loss: 0.049126\n",
      "Epoch: 5241 \tTraining Loss: 0.049032\n",
      "Epoch: 5242 \tTraining Loss: 0.049018\n",
      "Epoch: 5243 \tTraining Loss: 0.049016\n",
      "Epoch: 5244 \tTraining Loss: 0.049092\n",
      "Epoch: 5245 \tTraining Loss: 0.049075\n",
      "Epoch: 5246 \tTraining Loss: 0.049006\n",
      "Epoch: 5247 \tTraining Loss: 0.049039\n",
      "Epoch: 5248 \tTraining Loss: 0.049029\n",
      "Epoch: 5249 \tTraining Loss: 0.049019\n",
      "Epoch: 5250 \tTraining Loss: 0.049022\n",
      "Epoch: 5251 \tTraining Loss: 0.049045\n",
      "Epoch: 5252 \tTraining Loss: 0.049156\n",
      "Epoch: 5253 \tTraining Loss: 0.049009\n",
      "Epoch: 5254 \tTraining Loss: 0.049037\n",
      "Epoch: 5255 \tTraining Loss: 0.049148\n",
      "Epoch: 5256 \tTraining Loss: 0.049021\n",
      "Epoch: 5257 \tTraining Loss: 0.049018\n",
      "Epoch: 5258 \tTraining Loss: 0.049032\n",
      "Epoch: 5259 \tTraining Loss: 0.049020\n",
      "Epoch: 5260 \tTraining Loss: 0.049076\n",
      "Epoch: 5261 \tTraining Loss: 0.049053\n",
      "Epoch: 5262 \tTraining Loss: 0.049015\n",
      "Epoch: 5263 \tTraining Loss: 0.049001\n",
      "Epoch: 5264 \tTraining Loss: 0.049108\n",
      "Epoch: 5265 \tTraining Loss: 0.049009\n",
      "Epoch: 5266 \tTraining Loss: 0.049008\n",
      "Epoch: 5267 \tTraining Loss: 0.049034\n",
      "Epoch: 5268 \tTraining Loss: 0.048998\n",
      "Epoch: 5269 \tTraining Loss: 0.049110\n",
      "Epoch: 5270 \tTraining Loss: 0.049006\n",
      "Epoch: 5271 \tTraining Loss: 0.049015\n",
      "Epoch: 5272 \tTraining Loss: 0.049087\n",
      "Epoch: 5273 \tTraining Loss: 0.049117\n",
      "Epoch: 5274 \tTraining Loss: 0.048986\n",
      "Epoch: 5275 \tTraining Loss: 0.048981\n",
      "Epoch: 5276 \tTraining Loss: 0.049017\n",
      "Epoch: 5277 \tTraining Loss: 0.048977\n",
      "Epoch: 5278 \tTraining Loss: 0.048991\n",
      "Epoch: 5279 \tTraining Loss: 0.049021\n",
      "Epoch: 5280 \tTraining Loss: 0.049046\n",
      "Epoch: 5281 \tTraining Loss: 0.048989\n",
      "Epoch: 5282 \tTraining Loss: 0.049000\n",
      "Epoch: 5283 \tTraining Loss: 0.049050\n",
      "Epoch: 5284 \tTraining Loss: 0.049014\n",
      "Epoch: 5285 \tTraining Loss: 0.049095\n",
      "Epoch: 5286 \tTraining Loss: 0.049001\n",
      "Epoch: 5287 \tTraining Loss: 0.048975\n",
      "Epoch: 5288 \tTraining Loss: 0.049004\n",
      "Epoch: 5289 \tTraining Loss: 0.048995\n",
      "Epoch: 5290 \tTraining Loss: 0.048992\n",
      "Epoch: 5291 \tTraining Loss: 0.048994\n",
      "Epoch: 5292 \tTraining Loss: 0.049025\n",
      "Epoch: 5293 \tTraining Loss: 0.048967\n",
      "Epoch: 5294 \tTraining Loss: 0.048945\n",
      "Epoch: 5295 \tTraining Loss: 0.048992\n",
      "Epoch: 5296 \tTraining Loss: 0.048966\n",
      "Epoch: 5297 \tTraining Loss: 0.048996\n",
      "Epoch: 5298 \tTraining Loss: 0.048971\n",
      "Epoch: 5299 \tTraining Loss: 0.049019\n",
      "Epoch: 5300 \tTraining Loss: 0.048990\n",
      "Epoch: 5301 \tTraining Loss: 0.048956\n",
      "Epoch: 5302 \tTraining Loss: 0.049011\n",
      "Epoch: 5303 \tTraining Loss: 0.049081\n",
      "Epoch: 5304 \tTraining Loss: 0.049011\n",
      "Epoch: 5305 \tTraining Loss: 0.048978\n",
      "Epoch: 5306 \tTraining Loss: 0.049017\n",
      "Epoch: 5307 \tTraining Loss: 0.048943\n",
      "Epoch: 5308 \tTraining Loss: 0.048973\n",
      "Epoch: 5309 \tTraining Loss: 0.048950\n",
      "Epoch: 5310 \tTraining Loss: 0.048983\n",
      "Epoch: 5311 \tTraining Loss: 0.048956\n",
      "Epoch: 5312 \tTraining Loss: 0.049104\n",
      "Epoch: 5313 \tTraining Loss: 0.048991\n",
      "Epoch: 5314 \tTraining Loss: 0.049060\n",
      "Epoch: 5315 \tTraining Loss: 0.049047\n",
      "Epoch: 5316 \tTraining Loss: 0.049025\n",
      "Epoch: 5317 \tTraining Loss: 0.049081\n",
      "Epoch: 5318 \tTraining Loss: 0.048966\n",
      "Epoch: 5319 \tTraining Loss: 0.048975\n",
      "Epoch: 5320 \tTraining Loss: 0.049032\n",
      "Epoch: 5321 \tTraining Loss: 0.048947\n",
      "Epoch: 5322 \tTraining Loss: 0.048955\n",
      "Epoch: 5323 \tTraining Loss: 0.049005\n",
      "Epoch: 5324 \tTraining Loss: 0.048952\n",
      "Epoch: 5325 \tTraining Loss: 0.048981\n",
      "Epoch: 5326 \tTraining Loss: 0.048981\n",
      "Epoch: 5327 \tTraining Loss: 0.048955\n",
      "Epoch: 5328 \tTraining Loss: 0.048966\n",
      "Epoch: 5329 \tTraining Loss: 0.048950\n",
      "Epoch: 5330 \tTraining Loss: 0.048962\n",
      "Epoch: 5331 \tTraining Loss: 0.049016\n",
      "Epoch: 5332 \tTraining Loss: 0.048966\n",
      "Epoch: 5333 \tTraining Loss: 0.049249\n",
      "Epoch: 5334 \tTraining Loss: 0.049100\n",
      "Epoch: 5335 \tTraining Loss: 0.049063\n",
      "Epoch: 5336 \tTraining Loss: 0.048968\n",
      "Epoch: 5337 \tTraining Loss: 0.048971\n",
      "Epoch: 5338 \tTraining Loss: 0.049073\n",
      "Epoch: 5339 \tTraining Loss: 0.049035\n",
      "Epoch: 5340 \tTraining Loss: 0.048960\n",
      "Epoch: 5341 \tTraining Loss: 0.048935\n",
      "Epoch: 5342 \tTraining Loss: 0.048948\n",
      "Epoch: 5343 \tTraining Loss: 0.048972\n",
      "Epoch: 5344 \tTraining Loss: 0.048967\n",
      "Epoch: 5345 \tTraining Loss: 0.048923\n",
      "Epoch: 5346 \tTraining Loss: 0.048949\n",
      "Epoch: 5347 \tTraining Loss: 0.048968\n",
      "Epoch: 5348 \tTraining Loss: 0.060520\n",
      "Epoch: 5349 \tTraining Loss: 0.089587\n",
      "Epoch: 5350 \tTraining Loss: 0.158219\n",
      "Epoch: 5351 \tTraining Loss: 0.121971\n",
      "Epoch: 5352 \tTraining Loss: 0.086884\n",
      "Epoch: 5353 \tTraining Loss: 0.086523\n",
      "Epoch: 5354 \tTraining Loss: 0.130005\n",
      "Epoch: 5355 \tTraining Loss: 0.060606\n",
      "Epoch: 5356 \tTraining Loss: 0.065562\n",
      "Epoch: 5357 \tTraining Loss: 0.068405\n",
      "Epoch: 5358 \tTraining Loss: 0.057863\n",
      "Epoch: 5359 \tTraining Loss: 0.055147\n",
      "Epoch: 5360 \tTraining Loss: 0.064805\n",
      "Epoch: 5361 \tTraining Loss: 0.048754\n",
      "Epoch: 5362 \tTraining Loss: 0.047652\n",
      "Epoch: 5363 \tTraining Loss: 0.047170\n",
      "Epoch: 5364 \tTraining Loss: 0.046799\n",
      "Epoch: 5365 \tTraining Loss: 0.047269\n",
      "Epoch: 5366 \tTraining Loss: 0.045080\n",
      "Epoch: 5367 \tTraining Loss: 0.049478\n",
      "Epoch: 5368 \tTraining Loss: 0.045246\n",
      "Epoch: 5369 \tTraining Loss: 0.044358\n",
      "Epoch: 5370 \tTraining Loss: 0.044364\n",
      "Epoch: 5371 \tTraining Loss: 0.044249\n",
      "Epoch: 5372 \tTraining Loss: 0.044287\n",
      "Epoch: 5373 \tTraining Loss: 0.044328\n",
      "Epoch: 5374 \tTraining Loss: 0.044283\n",
      "Epoch: 5375 \tTraining Loss: 0.044442\n",
      "Epoch: 5376 \tTraining Loss: 0.044437\n",
      "Epoch: 5377 \tTraining Loss: 0.044770\n",
      "Epoch: 5378 \tTraining Loss: 0.045706\n",
      "Epoch: 5379 \tTraining Loss: 0.044028\n",
      "Epoch: 5380 \tTraining Loss: 0.045048\n",
      "Epoch: 5381 \tTraining Loss: 0.044592\n",
      "Epoch: 5382 \tTraining Loss: 0.044168\n",
      "Epoch: 5383 \tTraining Loss: 0.044081\n",
      "Epoch: 5384 \tTraining Loss: 0.044089\n",
      "Epoch: 5385 \tTraining Loss: 0.040970\n",
      "Epoch: 5386 \tTraining Loss: 0.048668\n",
      "Epoch: 5387 \tTraining Loss: 0.039867\n",
      "Epoch: 5388 \tTraining Loss: 0.040632\n",
      "Epoch: 5389 \tTraining Loss: 0.039183\n",
      "Epoch: 5390 \tTraining Loss: 0.039147\n",
      "Epoch: 5391 \tTraining Loss: 0.038975\n",
      "Epoch: 5392 \tTraining Loss: 0.038854\n",
      "Epoch: 5393 \tTraining Loss: 0.038899\n",
      "Epoch: 5394 \tTraining Loss: 0.038856\n",
      "Epoch: 5395 \tTraining Loss: 0.038770\n",
      "Epoch: 5396 \tTraining Loss: 0.038711\n",
      "Epoch: 5397 \tTraining Loss: 0.038753\n",
      "Epoch: 5398 \tTraining Loss: 0.038735\n",
      "Epoch: 5399 \tTraining Loss: 0.038749\n",
      "Epoch: 5400 \tTraining Loss: 0.038680\n",
      "Epoch: 5401 \tTraining Loss: 0.038723\n",
      "Epoch: 5402 \tTraining Loss: 0.038726\n",
      "Epoch: 5403 \tTraining Loss: 0.038641\n",
      "Epoch: 5404 \tTraining Loss: 0.038705\n",
      "Epoch: 5405 \tTraining Loss: 0.038621\n",
      "Epoch: 5406 \tTraining Loss: 0.038642\n",
      "Epoch: 5407 \tTraining Loss: 0.038644\n",
      "Epoch: 5408 \tTraining Loss: 0.038618\n",
      "Epoch: 5409 \tTraining Loss: 0.038705\n",
      "Epoch: 5410 \tTraining Loss: 0.038618\n",
      "Epoch: 5411 \tTraining Loss: 0.038611\n",
      "Epoch: 5412 \tTraining Loss: 0.038628\n",
      "Epoch: 5413 \tTraining Loss: 0.038588\n",
      "Epoch: 5414 \tTraining Loss: 0.038683\n",
      "Epoch: 5415 \tTraining Loss: 0.038628\n",
      "Epoch: 5416 \tTraining Loss: 0.038575\n",
      "Epoch: 5417 \tTraining Loss: 0.038577\n",
      "Epoch: 5418 \tTraining Loss: 0.038577\n",
      "Epoch: 5419 \tTraining Loss: 0.038601\n",
      "Epoch: 5420 \tTraining Loss: 0.038654\n",
      "Epoch: 5421 \tTraining Loss: 0.038637\n",
      "Epoch: 5422 \tTraining Loss: 0.038642\n",
      "Epoch: 5423 \tTraining Loss: 0.038579\n",
      "Epoch: 5424 \tTraining Loss: 0.038637\n",
      "Epoch: 5425 \tTraining Loss: 0.038656\n",
      "Epoch: 5426 \tTraining Loss: 0.038558\n",
      "Epoch: 5427 \tTraining Loss: 0.038590\n",
      "Epoch: 5428 \tTraining Loss: 0.038580\n",
      "Epoch: 5429 \tTraining Loss: 0.038559\n",
      "Epoch: 5430 \tTraining Loss: 0.038549\n",
      "Epoch: 5431 \tTraining Loss: 0.038556\n",
      "Epoch: 5432 \tTraining Loss: 0.038558\n",
      "Epoch: 5433 \tTraining Loss: 0.038665\n",
      "Epoch: 5434 \tTraining Loss: 0.038587\n",
      "Epoch: 5435 \tTraining Loss: 0.038577\n",
      "Epoch: 5436 \tTraining Loss: 0.038565\n",
      "Epoch: 5437 \tTraining Loss: 0.038581\n",
      "Epoch: 5438 \tTraining Loss: 0.038519\n",
      "Epoch: 5439 \tTraining Loss: 0.038671\n",
      "Epoch: 5440 \tTraining Loss: 0.038609\n",
      "Epoch: 5441 \tTraining Loss: 0.038542\n",
      "Epoch: 5442 \tTraining Loss: 0.038543\n",
      "Epoch: 5443 \tTraining Loss: 0.038535\n",
      "Epoch: 5444 \tTraining Loss: 0.038537\n",
      "Epoch: 5445 \tTraining Loss: 0.038557\n",
      "Epoch: 5446 \tTraining Loss: 0.038514\n",
      "Epoch: 5447 \tTraining Loss: 0.038700\n",
      "Epoch: 5448 \tTraining Loss: 0.038608\n",
      "Epoch: 5449 \tTraining Loss: 0.038545\n",
      "Epoch: 5450 \tTraining Loss: 0.038519\n",
      "Epoch: 5451 \tTraining Loss: 0.038517\n",
      "Epoch: 5452 \tTraining Loss: 0.038536\n",
      "Epoch: 5453 \tTraining Loss: 0.038522\n",
      "Epoch: 5454 \tTraining Loss: 0.038575\n",
      "Epoch: 5455 \tTraining Loss: 0.047153\n",
      "Epoch: 5456 \tTraining Loss: 0.139532\n",
      "Epoch: 5457 \tTraining Loss: 0.110103\n",
      "Epoch: 5458 \tTraining Loss: 0.074475\n",
      "Epoch: 5459 \tTraining Loss: 0.074855\n",
      "Epoch: 5460 \tTraining Loss: 0.175001\n",
      "Epoch: 5461 \tTraining Loss: 0.071753\n",
      "Epoch: 5462 \tTraining Loss: 0.067805\n",
      "Epoch: 5463 \tTraining Loss: 0.049978\n",
      "Epoch: 5464 \tTraining Loss: 0.048176\n",
      "Epoch: 5465 \tTraining Loss: 0.047446\n",
      "Epoch: 5466 \tTraining Loss: 0.046492\n",
      "Epoch: 5467 \tTraining Loss: 0.046578\n",
      "Epoch: 5468 \tTraining Loss: 0.046359\n",
      "Epoch: 5469 \tTraining Loss: 0.046643\n",
      "Epoch: 5470 \tTraining Loss: 0.046453\n",
      "Epoch: 5471 \tTraining Loss: 0.046928\n",
      "Epoch: 5472 \tTraining Loss: 0.047148\n",
      "Epoch: 5473 \tTraining Loss: 0.047720\n",
      "Epoch: 5474 \tTraining Loss: 0.046512\n",
      "Epoch: 5475 \tTraining Loss: 0.046625\n",
      "Epoch: 5476 \tTraining Loss: 0.046543\n",
      "Epoch: 5477 \tTraining Loss: 0.046363\n",
      "Epoch: 5478 \tTraining Loss: 0.046321\n",
      "Epoch: 5479 \tTraining Loss: 0.046236\n",
      "Epoch: 5480 \tTraining Loss: 0.046276\n",
      "Epoch: 5481 \tTraining Loss: 0.046235\n",
      "Epoch: 5482 \tTraining Loss: 0.046234\n",
      "Epoch: 5483 \tTraining Loss: 0.046211\n",
      "Epoch: 5484 \tTraining Loss: 0.046242\n",
      "Epoch: 5485 \tTraining Loss: 0.046269\n",
      "Epoch: 5486 \tTraining Loss: 0.046041\n",
      "Epoch: 5487 \tTraining Loss: 0.046280\n",
      "Epoch: 5488 \tTraining Loss: 0.047140\n",
      "Epoch: 5489 \tTraining Loss: 0.047966\n",
      "Epoch: 5490 \tTraining Loss: 0.047416\n",
      "Epoch: 5491 \tTraining Loss: 0.047226\n",
      "Epoch: 5492 \tTraining Loss: 0.047347\n",
      "Epoch: 5493 \tTraining Loss: 0.047239\n",
      "Epoch: 5494 \tTraining Loss: 0.047313\n",
      "Epoch: 5495 \tTraining Loss: 0.047348\n",
      "Epoch: 5496 \tTraining Loss: 0.047175\n",
      "Epoch: 5497 \tTraining Loss: 0.047270\n",
      "Epoch: 5498 \tTraining Loss: 0.047189\n",
      "Epoch: 5499 \tTraining Loss: 0.047081\n",
      "Epoch: 5500 \tTraining Loss: 0.047001\n",
      "Epoch: 5501 \tTraining Loss: 0.047467\n",
      "Epoch: 5502 \tTraining Loss: 0.047108\n",
      "Epoch: 5503 \tTraining Loss: 0.047547\n",
      "Epoch: 5504 \tTraining Loss: 0.047160\n",
      "Epoch: 5505 \tTraining Loss: 0.047010\n",
      "Epoch: 5506 \tTraining Loss: 0.047579\n",
      "Epoch: 5507 \tTraining Loss: 0.047570\n",
      "Epoch: 5508 \tTraining Loss: 0.046523\n",
      "Epoch: 5509 \tTraining Loss: 0.045988\n",
      "Epoch: 5510 \tTraining Loss: 0.045935\n",
      "Epoch: 5511 \tTraining Loss: 0.045941\n",
      "Epoch: 5512 \tTraining Loss: 0.046070\n",
      "Epoch: 5513 \tTraining Loss: 0.045923\n",
      "Epoch: 5514 \tTraining Loss: 0.045977\n",
      "Epoch: 5515 \tTraining Loss: 0.045878\n",
      "Epoch: 5516 \tTraining Loss: 0.045889\n",
      "Epoch: 5517 \tTraining Loss: 0.045888\n",
      "Epoch: 5518 \tTraining Loss: 0.045804\n",
      "Epoch: 5519 \tTraining Loss: 0.045897\n",
      "Epoch: 5520 \tTraining Loss: 0.045836\n",
      "Epoch: 5521 \tTraining Loss: 0.045853\n",
      "Epoch: 5522 \tTraining Loss: 0.046143\n",
      "Epoch: 5523 \tTraining Loss: 0.045782\n",
      "Epoch: 5524 \tTraining Loss: 0.045754\n",
      "Epoch: 5525 \tTraining Loss: 0.045753\n",
      "Epoch: 5526 \tTraining Loss: 0.045836\n",
      "Epoch: 5527 \tTraining Loss: 0.045783\n",
      "Epoch: 5528 \tTraining Loss: 0.045811\n",
      "Epoch: 5529 \tTraining Loss: 0.045813\n",
      "Epoch: 5530 \tTraining Loss: 0.045783\n",
      "Epoch: 5531 \tTraining Loss: 0.045827\n",
      "Epoch: 5532 \tTraining Loss: 0.045772\n",
      "Epoch: 5533 \tTraining Loss: 0.045799\n",
      "Epoch: 5534 \tTraining Loss: 0.045763\n",
      "Epoch: 5535 \tTraining Loss: 0.045798\n",
      "Epoch: 5536 \tTraining Loss: 0.045746\n",
      "Epoch: 5537 \tTraining Loss: 0.045760\n",
      "Epoch: 5538 \tTraining Loss: 0.045734\n",
      "Epoch: 5539 \tTraining Loss: 0.045778\n",
      "Epoch: 5540 \tTraining Loss: 0.045786\n",
      "Epoch: 5541 \tTraining Loss: 0.045726\n",
      "Epoch: 5542 \tTraining Loss: 0.045766\n",
      "Epoch: 5543 \tTraining Loss: 0.045758\n",
      "Epoch: 5544 \tTraining Loss: 0.045738\n",
      "Epoch: 5545 \tTraining Loss: 0.045817\n",
      "Epoch: 5546 \tTraining Loss: 0.045757\n",
      "Epoch: 5547 \tTraining Loss: 0.045725\n",
      "Epoch: 5548 \tTraining Loss: 0.045720\n",
      "Epoch: 5549 \tTraining Loss: 0.045751\n",
      "Epoch: 5550 \tTraining Loss: 0.045875\n",
      "Epoch: 5551 \tTraining Loss: 0.045847\n",
      "Epoch: 5552 \tTraining Loss: 0.045718\n",
      "Epoch: 5553 \tTraining Loss: 0.045730\n",
      "Epoch: 5554 \tTraining Loss: 0.045676\n",
      "Epoch: 5555 \tTraining Loss: 0.045737\n",
      "Epoch: 5556 \tTraining Loss: 0.045764\n",
      "Epoch: 5557 \tTraining Loss: 0.045810\n",
      "Epoch: 5558 \tTraining Loss: 0.045805\n",
      "Epoch: 5559 \tTraining Loss: 0.045679\n",
      "Epoch: 5560 \tTraining Loss: 0.045756\n",
      "Epoch: 5561 \tTraining Loss: 0.045686\n",
      "Epoch: 5562 \tTraining Loss: 0.045671\n",
      "Epoch: 5563 \tTraining Loss: 0.045886\n",
      "Epoch: 5564 \tTraining Loss: 0.045673\n",
      "Epoch: 5565 \tTraining Loss: 0.045723\n",
      "Epoch: 5566 \tTraining Loss: 0.045769\n",
      "Epoch: 5567 \tTraining Loss: 0.045656\n",
      "Epoch: 5568 \tTraining Loss: 0.045704\n",
      "Epoch: 5569 \tTraining Loss: 0.045774\n",
      "Epoch: 5570 \tTraining Loss: 0.045698\n",
      "Epoch: 5571 \tTraining Loss: 0.045735\n",
      "Epoch: 5572 \tTraining Loss: 0.045688\n",
      "Epoch: 5573 \tTraining Loss: 0.045654\n",
      "Epoch: 5574 \tTraining Loss: 0.045659\n",
      "Epoch: 5575 \tTraining Loss: 0.045664\n",
      "Epoch: 5576 \tTraining Loss: 0.045743\n",
      "Epoch: 5577 \tTraining Loss: 0.045662\n",
      "Epoch: 5578 \tTraining Loss: 0.045740\n",
      "Epoch: 5579 \tTraining Loss: 0.045678\n",
      "Epoch: 5580 \tTraining Loss: 0.045682\n",
      "Epoch: 5581 \tTraining Loss: 0.045696\n",
      "Epoch: 5582 \tTraining Loss: 0.045672\n",
      "Epoch: 5583 \tTraining Loss: 0.045658\n",
      "Epoch: 5584 \tTraining Loss: 0.045764\n",
      "Epoch: 5585 \tTraining Loss: 0.045791\n",
      "Epoch: 5586 \tTraining Loss: 0.045676\n",
      "Epoch: 5587 \tTraining Loss: 0.045656\n",
      "Epoch: 5588 \tTraining Loss: 0.045748\n",
      "Epoch: 5589 \tTraining Loss: 0.045650\n",
      "Epoch: 5590 \tTraining Loss: 0.045695\n",
      "Epoch: 5591 \tTraining Loss: 0.045662\n",
      "Epoch: 5592 \tTraining Loss: 0.045660\n",
      "Epoch: 5593 \tTraining Loss: 0.045642\n",
      "Epoch: 5594 \tTraining Loss: 0.045661\n",
      "Epoch: 5595 \tTraining Loss: 0.045646\n",
      "Epoch: 5596 \tTraining Loss: 0.045683\n",
      "Epoch: 5597 \tTraining Loss: 0.045674\n",
      "Epoch: 5598 \tTraining Loss: 0.045695\n",
      "Epoch: 5599 \tTraining Loss: 0.045648\n",
      "Epoch: 5600 \tTraining Loss: 0.045706\n",
      "Epoch: 5601 \tTraining Loss: 0.045596\n",
      "Epoch: 5602 \tTraining Loss: 0.045633\n",
      "Epoch: 5603 \tTraining Loss: 0.045845\n",
      "Epoch: 5604 \tTraining Loss: 0.045643\n",
      "Epoch: 5605 \tTraining Loss: 0.045594\n",
      "Epoch: 5606 \tTraining Loss: 0.054810\n",
      "Epoch: 5607 \tTraining Loss: 0.112056\n",
      "Epoch: 5608 \tTraining Loss: 0.175939\n",
      "Epoch: 5609 \tTraining Loss: 0.241728\n",
      "Epoch: 5610 \tTraining Loss: 0.184673\n",
      "Epoch: 5611 \tTraining Loss: 0.072940\n",
      "Epoch: 5612 \tTraining Loss: 0.084461\n",
      "Epoch: 5613 \tTraining Loss: 0.062144\n",
      "Epoch: 5614 \tTraining Loss: 0.058323\n",
      "Epoch: 5615 \tTraining Loss: 0.048331\n",
      "Epoch: 5616 \tTraining Loss: 0.047318\n",
      "Epoch: 5617 \tTraining Loss: 0.047407\n",
      "Epoch: 5618 \tTraining Loss: 0.044826\n",
      "Epoch: 5619 \tTraining Loss: 0.047199\n",
      "Epoch: 5620 \tTraining Loss: 0.042584\n",
      "Epoch: 5621 \tTraining Loss: 0.040269\n",
      "Epoch: 5622 \tTraining Loss: 0.040500\n",
      "Epoch: 5623 \tTraining Loss: 0.040114\n",
      "Epoch: 5624 \tTraining Loss: 0.039787\n",
      "Epoch: 5625 \tTraining Loss: 0.039851\n",
      "Epoch: 5626 \tTraining Loss: 0.039802\n",
      "Epoch: 5627 \tTraining Loss: 0.039695\n",
      "Epoch: 5628 \tTraining Loss: 0.039748\n",
      "Epoch: 5629 \tTraining Loss: 0.040408\n",
      "Epoch: 5630 \tTraining Loss: 0.040190\n",
      "Epoch: 5631 \tTraining Loss: 0.039508\n",
      "Epoch: 5632 \tTraining Loss: 0.038684\n",
      "Epoch: 5633 \tTraining Loss: 0.039750\n",
      "Epoch: 5634 \tTraining Loss: 0.039850\n",
      "Epoch: 5635 \tTraining Loss: 0.039571\n",
      "Epoch: 5636 \tTraining Loss: 0.039564\n",
      "Epoch: 5637 \tTraining Loss: 0.039417\n",
      "Epoch: 5638 \tTraining Loss: 0.039509\n",
      "Epoch: 5639 \tTraining Loss: 0.039403\n",
      "Epoch: 5640 \tTraining Loss: 0.039491\n",
      "Epoch: 5641 \tTraining Loss: 0.039496\n",
      "Epoch: 5642 \tTraining Loss: 0.039453\n",
      "Epoch: 5643 \tTraining Loss: 0.039348\n",
      "Epoch: 5644 \tTraining Loss: 0.039314\n",
      "Epoch: 5645 \tTraining Loss: 0.039442\n",
      "Epoch: 5646 \tTraining Loss: 0.039228\n",
      "Epoch: 5647 \tTraining Loss: 0.039357\n",
      "Epoch: 5648 \tTraining Loss: 0.039309\n",
      "Epoch: 5649 \tTraining Loss: 0.039400\n",
      "Epoch: 5650 \tTraining Loss: 0.039343\n",
      "Epoch: 5651 \tTraining Loss: 0.039223\n",
      "Epoch: 5652 \tTraining Loss: 0.039325\n",
      "Epoch: 5653 \tTraining Loss: 0.039279\n",
      "Epoch: 5654 \tTraining Loss: 0.039248\n",
      "Epoch: 5655 \tTraining Loss: 0.039263\n",
      "Epoch: 5656 \tTraining Loss: 0.039176\n",
      "Epoch: 5657 \tTraining Loss: 0.039391\n",
      "Epoch: 5658 \tTraining Loss: 0.039280\n",
      "Epoch: 5659 \tTraining Loss: 0.039148\n",
      "Epoch: 5660 \tTraining Loss: 0.039334\n",
      "Epoch: 5661 \tTraining Loss: 0.039219\n",
      "Epoch: 5662 \tTraining Loss: 0.039277\n",
      "Epoch: 5663 \tTraining Loss: 0.039166\n",
      "Epoch: 5664 \tTraining Loss: 0.039257\n",
      "Epoch: 5665 \tTraining Loss: 0.039464\n",
      "Epoch: 5666 \tTraining Loss: 0.039259\n",
      "Epoch: 5667 \tTraining Loss: 0.039229\n",
      "Epoch: 5668 \tTraining Loss: 0.039128\n",
      "Epoch: 5669 \tTraining Loss: 0.039299\n",
      "Epoch: 5670 \tTraining Loss: 0.039194\n",
      "Epoch: 5671 \tTraining Loss: 0.039241\n",
      "Epoch: 5672 \tTraining Loss: 0.039149\n",
      "Epoch: 5673 \tTraining Loss: 0.039184\n",
      "Epoch: 5674 \tTraining Loss: 0.039177\n",
      "Epoch: 5675 \tTraining Loss: 0.039196\n",
      "Epoch: 5676 \tTraining Loss: 0.039225\n",
      "Epoch: 5677 \tTraining Loss: 0.039143\n",
      "Epoch: 5678 \tTraining Loss: 0.039104\n",
      "Epoch: 5679 \tTraining Loss: 0.039072\n",
      "Epoch: 5680 \tTraining Loss: 0.039308\n",
      "Epoch: 5681 \tTraining Loss: 0.039132\n",
      "Epoch: 5682 \tTraining Loss: 0.039200\n",
      "Epoch: 5683 \tTraining Loss: 0.039160\n",
      "Epoch: 5684 \tTraining Loss: 0.039108\n",
      "Epoch: 5685 \tTraining Loss: 0.039234\n",
      "Epoch: 5686 \tTraining Loss: 0.039219\n",
      "Epoch: 5687 \tTraining Loss: 0.039193\n",
      "Epoch: 5688 \tTraining Loss: 0.039141\n",
      "Epoch: 5689 \tTraining Loss: 0.039108\n",
      "Epoch: 5690 \tTraining Loss: 0.039044\n",
      "Epoch: 5691 \tTraining Loss: 0.039173\n",
      "Epoch: 5692 \tTraining Loss: 0.039142\n",
      "Epoch: 5693 \tTraining Loss: 0.039109\n",
      "Epoch: 5694 \tTraining Loss: 0.039094\n",
      "Epoch: 5695 \tTraining Loss: 0.039116\n",
      "Epoch: 5696 \tTraining Loss: 0.039023\n",
      "Epoch: 5697 \tTraining Loss: 0.039268\n",
      "Epoch: 5698 \tTraining Loss: 0.039101\n",
      "Epoch: 5699 \tTraining Loss: 0.039188\n",
      "Epoch: 5700 \tTraining Loss: 0.039067\n",
      "Epoch: 5701 \tTraining Loss: 0.039079\n",
      "Epoch: 5702 \tTraining Loss: 0.038164\n",
      "Epoch: 5703 \tTraining Loss: 0.038931\n",
      "Epoch: 5704 \tTraining Loss: 0.038062\n",
      "Epoch: 5705 \tTraining Loss: 0.038536\n",
      "Epoch: 5706 \tTraining Loss: 0.038021\n",
      "Epoch: 5707 \tTraining Loss: 0.037972\n",
      "Epoch: 5708 \tTraining Loss: 0.037942\n",
      "Epoch: 5709 \tTraining Loss: 0.037928\n",
      "Epoch: 5710 \tTraining Loss: 0.038140\n",
      "Epoch: 5711 \tTraining Loss: 0.038065\n",
      "Epoch: 5712 \tTraining Loss: 0.038062\n",
      "Epoch: 5713 \tTraining Loss: 0.038009\n",
      "Epoch: 5714 \tTraining Loss: 0.037926\n",
      "Epoch: 5715 \tTraining Loss: 0.037974\n",
      "Epoch: 5716 \tTraining Loss: 0.037921\n",
      "Epoch: 5717 \tTraining Loss: 0.038098\n",
      "Epoch: 5718 \tTraining Loss: 0.038009\n",
      "Epoch: 5719 \tTraining Loss: 0.038037\n",
      "Epoch: 5720 \tTraining Loss: 0.038034\n",
      "Epoch: 5721 \tTraining Loss: 0.037995\n",
      "Epoch: 5722 \tTraining Loss: 0.038133\n",
      "Epoch: 5723 \tTraining Loss: 0.038010\n",
      "Epoch: 5724 \tTraining Loss: 0.037995\n",
      "Epoch: 5725 \tTraining Loss: 0.037947\n",
      "Epoch: 5726 \tTraining Loss: 0.038009\n",
      "Epoch: 5727 \tTraining Loss: 0.037936\n",
      "Epoch: 5728 \tTraining Loss: 0.038075\n",
      "Epoch: 5729 \tTraining Loss: 0.038019\n",
      "Epoch: 5730 \tTraining Loss: 0.038096\n",
      "Epoch: 5731 \tTraining Loss: 0.037950\n",
      "Epoch: 5732 \tTraining Loss: 0.037969\n",
      "Epoch: 5733 \tTraining Loss: 0.037893\n",
      "Epoch: 5734 \tTraining Loss: 0.037884\n",
      "Epoch: 5735 \tTraining Loss: 0.037939\n",
      "Epoch: 5736 \tTraining Loss: 0.037917\n",
      "Epoch: 5737 \tTraining Loss: 0.037915\n",
      "Epoch: 5738 \tTraining Loss: 0.037625\n",
      "Epoch: 5739 \tTraining Loss: 0.050724\n",
      "Epoch: 5740 \tTraining Loss: 0.041621\n",
      "Epoch: 5741 \tTraining Loss: 0.042004\n",
      "Epoch: 5742 \tTraining Loss: 0.056883\n",
      "Epoch: 5743 \tTraining Loss: 0.139208\n",
      "Epoch: 5744 \tTraining Loss: 0.257206\n",
      "Epoch: 5745 \tTraining Loss: 0.143526\n",
      "Epoch: 5746 \tTraining Loss: 0.139352\n",
      "Epoch: 5747 \tTraining Loss: 0.077825\n",
      "Epoch: 5748 \tTraining Loss: 0.059168\n",
      "Epoch: 5749 \tTraining Loss: 0.050141\n",
      "Epoch: 5750 \tTraining Loss: 0.048611\n",
      "Epoch: 5751 \tTraining Loss: 0.047633\n",
      "Epoch: 5752 \tTraining Loss: 0.045865\n",
      "Epoch: 5753 \tTraining Loss: 0.045419\n",
      "Epoch: 5754 \tTraining Loss: 0.045565\n",
      "Epoch: 5755 \tTraining Loss: 0.045359\n",
      "Epoch: 5756 \tTraining Loss: 0.045473\n",
      "Epoch: 5757 \tTraining Loss: 0.045542\n",
      "Epoch: 5758 \tTraining Loss: 0.045340\n",
      "Epoch: 5759 \tTraining Loss: 0.045368\n",
      "Epoch: 5760 \tTraining Loss: 0.045406\n",
      "Epoch: 5761 \tTraining Loss: 0.045272\n",
      "Epoch: 5762 \tTraining Loss: 0.045306\n",
      "Epoch: 5763 \tTraining Loss: 0.045306\n",
      "Epoch: 5764 \tTraining Loss: 0.045297\n",
      "Epoch: 5765 \tTraining Loss: 0.045649\n",
      "Epoch: 5766 \tTraining Loss: 0.046140\n",
      "Epoch: 5767 \tTraining Loss: 0.046144\n",
      "Epoch: 5768 \tTraining Loss: 0.046095\n",
      "Epoch: 5769 \tTraining Loss: 0.046094\n",
      "Epoch: 5770 \tTraining Loss: 0.046178\n",
      "Epoch: 5771 \tTraining Loss: 0.046146\n",
      "Epoch: 5772 \tTraining Loss: 0.046132\n",
      "Epoch: 5773 \tTraining Loss: 0.046154\n",
      "Epoch: 5774 \tTraining Loss: 0.046310\n",
      "Epoch: 5775 \tTraining Loss: 0.058755\n",
      "Epoch: 5776 \tTraining Loss: 0.046588\n",
      "Epoch: 5777 \tTraining Loss: 0.047643\n",
      "Epoch: 5778 \tTraining Loss: 0.046250\n",
      "Epoch: 5779 \tTraining Loss: 0.046183\n",
      "Epoch: 5780 \tTraining Loss: 0.046070\n",
      "Epoch: 5781 \tTraining Loss: 0.046067\n",
      "Epoch: 5782 \tTraining Loss: 0.046045\n",
      "Epoch: 5783 \tTraining Loss: 0.046128\n",
      "Epoch: 5784 \tTraining Loss: 0.046052\n",
      "Epoch: 5785 \tTraining Loss: 0.046073\n",
      "Epoch: 5786 \tTraining Loss: 0.046032\n",
      "Epoch: 5787 \tTraining Loss: 0.046153\n",
      "Epoch: 5788 \tTraining Loss: 0.046034\n",
      "Epoch: 5789 \tTraining Loss: 0.046053\n",
      "Epoch: 5790 \tTraining Loss: 0.046024\n",
      "Epoch: 5791 \tTraining Loss: 0.046058\n",
      "Epoch: 5792 \tTraining Loss: 0.046064\n",
      "Epoch: 5793 \tTraining Loss: 0.046021\n",
      "Epoch: 5794 \tTraining Loss: 0.046121\n",
      "Epoch: 5795 \tTraining Loss: 0.046008\n",
      "Epoch: 5796 \tTraining Loss: 0.046012\n",
      "Epoch: 5797 \tTraining Loss: 0.046064\n",
      "Epoch: 5798 \tTraining Loss: 0.046038\n",
      "Epoch: 5799 \tTraining Loss: 0.046006\n",
      "Epoch: 5800 \tTraining Loss: 0.046059\n",
      "Epoch: 5801 \tTraining Loss: 0.046014\n",
      "Epoch: 5802 \tTraining Loss: 0.046023\n",
      "Epoch: 5803 \tTraining Loss: 0.046025\n",
      "Epoch: 5804 \tTraining Loss: 0.046009\n",
      "Epoch: 5805 \tTraining Loss: 0.046046\n",
      "Epoch: 5806 \tTraining Loss: 0.045990\n",
      "Epoch: 5807 \tTraining Loss: 0.046016\n",
      "Epoch: 5808 \tTraining Loss: 0.046025\n",
      "Epoch: 5809 \tTraining Loss: 0.045985\n",
      "Epoch: 5810 \tTraining Loss: 0.046003\n",
      "Epoch: 5811 \tTraining Loss: 0.046029\n",
      "Epoch: 5812 \tTraining Loss: 0.046013\n",
      "Epoch: 5813 \tTraining Loss: 0.046037\n",
      "Epoch: 5814 \tTraining Loss: 0.045979\n",
      "Epoch: 5815 \tTraining Loss: 0.046041\n",
      "Epoch: 5816 \tTraining Loss: 0.045988\n",
      "Epoch: 5817 \tTraining Loss: 0.046059\n",
      "Epoch: 5818 \tTraining Loss: 0.046099\n",
      "Epoch: 5819 \tTraining Loss: 0.046018\n",
      "Epoch: 5820 \tTraining Loss: 0.046817\n",
      "Epoch: 5821 \tTraining Loss: 0.046874\n",
      "Epoch: 5822 \tTraining Loss: 0.046761\n",
      "Epoch: 5823 \tTraining Loss: 0.046664\n",
      "Epoch: 5824 \tTraining Loss: 0.046684\n",
      "Epoch: 5825 \tTraining Loss: 0.046666\n",
      "Epoch: 5826 \tTraining Loss: 0.046631\n",
      "Epoch: 5827 \tTraining Loss: 0.046644\n",
      "Epoch: 5828 \tTraining Loss: 0.046623\n",
      "Epoch: 5829 \tTraining Loss: 0.046602\n",
      "Epoch: 5830 \tTraining Loss: 0.046626\n",
      "Epoch: 5831 \tTraining Loss: 0.046602\n",
      "Epoch: 5832 \tTraining Loss: 0.046643\n",
      "Epoch: 5833 \tTraining Loss: 0.046598\n",
      "Epoch: 5834 \tTraining Loss: 0.046785\n",
      "Epoch: 5835 \tTraining Loss: 0.046595\n",
      "Epoch: 5836 \tTraining Loss: 0.046643\n",
      "Epoch: 5837 \tTraining Loss: 0.046599\n",
      "Epoch: 5838 \tTraining Loss: 0.046614\n",
      "Epoch: 5839 \tTraining Loss: 0.046594\n",
      "Epoch: 5840 \tTraining Loss: 0.046613\n",
      "Epoch: 5841 \tTraining Loss: 0.046602\n",
      "Epoch: 5842 \tTraining Loss: 0.046620\n",
      "Epoch: 5843 \tTraining Loss: 0.046584\n",
      "Epoch: 5844 \tTraining Loss: 0.046584\n",
      "Epoch: 5845 \tTraining Loss: 0.046694\n",
      "Epoch: 5846 \tTraining Loss: 0.047559\n",
      "Epoch: 5847 \tTraining Loss: 0.048647\n",
      "Epoch: 5848 \tTraining Loss: 0.048508\n",
      "Epoch: 5849 \tTraining Loss: 0.048520\n",
      "Epoch: 5850 \tTraining Loss: 0.048455\n",
      "Epoch: 5851 \tTraining Loss: 0.048439\n",
      "Epoch: 5852 \tTraining Loss: 0.048462\n",
      "Epoch: 5853 \tTraining Loss: 0.048496\n",
      "Epoch: 5854 \tTraining Loss: 0.048450\n",
      "Epoch: 5855 \tTraining Loss: 0.048456\n",
      "Epoch: 5856 \tTraining Loss: 0.048429\n",
      "Epoch: 5857 \tTraining Loss: 0.048466\n",
      "Epoch: 5858 \tTraining Loss: 0.048451\n",
      "Epoch: 5859 \tTraining Loss: 0.048460\n",
      "Epoch: 5860 \tTraining Loss: 0.048410\n",
      "Epoch: 5861 \tTraining Loss: 0.048423\n",
      "Epoch: 5862 \tTraining Loss: 0.048411\n",
      "Epoch: 5863 \tTraining Loss: 0.048415\n",
      "Epoch: 5864 \tTraining Loss: 0.048419\n",
      "Epoch: 5865 \tTraining Loss: 0.048416\n",
      "Epoch: 5866 \tTraining Loss: 0.048455\n",
      "Epoch: 5867 \tTraining Loss: 0.048580\n",
      "Epoch: 5868 \tTraining Loss: 0.048386\n",
      "Epoch: 5869 \tTraining Loss: 0.048435\n",
      "Epoch: 5870 \tTraining Loss: 0.048425\n",
      "Epoch: 5871 \tTraining Loss: 0.048392\n",
      "Epoch: 5872 \tTraining Loss: 0.048433\n",
      "Epoch: 5873 \tTraining Loss: 0.048437\n",
      "Epoch: 5874 \tTraining Loss: 0.048430\n",
      "Epoch: 5875 \tTraining Loss: 0.048413\n",
      "Epoch: 5876 \tTraining Loss: 0.048477\n",
      "Epoch: 5877 \tTraining Loss: 0.048413\n",
      "Epoch: 5878 \tTraining Loss: 0.048391\n",
      "Epoch: 5879 \tTraining Loss: 0.048415\n",
      "Epoch: 5880 \tTraining Loss: 0.048391\n",
      "Epoch: 5881 \tTraining Loss: 0.048433\n",
      "Epoch: 5882 \tTraining Loss: 0.048421\n",
      "Epoch: 5883 \tTraining Loss: 0.048429\n",
      "Epoch: 5884 \tTraining Loss: 0.048403\n",
      "Epoch: 5885 \tTraining Loss: 0.048421\n",
      "Epoch: 5886 \tTraining Loss: 0.048380\n",
      "Epoch: 5887 \tTraining Loss: 0.048391\n",
      "Epoch: 5888 \tTraining Loss: 0.048497\n",
      "Epoch: 5889 \tTraining Loss: 0.048448\n",
      "Epoch: 5890 \tTraining Loss: 0.048406\n",
      "Epoch: 5891 \tTraining Loss: 0.048417\n",
      "Epoch: 5892 \tTraining Loss: 0.048435\n",
      "Epoch: 5893 \tTraining Loss: 0.048396\n",
      "Epoch: 5894 \tTraining Loss: 0.048384\n",
      "Epoch: 5895 \tTraining Loss: 0.048406\n",
      "Epoch: 5896 \tTraining Loss: 0.048435\n",
      "Epoch: 5897 \tTraining Loss: 0.048419\n",
      "Epoch: 5898 \tTraining Loss: 0.048388\n",
      "Epoch: 5899 \tTraining Loss: 0.048396\n",
      "Epoch: 5900 \tTraining Loss: 0.048479\n",
      "Epoch: 5901 \tTraining Loss: 0.048376\n",
      "Epoch: 5902 \tTraining Loss: 0.048389\n",
      "Epoch: 5903 \tTraining Loss: 0.048373\n",
      "Epoch: 5904 \tTraining Loss: 0.048380\n",
      "Epoch: 5905 \tTraining Loss: 0.048380\n",
      "Epoch: 5906 \tTraining Loss: 0.048420\n",
      "Epoch: 5907 \tTraining Loss: 0.048382\n",
      "Epoch: 5908 \tTraining Loss: 0.048388\n",
      "Epoch: 5909 \tTraining Loss: 0.048376\n",
      "Epoch: 5910 \tTraining Loss: 0.048369\n",
      "Epoch: 5911 \tTraining Loss: 0.048390\n",
      "Epoch: 5912 \tTraining Loss: 0.048500\n",
      "Epoch: 5913 \tTraining Loss: 0.048441\n",
      "Epoch: 5914 \tTraining Loss: 0.048431\n",
      "Epoch: 5915 \tTraining Loss: 0.048431\n",
      "Epoch: 5916 \tTraining Loss: 0.046578\n",
      "Epoch: 5917 \tTraining Loss: 0.051046\n",
      "Epoch: 5918 \tTraining Loss: 0.066714\n",
      "Epoch: 5919 \tTraining Loss: 0.051205\n",
      "Epoch: 5920 \tTraining Loss: 0.057230\n",
      "Epoch: 5921 \tTraining Loss: 0.190035\n",
      "Epoch: 5922 \tTraining Loss: 0.100692\n",
      "Epoch: 5923 \tTraining Loss: 0.097268\n",
      "Epoch: 5924 \tTraining Loss: 0.088642\n",
      "Epoch: 5925 \tTraining Loss: 0.070991\n",
      "Epoch: 5926 \tTraining Loss: 0.056241\n",
      "Epoch: 5927 \tTraining Loss: 0.067018\n",
      "Epoch: 5928 \tTraining Loss: 0.053819\n",
      "Epoch: 5929 \tTraining Loss: 0.048701\n",
      "Epoch: 5930 \tTraining Loss: 0.047696\n",
      "Epoch: 5931 \tTraining Loss: 0.047448\n",
      "Epoch: 5932 \tTraining Loss: 0.047429\n",
      "Epoch: 5933 \tTraining Loss: 0.047430\n",
      "Epoch: 5934 \tTraining Loss: 0.047392\n",
      "Epoch: 5935 \tTraining Loss: 0.047444\n",
      "Epoch: 5936 \tTraining Loss: 0.047364\n",
      "Epoch: 5937 \tTraining Loss: 0.047432\n",
      "Epoch: 5938 \tTraining Loss: 0.047425\n",
      "Epoch: 5939 \tTraining Loss: 0.047364\n",
      "Epoch: 5940 \tTraining Loss: 0.047325\n",
      "Epoch: 5941 \tTraining Loss: 0.047355\n",
      "Epoch: 5942 \tTraining Loss: 0.047379\n",
      "Epoch: 5943 \tTraining Loss: 0.047445\n",
      "Epoch: 5944 \tTraining Loss: 0.047335\n",
      "Epoch: 5945 \tTraining Loss: 0.047365\n",
      "Epoch: 5946 \tTraining Loss: 0.047331\n",
      "Epoch: 5947 \tTraining Loss: 0.047374\n",
      "Epoch: 5948 \tTraining Loss: 0.047333\n",
      "Epoch: 5949 \tTraining Loss: 0.047339\n",
      "Epoch: 5950 \tTraining Loss: 0.047309\n",
      "Epoch: 5951 \tTraining Loss: 0.047300\n",
      "Epoch: 5952 \tTraining Loss: 0.047452\n",
      "Epoch: 5953 \tTraining Loss: 0.047313\n",
      "Epoch: 5954 \tTraining Loss: 0.047326\n",
      "Epoch: 5955 \tTraining Loss: 0.047308\n",
      "Epoch: 5956 \tTraining Loss: 0.047280\n",
      "Epoch: 5957 \tTraining Loss: 0.047314\n",
      "Epoch: 5958 \tTraining Loss: 0.047327\n",
      "Epoch: 5959 \tTraining Loss: 0.047278\n",
      "Epoch: 5960 \tTraining Loss: 0.047323\n",
      "Epoch: 5961 \tTraining Loss: 0.047294\n",
      "Epoch: 5962 \tTraining Loss: 0.047338\n",
      "Epoch: 5963 \tTraining Loss: 0.047304\n",
      "Epoch: 5964 \tTraining Loss: 0.047333\n",
      "Epoch: 5965 \tTraining Loss: 0.047320\n",
      "Epoch: 5966 \tTraining Loss: 0.047344\n",
      "Epoch: 5967 \tTraining Loss: 0.047277\n",
      "Epoch: 5968 \tTraining Loss: 0.047348\n",
      "Epoch: 5969 \tTraining Loss: 0.047297\n",
      "Epoch: 5970 \tTraining Loss: 0.047310\n",
      "Epoch: 5971 \tTraining Loss: 0.047280\n",
      "Epoch: 5972 \tTraining Loss: 0.047296\n",
      "Epoch: 5973 \tTraining Loss: 0.047294\n",
      "Epoch: 5974 \tTraining Loss: 0.047268\n",
      "Epoch: 5975 \tTraining Loss: 0.047279\n",
      "Epoch: 5976 \tTraining Loss: 0.047284\n",
      "Epoch: 5977 \tTraining Loss: 0.047348\n",
      "Epoch: 5978 \tTraining Loss: 0.047287\n",
      "Epoch: 5979 \tTraining Loss: 0.047286\n",
      "Epoch: 5980 \tTraining Loss: 0.047344\n",
      "Epoch: 5981 \tTraining Loss: 0.047269\n",
      "Epoch: 5982 \tTraining Loss: 0.047270\n",
      "Epoch: 5983 \tTraining Loss: 0.047251\n",
      "Epoch: 5984 \tTraining Loss: 0.047309\n",
      "Epoch: 5985 \tTraining Loss: 0.047482\n",
      "Epoch: 5986 \tTraining Loss: 0.047298\n",
      "Epoch: 5987 \tTraining Loss: 0.047302\n",
      "Epoch: 5988 \tTraining Loss: 0.047361\n",
      "Epoch: 5989 \tTraining Loss: 0.047284\n",
      "Epoch: 5990 \tTraining Loss: 0.047256\n",
      "Epoch: 5991 \tTraining Loss: 0.047289\n",
      "Epoch: 5992 \tTraining Loss: 0.047243\n",
      "Epoch: 5993 \tTraining Loss: 0.046625\n",
      "Epoch: 5994 \tTraining Loss: 0.043931\n",
      "Epoch: 5995 \tTraining Loss: 0.048887\n",
      "Epoch: 5996 \tTraining Loss: 0.044383\n",
      "Epoch: 5997 \tTraining Loss: 0.045058\n",
      "Epoch: 5998 \tTraining Loss: 0.045033\n",
      "Epoch: 5999 \tTraining Loss: 0.045031\n",
      "Epoch: 6000 \tTraining Loss: 0.045018\n",
      "Epoch: 6001 \tTraining Loss: 0.044999\n",
      "Epoch: 6002 \tTraining Loss: 0.045135\n",
      "Epoch: 6003 \tTraining Loss: 0.044976\n",
      "Epoch: 6004 \tTraining Loss: 0.044985\n",
      "Epoch: 6005 \tTraining Loss: 0.044999\n",
      "Epoch: 6006 \tTraining Loss: 0.045000\n",
      "Epoch: 6007 \tTraining Loss: 0.045025\n",
      "Epoch: 6008 \tTraining Loss: 0.045010\n",
      "Epoch: 6009 \tTraining Loss: 0.044961\n",
      "Epoch: 6010 \tTraining Loss: 0.071412\n",
      "Epoch: 6011 \tTraining Loss: 0.049182\n",
      "Epoch: 6012 \tTraining Loss: 0.046971\n",
      "Epoch: 6013 \tTraining Loss: 0.051075\n",
      "Epoch: 6014 \tTraining Loss: 0.056112\n",
      "Epoch: 6015 \tTraining Loss: 0.043625\n",
      "Epoch: 6016 \tTraining Loss: 0.044178\n",
      "Epoch: 6017 \tTraining Loss: 0.044740\n",
      "Epoch: 6018 \tTraining Loss: 0.055394\n",
      "Epoch: 6019 \tTraining Loss: 0.048412\n",
      "Epoch: 6020 \tTraining Loss: 0.040522\n",
      "Epoch: 6021 \tTraining Loss: 0.039503\n",
      "Epoch: 6022 \tTraining Loss: 0.038053\n",
      "Epoch: 6023 \tTraining Loss: 0.037955\n",
      "Epoch: 6024 \tTraining Loss: 0.037919\n",
      "Epoch: 6025 \tTraining Loss: 0.037929\n",
      "Epoch: 6026 \tTraining Loss: 0.037917\n",
      "Epoch: 6027 \tTraining Loss: 0.037897\n",
      "Epoch: 6028 \tTraining Loss: 0.037854\n",
      "Epoch: 6029 \tTraining Loss: 0.037952\n",
      "Epoch: 6030 \tTraining Loss: 0.037856\n",
      "Epoch: 6031 \tTraining Loss: 0.037877\n",
      "Epoch: 6032 \tTraining Loss: 0.037839\n",
      "Epoch: 6033 \tTraining Loss: 0.037846\n",
      "Epoch: 6034 \tTraining Loss: 0.037820\n",
      "Epoch: 6035 \tTraining Loss: 0.037819\n",
      "Epoch: 6036 \tTraining Loss: 0.037803\n",
      "Epoch: 6037 \tTraining Loss: 0.037797\n",
      "Epoch: 6038 \tTraining Loss: 0.037878\n",
      "Epoch: 6039 \tTraining Loss: 0.037810\n",
      "Epoch: 6040 \tTraining Loss: 0.037803\n",
      "Epoch: 6041 \tTraining Loss: 0.037823\n",
      "Epoch: 6042 \tTraining Loss: 0.037809\n",
      "Epoch: 6043 \tTraining Loss: 0.037805\n",
      "Epoch: 6044 \tTraining Loss: 0.037788\n",
      "Epoch: 6045 \tTraining Loss: 0.037803\n",
      "Epoch: 6046 \tTraining Loss: 0.037877\n",
      "Epoch: 6047 \tTraining Loss: 0.037816\n",
      "Epoch: 6048 \tTraining Loss: 0.037835\n",
      "Epoch: 6049 \tTraining Loss: 0.037929\n",
      "Epoch: 6050 \tTraining Loss: 0.037861\n",
      "Epoch: 6051 \tTraining Loss: 0.037787\n",
      "Epoch: 6052 \tTraining Loss: 0.037783\n",
      "Epoch: 6053 \tTraining Loss: 0.037774\n",
      "Epoch: 6054 \tTraining Loss: 0.037775\n",
      "Epoch: 6055 \tTraining Loss: 0.037757\n",
      "Epoch: 6056 \tTraining Loss: 0.037789\n",
      "Epoch: 6057 \tTraining Loss: 0.037800\n",
      "Epoch: 6058 \tTraining Loss: 0.037804\n",
      "Epoch: 6059 \tTraining Loss: 0.037833\n",
      "Epoch: 6060 \tTraining Loss: 0.037761\n",
      "Epoch: 6061 \tTraining Loss: 0.037811\n",
      "Epoch: 6062 \tTraining Loss: 0.037778\n",
      "Epoch: 6063 \tTraining Loss: 0.037808\n",
      "Epoch: 6064 \tTraining Loss: 0.037757\n",
      "Epoch: 6065 \tTraining Loss: 0.037786\n",
      "Epoch: 6066 \tTraining Loss: 0.037786\n",
      "Epoch: 6067 \tTraining Loss: 0.037854\n",
      "Epoch: 6068 \tTraining Loss: 0.037793\n",
      "Epoch: 6069 \tTraining Loss: 0.037800\n",
      "Epoch: 6070 \tTraining Loss: 0.037779\n",
      "Epoch: 6071 \tTraining Loss: 0.037817\n",
      "Epoch: 6072 \tTraining Loss: 0.037854\n",
      "Epoch: 6073 \tTraining Loss: 0.037775\n",
      "Epoch: 6074 \tTraining Loss: 0.037727\n",
      "Epoch: 6075 \tTraining Loss: 0.037855\n",
      "Epoch: 6076 \tTraining Loss: 0.038009\n",
      "Epoch: 6077 \tTraining Loss: 0.037845\n",
      "Epoch: 6078 \tTraining Loss: 0.037802\n",
      "Epoch: 6079 \tTraining Loss: 0.037757\n",
      "Epoch: 6080 \tTraining Loss: 0.037817\n",
      "Epoch: 6081 \tTraining Loss: 0.037770\n",
      "Epoch: 6082 \tTraining Loss: 0.037785\n",
      "Epoch: 6083 \tTraining Loss: 0.037763\n",
      "Epoch: 6084 \tTraining Loss: 0.037737\n",
      "Epoch: 6085 \tTraining Loss: 0.037757\n",
      "Epoch: 6086 \tTraining Loss: 0.037806\n",
      "Epoch: 6087 \tTraining Loss: 0.037846\n",
      "Epoch: 6088 \tTraining Loss: 0.037754\n",
      "Epoch: 6089 \tTraining Loss: 0.037768\n",
      "Epoch: 6090 \tTraining Loss: 0.037823\n",
      "Epoch: 6091 \tTraining Loss: 0.037772\n",
      "Epoch: 6092 \tTraining Loss: 0.037733\n",
      "Epoch: 6093 \tTraining Loss: 0.037773\n",
      "Epoch: 6094 \tTraining Loss: 0.037824\n",
      "Epoch: 6095 \tTraining Loss: 0.037754\n",
      "Epoch: 6096 \tTraining Loss: 0.037753\n",
      "Epoch: 6097 \tTraining Loss: 0.037763\n",
      "Epoch: 6098 \tTraining Loss: 0.037776\n",
      "Epoch: 6099 \tTraining Loss: 0.037776\n",
      "Epoch: 6100 \tTraining Loss: 0.037718\n",
      "Epoch: 6101 \tTraining Loss: 0.037787\n",
      "Epoch: 6102 \tTraining Loss: 0.037713\n",
      "Epoch: 6103 \tTraining Loss: 0.037783\n",
      "Epoch: 6104 \tTraining Loss: 0.037770\n",
      "Epoch: 6105 \tTraining Loss: 0.037753\n",
      "Epoch: 6106 \tTraining Loss: 0.037781\n",
      "Epoch: 6107 \tTraining Loss: 0.037760\n",
      "Epoch: 6108 \tTraining Loss: 0.037778\n",
      "Epoch: 6109 \tTraining Loss: 0.037759\n",
      "Epoch: 6110 \tTraining Loss: 0.037773\n",
      "Epoch: 6111 \tTraining Loss: 0.037745\n",
      "Epoch: 6112 \tTraining Loss: 0.037788\n",
      "Epoch: 6113 \tTraining Loss: 0.037732\n",
      "Epoch: 6114 \tTraining Loss: 0.037784\n",
      "Epoch: 6115 \tTraining Loss: 0.037795\n",
      "Epoch: 6116 \tTraining Loss: 0.037748\n",
      "Epoch: 6117 \tTraining Loss: 0.037757\n",
      "Epoch: 6118 \tTraining Loss: 0.037795\n",
      "Epoch: 6119 \tTraining Loss: 0.037759\n",
      "Epoch: 6120 \tTraining Loss: 0.037722\n",
      "Epoch: 6121 \tTraining Loss: 0.037776\n",
      "Epoch: 6122 \tTraining Loss: 0.037802\n",
      "Epoch: 6123 \tTraining Loss: 0.037747\n",
      "Epoch: 6124 \tTraining Loss: 0.037749\n",
      "Epoch: 6125 \tTraining Loss: 0.037837\n",
      "Epoch: 6126 \tTraining Loss: 0.037753\n",
      "Epoch: 6127 \tTraining Loss: 0.037736\n",
      "Epoch: 6128 \tTraining Loss: 0.037748\n",
      "Epoch: 6129 \tTraining Loss: 0.037752\n",
      "Epoch: 6130 \tTraining Loss: 0.037753\n",
      "Epoch: 6131 \tTraining Loss: 0.037820\n",
      "Epoch: 6132 \tTraining Loss: 0.037737\n",
      "Epoch: 6133 \tTraining Loss: 0.037777\n",
      "Epoch: 6134 \tTraining Loss: 0.037731\n",
      "Epoch: 6135 \tTraining Loss: 0.037757\n",
      "Epoch: 6136 \tTraining Loss: 0.037733\n",
      "Epoch: 6137 \tTraining Loss: 0.037772\n",
      "Epoch: 6138 \tTraining Loss: 0.037745\n",
      "Epoch: 6139 \tTraining Loss: 0.037760\n",
      "Epoch: 6140 \tTraining Loss: 0.037749\n",
      "Epoch: 6141 \tTraining Loss: 0.037750\n",
      "Epoch: 6142 \tTraining Loss: 0.037737\n",
      "Epoch: 6143 \tTraining Loss: 0.037732\n",
      "Epoch: 6144 \tTraining Loss: 0.037761\n",
      "Epoch: 6145 \tTraining Loss: 0.037754\n",
      "Epoch: 6146 \tTraining Loss: 0.037772\n",
      "Epoch: 6147 \tTraining Loss: 0.037767\n",
      "Epoch: 6148 \tTraining Loss: 0.037770\n",
      "Epoch: 6149 \tTraining Loss: 0.037778\n",
      "Epoch: 6150 \tTraining Loss: 0.037733\n",
      "Epoch: 6151 \tTraining Loss: 0.037762\n",
      "Epoch: 6152 \tTraining Loss: 0.037737\n",
      "Epoch: 6153 \tTraining Loss: 0.037735\n",
      "Epoch: 6154 \tTraining Loss: 0.037797\n",
      "Epoch: 6155 \tTraining Loss: 0.037743\n",
      "Epoch: 6156 \tTraining Loss: 0.037710\n",
      "Epoch: 6157 \tTraining Loss: 0.037774\n",
      "Epoch: 6158 \tTraining Loss: 0.037752\n",
      "Epoch: 6159 \tTraining Loss: 0.037764\n",
      "Epoch: 6160 \tTraining Loss: 0.037744\n",
      "Epoch: 6161 \tTraining Loss: 0.037725\n",
      "Epoch: 6162 \tTraining Loss: 0.037756\n",
      "Epoch: 6163 \tTraining Loss: 0.037704\n",
      "Epoch: 6164 \tTraining Loss: 0.037846\n",
      "Epoch: 6165 \tTraining Loss: 0.037716\n",
      "Epoch: 6166 \tTraining Loss: 0.037760\n",
      "Epoch: 6167 \tTraining Loss: 0.037742\n",
      "Epoch: 6168 \tTraining Loss: 0.037820\n",
      "Epoch: 6169 \tTraining Loss: 0.037721\n",
      "Epoch: 6170 \tTraining Loss: 0.037724\n",
      "Epoch: 6171 \tTraining Loss: 0.037711\n",
      "Epoch: 6172 \tTraining Loss: 0.037739\n",
      "Epoch: 6173 \tTraining Loss: 0.037726\n",
      "Epoch: 6174 \tTraining Loss: 0.037701\n",
      "Epoch: 6175 \tTraining Loss: 0.037757\n",
      "Epoch: 6176 \tTraining Loss: 0.037734\n",
      "Epoch: 6177 \tTraining Loss: 0.037747\n",
      "Epoch: 6178 \tTraining Loss: 0.037728\n",
      "Epoch: 6179 \tTraining Loss: 0.037783\n",
      "Epoch: 6180 \tTraining Loss: 0.037736\n",
      "Epoch: 6181 \tTraining Loss: 0.037755\n",
      "Epoch: 6182 \tTraining Loss: 0.037725\n",
      "Epoch: 6183 \tTraining Loss: 0.037766\n",
      "Epoch: 6184 \tTraining Loss: 0.037748\n",
      "Epoch: 6185 \tTraining Loss: 0.037746\n",
      "Epoch: 6186 \tTraining Loss: 0.037740\n",
      "Epoch: 6187 \tTraining Loss: 0.037757\n",
      "Epoch: 6188 \tTraining Loss: 0.037775\n",
      "Epoch: 6189 \tTraining Loss: 0.037746\n",
      "Epoch: 6190 \tTraining Loss: 0.037762\n",
      "Epoch: 6191 \tTraining Loss: 0.037738\n",
      "Epoch: 6192 \tTraining Loss: 0.037757\n",
      "Epoch: 6193 \tTraining Loss: 0.037738\n",
      "Epoch: 6194 \tTraining Loss: 0.037732\n",
      "Epoch: 6195 \tTraining Loss: 0.037744\n",
      "Epoch: 6196 \tTraining Loss: 0.037728\n",
      "Epoch: 6197 \tTraining Loss: 0.037706\n",
      "Epoch: 6198 \tTraining Loss: 0.037749\n",
      "Epoch: 6199 \tTraining Loss: 0.037741\n",
      "Epoch: 6200 \tTraining Loss: 0.037735\n",
      "Epoch: 6201 \tTraining Loss: 0.037775\n",
      "Epoch: 6202 \tTraining Loss: 0.037761\n",
      "Epoch: 6203 \tTraining Loss: 0.037756\n",
      "Epoch: 6204 \tTraining Loss: 0.037770\n",
      "Epoch: 6205 \tTraining Loss: 0.037726\n",
      "Epoch: 6206 \tTraining Loss: 0.037734\n",
      "Epoch: 6207 \tTraining Loss: 0.037715\n",
      "Epoch: 6208 \tTraining Loss: 0.037741\n",
      "Epoch: 6209 \tTraining Loss: 0.037725\n",
      "Epoch: 6210 \tTraining Loss: 0.037760\n",
      "Epoch: 6211 \tTraining Loss: 0.037738\n",
      "Epoch: 6212 \tTraining Loss: 0.037713\n",
      "Epoch: 6213 \tTraining Loss: 0.037948\n",
      "Epoch: 6214 \tTraining Loss: 0.037730\n",
      "Epoch: 6215 \tTraining Loss: 0.037727\n",
      "Epoch: 6216 \tTraining Loss: 0.037777\n",
      "Epoch: 6217 \tTraining Loss: 0.037756\n",
      "Epoch: 6218 \tTraining Loss: 0.037724\n",
      "Epoch: 6219 \tTraining Loss: 0.037754\n",
      "Epoch: 6220 \tTraining Loss: 0.037729\n",
      "Epoch: 6221 \tTraining Loss: 0.037777\n",
      "Epoch: 6222 \tTraining Loss: 0.037757\n",
      "Epoch: 6223 \tTraining Loss: 0.037816\n",
      "Epoch: 6224 \tTraining Loss: 0.037805\n",
      "Epoch: 6225 \tTraining Loss: 0.037706\n",
      "Epoch: 6226 \tTraining Loss: 0.037742\n",
      "Epoch: 6227 \tTraining Loss: 0.037810\n",
      "Epoch: 6228 \tTraining Loss: 0.037851\n",
      "Epoch: 6229 \tTraining Loss: 0.037732\n",
      "Epoch: 6230 \tTraining Loss: 0.037724\n",
      "Epoch: 6231 \tTraining Loss: 0.037768\n",
      "Epoch: 6232 \tTraining Loss: 0.037826\n",
      "Epoch: 6233 \tTraining Loss: 0.053425\n",
      "Epoch: 6234 \tTraining Loss: 0.142673\n",
      "Epoch: 6235 \tTraining Loss: 0.309466\n",
      "Epoch: 6236 \tTraining Loss: 0.200746\n",
      "Epoch: 6237 \tTraining Loss: 0.080523\n",
      "Epoch: 6238 \tTraining Loss: 0.068373\n",
      "Epoch: 6239 \tTraining Loss: 0.043923\n",
      "Epoch: 6240 \tTraining Loss: 0.043930\n",
      "Epoch: 6241 \tTraining Loss: 0.041856\n",
      "Epoch: 6242 \tTraining Loss: 0.041745\n",
      "Epoch: 6243 \tTraining Loss: 0.041642\n",
      "Epoch: 6244 \tTraining Loss: 0.041595\n",
      "Epoch: 6245 \tTraining Loss: 0.041571\n",
      "Epoch: 6246 \tTraining Loss: 0.041541\n",
      "Epoch: 6247 \tTraining Loss: 0.041539\n",
      "Epoch: 6248 \tTraining Loss: 0.041502\n",
      "Epoch: 6249 \tTraining Loss: 0.041488\n",
      "Epoch: 6250 \tTraining Loss: 0.041506\n",
      "Epoch: 6251 \tTraining Loss: 0.041505\n",
      "Epoch: 6252 \tTraining Loss: 0.041470\n",
      "Epoch: 6253 \tTraining Loss: 0.041464\n",
      "Epoch: 6254 \tTraining Loss: 0.041513\n",
      "Epoch: 6255 \tTraining Loss: 0.041472\n",
      "Epoch: 6256 \tTraining Loss: 0.041464\n",
      "Epoch: 6257 \tTraining Loss: 0.041454\n",
      "Epoch: 6258 \tTraining Loss: 0.041481\n",
      "Epoch: 6259 \tTraining Loss: 0.041472\n",
      "Epoch: 6260 \tTraining Loss: 0.041513\n",
      "Epoch: 6261 \tTraining Loss: 0.041488\n",
      "Epoch: 6262 \tTraining Loss: 0.042028\n",
      "Epoch: 6263 \tTraining Loss: 0.041543\n",
      "Epoch: 6264 \tTraining Loss: 0.041490\n",
      "Epoch: 6265 \tTraining Loss: 0.041558\n",
      "Epoch: 6266 \tTraining Loss: 0.041477\n",
      "Epoch: 6267 \tTraining Loss: 0.041484\n",
      "Epoch: 6268 \tTraining Loss: 0.041520\n",
      "Epoch: 6269 \tTraining Loss: 0.041468\n",
      "Epoch: 6270 \tTraining Loss: 0.041528\n",
      "Epoch: 6271 \tTraining Loss: 0.041472\n",
      "Epoch: 6272 \tTraining Loss: 0.041473\n",
      "Epoch: 6273 \tTraining Loss: 0.041453\n",
      "Epoch: 6274 \tTraining Loss: 0.041471\n",
      "Epoch: 6275 \tTraining Loss: 0.041469\n",
      "Epoch: 6276 \tTraining Loss: 0.041481\n",
      "Epoch: 6277 \tTraining Loss: 0.041468\n",
      "Epoch: 6278 \tTraining Loss: 0.041501\n",
      "Epoch: 6279 \tTraining Loss: 0.041464\n",
      "Epoch: 6280 \tTraining Loss: 0.041459\n",
      "Epoch: 6281 \tTraining Loss: 0.041442\n",
      "Epoch: 6282 \tTraining Loss: 0.041519\n",
      "Epoch: 6283 \tTraining Loss: 0.041456\n",
      "Epoch: 6284 \tTraining Loss: 0.041472\n",
      "Epoch: 6285 \tTraining Loss: 0.041445\n",
      "Epoch: 6286 \tTraining Loss: 0.041479\n",
      "Epoch: 6287 \tTraining Loss: 0.040334\n",
      "Epoch: 6288 \tTraining Loss: 0.040350\n",
      "Epoch: 6289 \tTraining Loss: 0.040369\n",
      "Epoch: 6290 \tTraining Loss: 0.040319\n",
      "Epoch: 6291 \tTraining Loss: 0.040280\n",
      "Epoch: 6292 \tTraining Loss: 0.040297\n",
      "Epoch: 6293 \tTraining Loss: 0.040268\n",
      "Epoch: 6294 \tTraining Loss: 0.040316\n",
      "Epoch: 6295 \tTraining Loss: 0.040274\n",
      "Epoch: 6296 \tTraining Loss: 0.040284\n",
      "Epoch: 6297 \tTraining Loss: 0.040253\n",
      "Epoch: 6298 \tTraining Loss: 0.040305\n",
      "Epoch: 6299 \tTraining Loss: 0.040276\n",
      "Epoch: 6300 \tTraining Loss: 0.040324\n",
      "Epoch: 6301 \tTraining Loss: 0.040264\n",
      "Epoch: 6302 \tTraining Loss: 0.040315\n",
      "Epoch: 6303 \tTraining Loss: 0.040265\n",
      "Epoch: 6304 \tTraining Loss: 0.040267\n",
      "Epoch: 6305 \tTraining Loss: 0.040313\n",
      "Epoch: 6306 \tTraining Loss: 0.040256\n",
      "Epoch: 6307 \tTraining Loss: 0.040300\n",
      "Epoch: 6308 \tTraining Loss: 0.040312\n",
      "Epoch: 6309 \tTraining Loss: 0.040250\n",
      "Epoch: 6310 \tTraining Loss: 0.040374\n",
      "Epoch: 6311 \tTraining Loss: 0.040262\n",
      "Epoch: 6312 \tTraining Loss: 0.040290\n",
      "Epoch: 6313 \tTraining Loss: 0.040253\n",
      "Epoch: 6314 \tTraining Loss: 0.040273\n",
      "Epoch: 6315 \tTraining Loss: 0.040262\n",
      "Epoch: 6316 \tTraining Loss: 0.040359\n",
      "Epoch: 6317 \tTraining Loss: 0.040260\n",
      "Epoch: 6318 \tTraining Loss: 0.040260\n",
      "Epoch: 6319 \tTraining Loss: 0.040276\n",
      "Epoch: 6320 \tTraining Loss: 0.040336\n",
      "Epoch: 6321 \tTraining Loss: 0.040275\n",
      "Epoch: 6322 \tTraining Loss: 0.040231\n",
      "Epoch: 6323 \tTraining Loss: 0.040233\n",
      "Epoch: 6324 \tTraining Loss: 0.040247\n",
      "Epoch: 6325 \tTraining Loss: 0.040260\n",
      "Epoch: 6326 \tTraining Loss: 0.040237\n",
      "Epoch: 6327 \tTraining Loss: 0.040292\n",
      "Epoch: 6328 \tTraining Loss: 0.040252\n",
      "Epoch: 6329 \tTraining Loss: 0.040276\n",
      "Epoch: 6330 \tTraining Loss: 0.040304\n",
      "Epoch: 6331 \tTraining Loss: 0.040286\n",
      "Epoch: 6332 \tTraining Loss: 0.040251\n",
      "Epoch: 6333 \tTraining Loss: 0.040270\n",
      "Epoch: 6334 \tTraining Loss: 0.040241\n",
      "Epoch: 6335 \tTraining Loss: 0.040247\n",
      "Epoch: 6336 \tTraining Loss: 0.040236\n",
      "Epoch: 6337 \tTraining Loss: 0.040270\n",
      "Epoch: 6338 \tTraining Loss: 0.040238\n",
      "Epoch: 6339 \tTraining Loss: 0.040302\n",
      "Epoch: 6340 \tTraining Loss: 0.040312\n",
      "Epoch: 6341 \tTraining Loss: 0.040256\n",
      "Epoch: 6342 \tTraining Loss: 0.040254\n",
      "Epoch: 6343 \tTraining Loss: 0.040273\n",
      "Epoch: 6344 \tTraining Loss: 0.040259\n",
      "Epoch: 6345 \tTraining Loss: 0.040268\n",
      "Epoch: 6346 \tTraining Loss: 0.040260\n",
      "Epoch: 6347 \tTraining Loss: 0.040239\n",
      "Epoch: 6348 \tTraining Loss: 0.040230\n",
      "Epoch: 6349 \tTraining Loss: 0.040272\n",
      "Epoch: 6350 \tTraining Loss: 0.040378\n",
      "Epoch: 6351 \tTraining Loss: 0.040285\n",
      "Epoch: 6352 \tTraining Loss: 0.040334\n",
      "Epoch: 6353 \tTraining Loss: 0.040286\n",
      "Epoch: 6354 \tTraining Loss: 0.040257\n",
      "Epoch: 6355 \tTraining Loss: 0.040307\n",
      "Epoch: 6356 \tTraining Loss: 0.040237\n",
      "Epoch: 6357 \tTraining Loss: 0.040224\n",
      "Epoch: 6358 \tTraining Loss: 0.040295\n",
      "Epoch: 6359 \tTraining Loss: 0.040231\n",
      "Epoch: 6360 \tTraining Loss: 0.040207\n",
      "Epoch: 6361 \tTraining Loss: 0.040243\n",
      "Epoch: 6362 \tTraining Loss: 0.040284\n",
      "Epoch: 6363 \tTraining Loss: 0.040227\n",
      "Epoch: 6364 \tTraining Loss: 0.040235\n",
      "Epoch: 6365 \tTraining Loss: 0.040243\n",
      "Epoch: 6366 \tTraining Loss: 0.040257\n",
      "Epoch: 6367 \tTraining Loss: 0.040242\n",
      "Epoch: 6368 \tTraining Loss: 0.040259\n",
      "Epoch: 6369 \tTraining Loss: 0.040211\n",
      "Epoch: 6370 \tTraining Loss: 0.040312\n",
      "Epoch: 6371 \tTraining Loss: 0.040246\n",
      "Epoch: 6372 \tTraining Loss: 0.040249\n",
      "Epoch: 6373 \tTraining Loss: 0.040222\n",
      "Epoch: 6374 \tTraining Loss: 0.040255\n",
      "Epoch: 6375 \tTraining Loss: 0.040256\n",
      "Epoch: 6376 \tTraining Loss: 0.040217\n",
      "Epoch: 6377 \tTraining Loss: 0.040249\n",
      "Epoch: 6378 \tTraining Loss: 0.040214\n",
      "Epoch: 6379 \tTraining Loss: 0.040287\n",
      "Epoch: 6380 \tTraining Loss: 0.040222\n",
      "Epoch: 6381 \tTraining Loss: 0.040208\n",
      "Epoch: 6382 \tTraining Loss: 0.040240\n",
      "Epoch: 6383 \tTraining Loss: 0.040232\n",
      "Epoch: 6384 \tTraining Loss: 0.040232\n",
      "Epoch: 6385 \tTraining Loss: 0.040219\n",
      "Epoch: 6386 \tTraining Loss: 0.040220\n",
      "Epoch: 6387 \tTraining Loss: 0.040237\n",
      "Epoch: 6388 \tTraining Loss: 0.040276\n",
      "Epoch: 6389 \tTraining Loss: 0.040201\n",
      "Epoch: 6390 \tTraining Loss: 0.040239\n",
      "Epoch: 6391 \tTraining Loss: 0.040239\n",
      "Epoch: 6392 \tTraining Loss: 0.040237\n",
      "Epoch: 6393 \tTraining Loss: 0.040218\n",
      "Epoch: 6394 \tTraining Loss: 0.040217\n",
      "Epoch: 6395 \tTraining Loss: 0.040300\n",
      "Epoch: 6396 \tTraining Loss: 0.040225\n",
      "Epoch: 6397 \tTraining Loss: 0.040269\n",
      "Epoch: 6398 \tTraining Loss: 0.040245\n",
      "Epoch: 6399 \tTraining Loss: 0.040266\n",
      "Epoch: 6400 \tTraining Loss: 0.040279\n",
      "Epoch: 6401 \tTraining Loss: 0.040294\n",
      "Epoch: 6402 \tTraining Loss: 0.040245\n",
      "Epoch: 6403 \tTraining Loss: 0.040232\n",
      "Epoch: 6404 \tTraining Loss: 0.040244\n",
      "Epoch: 6405 \tTraining Loss: 0.040217\n",
      "Epoch: 6406 \tTraining Loss: 0.040269\n",
      "Epoch: 6407 \tTraining Loss: 0.040225\n",
      "Epoch: 6408 \tTraining Loss: 0.040231\n",
      "Epoch: 6409 \tTraining Loss: 0.040238\n",
      "Epoch: 6410 \tTraining Loss: 0.040225\n",
      "Epoch: 6411 \tTraining Loss: 0.040253\n",
      "Epoch: 6412 \tTraining Loss: 0.040220\n",
      "Epoch: 6413 \tTraining Loss: 0.040181\n",
      "Epoch: 6414 \tTraining Loss: 0.040212\n",
      "Epoch: 6415 \tTraining Loss: 0.040269\n",
      "Epoch: 6416 \tTraining Loss: 0.040210\n",
      "Epoch: 6417 \tTraining Loss: 0.040214\n",
      "Epoch: 6418 \tTraining Loss: 0.040496\n",
      "Epoch: 6419 \tTraining Loss: 0.040300\n",
      "Epoch: 6420 \tTraining Loss: 0.040229\n",
      "Epoch: 6421 \tTraining Loss: 0.040227\n",
      "Epoch: 6422 \tTraining Loss: 0.040202\n",
      "Epoch: 6423 \tTraining Loss: 0.040260\n",
      "Epoch: 6424 \tTraining Loss: 0.040325\n",
      "Epoch: 6425 \tTraining Loss: 0.040216\n",
      "Epoch: 6426 \tTraining Loss: 0.040224\n",
      "Epoch: 6427 \tTraining Loss: 0.040226\n",
      "Epoch: 6428 \tTraining Loss: 0.040204\n",
      "Epoch: 6429 \tTraining Loss: 0.040335\n",
      "Epoch: 6430 \tTraining Loss: 0.040209\n",
      "Epoch: 6431 \tTraining Loss: 0.040253\n",
      "Epoch: 6432 \tTraining Loss: 0.040212\n",
      "Epoch: 6433 \tTraining Loss: 0.040225\n",
      "Epoch: 6434 \tTraining Loss: 0.040230\n",
      "Epoch: 6435 \tTraining Loss: 0.040264\n",
      "Epoch: 6436 \tTraining Loss: 0.040200\n",
      "Epoch: 6437 \tTraining Loss: 0.040248\n",
      "Epoch: 6438 \tTraining Loss: 0.040233\n",
      "Epoch: 6439 \tTraining Loss: 0.040197\n",
      "Epoch: 6440 \tTraining Loss: 0.040216\n",
      "Epoch: 6441 \tTraining Loss: 0.040213\n",
      "Epoch: 6442 \tTraining Loss: 0.040296\n",
      "Epoch: 6443 \tTraining Loss: 0.040263\n",
      "Epoch: 6444 \tTraining Loss: 0.040275\n",
      "Epoch: 6445 \tTraining Loss: 0.040206\n",
      "Epoch: 6446 \tTraining Loss: 0.040297\n",
      "Epoch: 6447 \tTraining Loss: 0.040186\n",
      "Epoch: 6448 \tTraining Loss: 0.040319\n",
      "Epoch: 6449 \tTraining Loss: 0.040206\n",
      "Epoch: 6450 \tTraining Loss: 0.040325\n",
      "Epoch: 6451 \tTraining Loss: 0.040227\n",
      "Epoch: 6452 \tTraining Loss: 0.040231\n",
      "Epoch: 6453 \tTraining Loss: 0.095062\n",
      "Epoch: 6454 \tTraining Loss: 0.086841\n",
      "Epoch: 6455 \tTraining Loss: 0.157259\n",
      "Epoch: 6456 \tTraining Loss: 0.188987\n",
      "Epoch: 6457 \tTraining Loss: 0.097384\n",
      "Epoch: 6458 \tTraining Loss: 0.081040\n",
      "Epoch: 6459 \tTraining Loss: 0.044984\n",
      "Epoch: 6460 \tTraining Loss: 0.043250\n",
      "Epoch: 6461 \tTraining Loss: 0.041214\n",
      "Epoch: 6462 \tTraining Loss: 0.040712\n",
      "Epoch: 6463 \tTraining Loss: 0.040691\n",
      "Epoch: 6464 \tTraining Loss: 0.040607\n",
      "Epoch: 6465 \tTraining Loss: 0.040582\n",
      "Epoch: 6466 \tTraining Loss: 0.040587\n",
      "Epoch: 6467 \tTraining Loss: 0.040547\n",
      "Epoch: 6468 \tTraining Loss: 0.040508\n",
      "Epoch: 6469 \tTraining Loss: 0.040531\n",
      "Epoch: 6470 \tTraining Loss: 0.040531\n",
      "Epoch: 6471 \tTraining Loss: 0.040476\n",
      "Epoch: 6472 \tTraining Loss: 0.040491\n",
      "Epoch: 6473 \tTraining Loss: 0.040454\n",
      "Epoch: 6474 \tTraining Loss: 0.040457\n",
      "Epoch: 6475 \tTraining Loss: 0.040459\n",
      "Epoch: 6476 \tTraining Loss: 0.040449\n",
      "Epoch: 6477 \tTraining Loss: 0.040454\n",
      "Epoch: 6478 \tTraining Loss: 0.040467\n",
      "Epoch: 6479 \tTraining Loss: 0.040435\n",
      "Epoch: 6480 \tTraining Loss: 0.040479\n",
      "Epoch: 6481 \tTraining Loss: 0.040400\n",
      "Epoch: 6482 \tTraining Loss: 0.040411\n",
      "Epoch: 6483 \tTraining Loss: 0.040442\n",
      "Epoch: 6484 \tTraining Loss: 0.040405\n",
      "Epoch: 6485 \tTraining Loss: 0.040391\n",
      "Epoch: 6486 \tTraining Loss: 0.040391\n",
      "Epoch: 6487 \tTraining Loss: 0.040364\n",
      "Epoch: 6488 \tTraining Loss: 0.040430\n",
      "Epoch: 6489 \tTraining Loss: 0.040478\n",
      "Epoch: 6490 \tTraining Loss: 0.040381\n",
      "Epoch: 6491 \tTraining Loss: 0.040363\n",
      "Epoch: 6492 \tTraining Loss: 0.040381\n",
      "Epoch: 6493 \tTraining Loss: 0.040383\n",
      "Epoch: 6494 \tTraining Loss: 0.040366\n",
      "Epoch: 6495 \tTraining Loss: 0.040368\n",
      "Epoch: 6496 \tTraining Loss: 0.040372\n",
      "Epoch: 6497 \tTraining Loss: 0.040365\n",
      "Epoch: 6498 \tTraining Loss: 0.040410\n",
      "Epoch: 6499 \tTraining Loss: 0.040427\n",
      "Epoch: 6500 \tTraining Loss: 0.040345\n",
      "Epoch: 6501 \tTraining Loss: 0.040333\n",
      "Epoch: 6502 \tTraining Loss: 0.040325\n",
      "Epoch: 6503 \tTraining Loss: 0.040317\n",
      "Epoch: 6504 \tTraining Loss: 0.040359\n",
      "Epoch: 6505 \tTraining Loss: 0.040356\n",
      "Epoch: 6506 \tTraining Loss: 0.040338\n",
      "Epoch: 6507 \tTraining Loss: 0.040393\n",
      "Epoch: 6508 \tTraining Loss: 0.040318\n",
      "Epoch: 6509 \tTraining Loss: 0.040365\n",
      "Epoch: 6510 \tTraining Loss: 0.040350\n",
      "Epoch: 6511 \tTraining Loss: 0.040365\n",
      "Epoch: 6512 \tTraining Loss: 0.040568\n",
      "Epoch: 6513 \tTraining Loss: 0.040350\n",
      "Epoch: 6514 \tTraining Loss: 0.040367\n",
      "Epoch: 6515 \tTraining Loss: 0.040310\n",
      "Epoch: 6516 \tTraining Loss: 0.040327\n",
      "Epoch: 6517 \tTraining Loss: 0.040339\n",
      "Epoch: 6518 \tTraining Loss: 0.040306\n",
      "Epoch: 6519 \tTraining Loss: 0.040333\n",
      "Epoch: 6520 \tTraining Loss: 0.040303\n",
      "Epoch: 6521 \tTraining Loss: 0.040323\n",
      "Epoch: 6522 \tTraining Loss: 0.040342\n",
      "Epoch: 6523 \tTraining Loss: 0.040365\n",
      "Epoch: 6524 \tTraining Loss: 0.040289\n",
      "Epoch: 6525 \tTraining Loss: 0.040273\n",
      "Epoch: 6526 \tTraining Loss: 0.040373\n",
      "Epoch: 6527 \tTraining Loss: 0.040321\n",
      "Epoch: 6528 \tTraining Loss: 0.040298\n",
      "Epoch: 6529 \tTraining Loss: 0.040296\n",
      "Epoch: 6530 \tTraining Loss: 0.040266\n",
      "Epoch: 6531 \tTraining Loss: 0.040280\n",
      "Epoch: 6532 \tTraining Loss: 0.040288\n",
      "Epoch: 6533 \tTraining Loss: 0.040302\n",
      "Epoch: 6534 \tTraining Loss: 0.040260\n",
      "Epoch: 6535 \tTraining Loss: 0.040268\n",
      "Epoch: 6536 \tTraining Loss: 0.040314\n",
      "Epoch: 6537 \tTraining Loss: 0.040315\n",
      "Epoch: 6538 \tTraining Loss: 0.040310\n",
      "Epoch: 6539 \tTraining Loss: 0.040263\n",
      "Epoch: 6540 \tTraining Loss: 0.040305\n",
      "Epoch: 6541 \tTraining Loss: 0.040262\n",
      "Epoch: 6542 \tTraining Loss: 0.040293\n",
      "Epoch: 6543 \tTraining Loss: 0.040266\n",
      "Epoch: 6544 \tTraining Loss: 0.040270\n",
      "Epoch: 6545 \tTraining Loss: 0.040257\n",
      "Epoch: 6546 \tTraining Loss: 0.040265\n",
      "Epoch: 6547 \tTraining Loss: 0.040285\n",
      "Epoch: 6548 \tTraining Loss: 0.040276\n",
      "Epoch: 6549 \tTraining Loss: 0.040248\n",
      "Epoch: 6550 \tTraining Loss: 0.040282\n",
      "Epoch: 6551 \tTraining Loss: 0.040247\n",
      "Epoch: 6552 \tTraining Loss: 0.040371\n",
      "Epoch: 6553 \tTraining Loss: 0.040235\n",
      "Epoch: 6554 \tTraining Loss: 0.040236\n",
      "Epoch: 6555 \tTraining Loss: 0.040302\n",
      "Epoch: 6556 \tTraining Loss: 0.040232\n",
      "Epoch: 6557 \tTraining Loss: 0.040315\n",
      "Epoch: 6558 \tTraining Loss: 0.040252\n",
      "Epoch: 6559 \tTraining Loss: 0.040261\n",
      "Epoch: 6560 \tTraining Loss: 0.040379\n",
      "Epoch: 6561 \tTraining Loss: 0.040529\n",
      "Epoch: 6562 \tTraining Loss: 0.040172\n",
      "Epoch: 6563 \tTraining Loss: 0.040202\n",
      "Epoch: 6564 \tTraining Loss: 0.040270\n",
      "Epoch: 6565 \tTraining Loss: 0.040341\n",
      "Epoch: 6566 \tTraining Loss: 0.040283\n",
      "Epoch: 6567 \tTraining Loss: 0.040252\n",
      "Epoch: 6568 \tTraining Loss: 0.040274\n",
      "Epoch: 6569 \tTraining Loss: 0.040303\n",
      "Epoch: 6570 \tTraining Loss: 0.040342\n",
      "Epoch: 6571 \tTraining Loss: 0.040243\n",
      "Epoch: 6572 \tTraining Loss: 0.040270\n",
      "Epoch: 6573 \tTraining Loss: 0.040239\n",
      "Epoch: 6574 \tTraining Loss: 0.040266\n",
      "Epoch: 6575 \tTraining Loss: 0.040237\n",
      "Epoch: 6576 \tTraining Loss: 0.040237\n",
      "Epoch: 6577 \tTraining Loss: 0.040261\n",
      "Epoch: 6578 \tTraining Loss: 0.040239\n",
      "Epoch: 6579 \tTraining Loss: 0.040266\n",
      "Epoch: 6580 \tTraining Loss: 0.040240\n",
      "Epoch: 6581 \tTraining Loss: 0.040259\n",
      "Epoch: 6582 \tTraining Loss: 0.040305\n",
      "Epoch: 6583 \tTraining Loss: 0.040305\n",
      "Epoch: 6584 \tTraining Loss: 0.040234\n",
      "Epoch: 6585 \tTraining Loss: 0.040273\n",
      "Epoch: 6586 \tTraining Loss: 0.040247\n",
      "Epoch: 6587 \tTraining Loss: 0.040264\n",
      "Epoch: 6588 \tTraining Loss: 0.040222\n",
      "Epoch: 6589 \tTraining Loss: 0.040253\n",
      "Epoch: 6590 \tTraining Loss: 0.040218\n",
      "Epoch: 6591 \tTraining Loss: 0.040250\n",
      "Epoch: 6592 \tTraining Loss: 0.040235\n",
      "Epoch: 6593 \tTraining Loss: 0.040212\n",
      "Epoch: 6594 \tTraining Loss: 0.040258\n",
      "Epoch: 6595 \tTraining Loss: 0.040289\n",
      "Epoch: 6596 \tTraining Loss: 0.040238\n",
      "Epoch: 6597 \tTraining Loss: 0.040247\n",
      "Epoch: 6598 \tTraining Loss: 0.040238\n",
      "Epoch: 6599 \tTraining Loss: 0.040248\n",
      "Epoch: 6600 \tTraining Loss: 0.040211\n",
      "Epoch: 6601 \tTraining Loss: 0.040241\n",
      "Epoch: 6602 \tTraining Loss: 0.040230\n",
      "Epoch: 6603 \tTraining Loss: 0.040254\n",
      "Epoch: 6604 \tTraining Loss: 0.040309\n",
      "Epoch: 6605 \tTraining Loss: 0.040276\n",
      "Epoch: 6606 \tTraining Loss: 0.040255\n",
      "Epoch: 6607 \tTraining Loss: 0.040211\n",
      "Epoch: 6608 \tTraining Loss: 0.040253\n",
      "Epoch: 6609 \tTraining Loss: 0.040226\n",
      "Epoch: 6610 \tTraining Loss: 0.040243\n",
      "Epoch: 6611 \tTraining Loss: 0.040238\n",
      "Epoch: 6612 \tTraining Loss: 0.040244\n",
      "Epoch: 6613 \tTraining Loss: 0.040313\n",
      "Epoch: 6614 \tTraining Loss: 0.071354\n",
      "Epoch: 6615 \tTraining Loss: 0.050145\n",
      "Epoch: 6616 \tTraining Loss: 0.049513\n",
      "Epoch: 6617 \tTraining Loss: 0.048289\n",
      "Epoch: 6618 \tTraining Loss: 0.047994\n",
      "Epoch: 6619 \tTraining Loss: 0.047850\n",
      "Epoch: 6620 \tTraining Loss: 0.047739\n",
      "Epoch: 6621 \tTraining Loss: 0.047679\n",
      "Epoch: 6622 \tTraining Loss: 0.047677\n",
      "Epoch: 6623 \tTraining Loss: 0.047574\n",
      "Epoch: 6624 \tTraining Loss: 0.047551\n",
      "Epoch: 6625 \tTraining Loss: 0.047510\n",
      "Epoch: 6626 \tTraining Loss: 0.047474\n",
      "Epoch: 6627 \tTraining Loss: 0.047505\n",
      "Epoch: 6628 \tTraining Loss: 0.047616\n",
      "Epoch: 6629 \tTraining Loss: 0.047486\n",
      "Epoch: 6630 \tTraining Loss: 0.047485\n",
      "Epoch: 6631 \tTraining Loss: 0.047501\n",
      "Epoch: 6632 \tTraining Loss: 0.047562\n",
      "Epoch: 6633 \tTraining Loss: 0.047693\n",
      "Epoch: 6634 \tTraining Loss: 0.047531\n",
      "Epoch: 6635 \tTraining Loss: 0.047486\n",
      "Epoch: 6636 \tTraining Loss: 0.047518\n",
      "Epoch: 6637 \tTraining Loss: 0.047487\n",
      "Epoch: 6638 \tTraining Loss: 0.047491\n",
      "Epoch: 6639 \tTraining Loss: 0.047466\n",
      "Epoch: 6640 \tTraining Loss: 0.047519\n",
      "Epoch: 6641 \tTraining Loss: 0.047526\n",
      "Epoch: 6642 \tTraining Loss: 0.047540\n",
      "Epoch: 6643 \tTraining Loss: 0.047469\n",
      "Epoch: 6644 \tTraining Loss: 0.047587\n",
      "Epoch: 6645 \tTraining Loss: 0.047463\n",
      "Epoch: 6646 \tTraining Loss: 0.047486\n",
      "Epoch: 6647 \tTraining Loss: 0.047505\n",
      "Epoch: 6648 \tTraining Loss: 0.047509\n",
      "Epoch: 6649 \tTraining Loss: 0.047528\n",
      "Epoch: 6650 \tTraining Loss: 0.047467\n",
      "Epoch: 6651 \tTraining Loss: 0.047540\n",
      "Epoch: 6652 \tTraining Loss: 0.047451\n",
      "Epoch: 6653 \tTraining Loss: 0.047497\n",
      "Epoch: 6654 \tTraining Loss: 0.047467\n",
      "Epoch: 6655 \tTraining Loss: 0.047503\n",
      "Epoch: 6656 \tTraining Loss: 0.047472\n",
      "Epoch: 6657 \tTraining Loss: 0.047467\n",
      "Epoch: 6658 \tTraining Loss: 0.047462\n",
      "Epoch: 6659 \tTraining Loss: 0.044814\n",
      "Epoch: 6660 \tTraining Loss: 0.100881\n",
      "Epoch: 6661 \tTraining Loss: 0.058605\n",
      "Epoch: 6662 \tTraining Loss: 0.085061\n",
      "Epoch: 6663 \tTraining Loss: 0.107700\n",
      "Epoch: 6664 \tTraining Loss: 0.170872\n",
      "Epoch: 6665 \tTraining Loss: 0.195553\n",
      "Epoch: 6666 \tTraining Loss: 0.143963\n",
      "Epoch: 6667 \tTraining Loss: 0.069446\n",
      "Epoch: 6668 \tTraining Loss: 0.059197\n",
      "Epoch: 6669 \tTraining Loss: 0.057792\n",
      "Epoch: 6670 \tTraining Loss: 0.050463\n",
      "Epoch: 6671 \tTraining Loss: 0.046031\n",
      "Epoch: 6672 \tTraining Loss: 0.045648\n",
      "Epoch: 6673 \tTraining Loss: 0.045350\n",
      "Epoch: 6674 \tTraining Loss: 0.045355\n",
      "Epoch: 6675 \tTraining Loss: 0.045103\n",
      "Epoch: 6676 \tTraining Loss: 0.045136\n",
      "Epoch: 6677 \tTraining Loss: 0.045058\n",
      "Epoch: 6678 \tTraining Loss: 0.045043\n",
      "Epoch: 6679 \tTraining Loss: 0.045017\n",
      "Epoch: 6680 \tTraining Loss: 0.044990\n",
      "Epoch: 6681 \tTraining Loss: 0.044948\n",
      "Epoch: 6682 \tTraining Loss: 0.044983\n",
      "Epoch: 6683 \tTraining Loss: 0.044955\n",
      "Epoch: 6684 \tTraining Loss: 0.044996\n",
      "Epoch: 6685 \tTraining Loss: 0.044970\n",
      "Epoch: 6686 \tTraining Loss: 0.044982\n",
      "Epoch: 6687 \tTraining Loss: 0.044974\n",
      "Epoch: 6688 \tTraining Loss: 0.044931\n",
      "Epoch: 6689 \tTraining Loss: 0.044934\n",
      "Epoch: 6690 \tTraining Loss: 0.044958\n",
      "Epoch: 6691 \tTraining Loss: 0.044910\n",
      "Epoch: 6692 \tTraining Loss: 0.044912\n",
      "Epoch: 6693 \tTraining Loss: 0.044389\n",
      "Epoch: 6694 \tTraining Loss: 0.041158\n",
      "Epoch: 6695 \tTraining Loss: 0.041109\n",
      "Epoch: 6696 \tTraining Loss: 0.041032\n",
      "Epoch: 6697 \tTraining Loss: 0.041043\n",
      "Epoch: 6698 \tTraining Loss: 0.040950\n",
      "Epoch: 6699 \tTraining Loss: 0.040987\n",
      "Epoch: 6700 \tTraining Loss: 0.040910\n",
      "Epoch: 6701 \tTraining Loss: 0.040938\n",
      "Epoch: 6702 \tTraining Loss: 0.040381\n",
      "Epoch: 6703 \tTraining Loss: 0.038964\n",
      "Epoch: 6704 \tTraining Loss: 0.038954\n",
      "Epoch: 6705 \tTraining Loss: 0.038877\n",
      "Epoch: 6706 \tTraining Loss: 0.038863\n",
      "Epoch: 6707 \tTraining Loss: 0.038855\n",
      "Epoch: 6708 \tTraining Loss: 0.038862\n",
      "Epoch: 6709 \tTraining Loss: 0.038853\n",
      "Epoch: 6710 \tTraining Loss: 0.038877\n",
      "Epoch: 6711 \tTraining Loss: 0.038802\n",
      "Epoch: 6712 \tTraining Loss: 0.038863\n",
      "Epoch: 6713 \tTraining Loss: 0.038811\n",
      "Epoch: 6714 \tTraining Loss: 0.038845\n",
      "Epoch: 6715 \tTraining Loss: 0.038853\n",
      "Epoch: 6716 \tTraining Loss: 0.038814\n",
      "Epoch: 6717 \tTraining Loss: 0.038843\n",
      "Epoch: 6718 \tTraining Loss: 0.038863\n",
      "Epoch: 6719 \tTraining Loss: 0.038828\n",
      "Epoch: 6720 \tTraining Loss: 0.038888\n",
      "Epoch: 6721 \tTraining Loss: 0.038854\n",
      "Epoch: 6722 \tTraining Loss: 0.038818\n",
      "Epoch: 6723 \tTraining Loss: 0.038852\n",
      "Epoch: 6724 \tTraining Loss: 0.038846\n",
      "Epoch: 6725 \tTraining Loss: 0.038803\n",
      "Epoch: 6726 \tTraining Loss: 0.038823\n",
      "Epoch: 6727 \tTraining Loss: 0.038895\n",
      "Epoch: 6728 \tTraining Loss: 0.038866\n",
      "Epoch: 6729 \tTraining Loss: 0.038815\n",
      "Epoch: 6730 \tTraining Loss: 0.038820\n",
      "Epoch: 6731 \tTraining Loss: 0.038881\n",
      "Epoch: 6732 \tTraining Loss: 0.038939\n",
      "Epoch: 6733 \tTraining Loss: 0.038800\n",
      "Epoch: 6734 \tTraining Loss: 0.038821\n",
      "Epoch: 6735 \tTraining Loss: 0.038953\n",
      "Epoch: 6736 \tTraining Loss: 0.038794\n",
      "Epoch: 6737 \tTraining Loss: 0.038799\n",
      "Epoch: 6738 \tTraining Loss: 0.038823\n",
      "Epoch: 6739 \tTraining Loss: 0.038809\n",
      "Epoch: 6740 \tTraining Loss: 0.038798\n",
      "Epoch: 6741 \tTraining Loss: 0.038792\n",
      "Epoch: 6742 \tTraining Loss: 0.038838\n",
      "Epoch: 6743 \tTraining Loss: 0.038831\n",
      "Epoch: 6744 \tTraining Loss: 0.038812\n",
      "Epoch: 6745 \tTraining Loss: 0.038786\n",
      "Epoch: 6746 \tTraining Loss: 0.038790\n",
      "Epoch: 6747 \tTraining Loss: 0.038858\n",
      "Epoch: 6748 \tTraining Loss: 0.038792\n",
      "Epoch: 6749 \tTraining Loss: 0.038792\n",
      "Epoch: 6750 \tTraining Loss: 0.038827\n",
      "Epoch: 6751 \tTraining Loss: 0.038777\n",
      "Epoch: 6752 \tTraining Loss: 0.038806\n",
      "Epoch: 6753 \tTraining Loss: 0.038940\n",
      "Epoch: 6754 \tTraining Loss: 0.038838\n",
      "Epoch: 6755 \tTraining Loss: 0.038784\n",
      "Epoch: 6756 \tTraining Loss: 0.038794\n",
      "Epoch: 6757 \tTraining Loss: 0.038807\n",
      "Epoch: 6758 \tTraining Loss: 0.038800\n",
      "Epoch: 6759 \tTraining Loss: 0.038800\n",
      "Epoch: 6760 \tTraining Loss: 0.038819\n",
      "Epoch: 6761 \tTraining Loss: 0.038850\n",
      "Epoch: 6762 \tTraining Loss: 0.038795\n",
      "Epoch: 6763 \tTraining Loss: 0.038820\n",
      "Epoch: 6764 \tTraining Loss: 0.038794\n",
      "Epoch: 6765 \tTraining Loss: 0.038815\n",
      "Epoch: 6766 \tTraining Loss: 0.038806\n",
      "Epoch: 6767 \tTraining Loss: 0.038791\n",
      "Epoch: 6768 \tTraining Loss: 0.038848\n",
      "Epoch: 6769 \tTraining Loss: 0.038804\n",
      "Epoch: 6770 \tTraining Loss: 0.038808\n",
      "Epoch: 6771 \tTraining Loss: 0.038762\n",
      "Epoch: 6772 \tTraining Loss: 0.046116\n",
      "Epoch: 6773 \tTraining Loss: 0.121588\n",
      "Epoch: 6774 \tTraining Loss: 0.063862\n",
      "Epoch: 6775 \tTraining Loss: 0.056355\n",
      "Epoch: 6776 \tTraining Loss: 0.072466\n",
      "Epoch: 6777 \tTraining Loss: 0.070302\n",
      "Epoch: 6778 \tTraining Loss: 0.103251\n",
      "Epoch: 6779 \tTraining Loss: 0.054473\n",
      "Epoch: 6780 \tTraining Loss: 0.055737\n",
      "Epoch: 6781 \tTraining Loss: 0.045245\n",
      "Epoch: 6782 \tTraining Loss: 0.044793\n",
      "Epoch: 6783 \tTraining Loss: 0.044664\n",
      "Epoch: 6784 \tTraining Loss: 0.044659\n",
      "Epoch: 6785 \tTraining Loss: 0.044529\n",
      "Epoch: 6786 \tTraining Loss: 0.044434\n",
      "Epoch: 6787 \tTraining Loss: 0.044406\n",
      "Epoch: 6788 \tTraining Loss: 0.044520\n",
      "Epoch: 6789 \tTraining Loss: 0.044453\n",
      "Epoch: 6790 \tTraining Loss: 0.044409\n",
      "Epoch: 6791 \tTraining Loss: 0.044512\n",
      "Epoch: 6792 \tTraining Loss: 0.044483\n",
      "Epoch: 6793 \tTraining Loss: 0.044416\n",
      "Epoch: 6794 \tTraining Loss: 0.044389\n",
      "Epoch: 6795 \tTraining Loss: 0.044358\n",
      "Epoch: 6796 \tTraining Loss: 0.044372\n",
      "Epoch: 6797 \tTraining Loss: 0.044358\n",
      "Epoch: 6798 \tTraining Loss: 0.044377\n",
      "Epoch: 6799 \tTraining Loss: 0.044361\n",
      "Epoch: 6800 \tTraining Loss: 0.044382\n",
      "Epoch: 6801 \tTraining Loss: 0.044313\n",
      "Epoch: 6802 \tTraining Loss: 0.044318\n",
      "Epoch: 6803 \tTraining Loss: 0.044291\n",
      "Epoch: 6804 \tTraining Loss: 0.044299\n",
      "Epoch: 6805 \tTraining Loss: 0.044427\n",
      "Epoch: 6806 \tTraining Loss: 0.044287\n",
      "Epoch: 6807 \tTraining Loss: 0.044275\n",
      "Epoch: 6808 \tTraining Loss: 0.044262\n",
      "Epoch: 6809 \tTraining Loss: 0.044258\n",
      "Epoch: 6810 \tTraining Loss: 0.044275\n",
      "Epoch: 6811 \tTraining Loss: 0.044236\n",
      "Epoch: 6812 \tTraining Loss: 0.044251\n",
      "Epoch: 6813 \tTraining Loss: 0.044273\n",
      "Epoch: 6814 \tTraining Loss: 0.044298\n",
      "Epoch: 6815 \tTraining Loss: 0.044248\n",
      "Epoch: 6816 \tTraining Loss: 0.044235\n",
      "Epoch: 6817 \tTraining Loss: 0.044251\n",
      "Epoch: 6818 \tTraining Loss: 0.044215\n",
      "Epoch: 6819 \tTraining Loss: 0.044254\n",
      "Epoch: 6820 \tTraining Loss: 0.044236\n",
      "Epoch: 6821 \tTraining Loss: 0.044211\n",
      "Epoch: 6822 \tTraining Loss: 0.044211\n",
      "Epoch: 6823 \tTraining Loss: 0.044265\n",
      "Epoch: 6824 \tTraining Loss: 0.044219\n",
      "Epoch: 6825 \tTraining Loss: 0.044206\n",
      "Epoch: 6826 \tTraining Loss: 0.044212\n",
      "Epoch: 6827 \tTraining Loss: 0.044194\n",
      "Epoch: 6828 \tTraining Loss: 0.044239\n",
      "Epoch: 6829 \tTraining Loss: 0.044214\n",
      "Epoch: 6830 \tTraining Loss: 0.044234\n",
      "Epoch: 6831 \tTraining Loss: 0.044192\n",
      "Epoch: 6832 \tTraining Loss: 0.044241\n",
      "Epoch: 6833 \tTraining Loss: 0.044202\n",
      "Epoch: 6834 \tTraining Loss: 0.044219\n",
      "Epoch: 6835 \tTraining Loss: 0.044167\n",
      "Epoch: 6836 \tTraining Loss: 0.044195\n",
      "Epoch: 6837 \tTraining Loss: 0.044212\n",
      "Epoch: 6838 \tTraining Loss: 0.044311\n",
      "Epoch: 6839 \tTraining Loss: 0.044169\n",
      "Epoch: 6840 \tTraining Loss: 0.044191\n",
      "Epoch: 6841 \tTraining Loss: 0.044187\n",
      "Epoch: 6842 \tTraining Loss: 0.044173\n",
      "Epoch: 6843 \tTraining Loss: 0.044174\n",
      "Epoch: 6844 \tTraining Loss: 0.043713\n",
      "Epoch: 6845 \tTraining Loss: 0.052394\n",
      "Epoch: 6846 \tTraining Loss: 0.083475\n",
      "Epoch: 6847 \tTraining Loss: 0.091663\n",
      "Epoch: 6848 \tTraining Loss: 0.067688\n",
      "Epoch: 6849 \tTraining Loss: 0.071719\n",
      "Epoch: 6850 \tTraining Loss: 0.044730\n",
      "Epoch: 6851 \tTraining Loss: 0.043616\n",
      "Epoch: 6852 \tTraining Loss: 0.042781\n",
      "Epoch: 6853 \tTraining Loss: 0.042899\n",
      "Epoch: 6854 \tTraining Loss: 0.042819\n",
      "Epoch: 6855 \tTraining Loss: 0.042782\n",
      "Epoch: 6856 \tTraining Loss: 0.042709\n",
      "Epoch: 6857 \tTraining Loss: 0.041056\n",
      "Epoch: 6858 \tTraining Loss: 0.041017\n",
      "Epoch: 6859 \tTraining Loss: 0.040933\n",
      "Epoch: 6860 \tTraining Loss: 0.040941\n",
      "Epoch: 6861 \tTraining Loss: 0.040926\n",
      "Epoch: 6862 \tTraining Loss: 0.040902\n",
      "Epoch: 6863 \tTraining Loss: 0.040895\n",
      "Epoch: 6864 \tTraining Loss: 0.040892\n",
      "Epoch: 6865 \tTraining Loss: 0.040889\n",
      "Epoch: 6866 \tTraining Loss: 0.040907\n",
      "Epoch: 6867 \tTraining Loss: 0.040919\n",
      "Epoch: 6868 \tTraining Loss: 0.040913\n",
      "Epoch: 6869 \tTraining Loss: 0.040877\n",
      "Epoch: 6870 \tTraining Loss: 0.040871\n",
      "Epoch: 6871 \tTraining Loss: 0.040891\n",
      "Epoch: 6872 \tTraining Loss: 0.040862\n",
      "Epoch: 6873 \tTraining Loss: 0.040883\n",
      "Epoch: 6874 \tTraining Loss: 0.040875\n",
      "Epoch: 6875 \tTraining Loss: 0.040915\n",
      "Epoch: 6876 \tTraining Loss: 0.040979\n",
      "Epoch: 6877 \tTraining Loss: 0.040850\n",
      "Epoch: 6878 \tTraining Loss: 0.040876\n",
      "Epoch: 6879 \tTraining Loss: 0.040879\n",
      "Epoch: 6880 \tTraining Loss: 0.040867\n",
      "Epoch: 6881 \tTraining Loss: 0.040849\n",
      "Epoch: 6882 \tTraining Loss: 0.040890\n",
      "Epoch: 6883 \tTraining Loss: 0.040851\n",
      "Epoch: 6884 \tTraining Loss: 0.040898\n",
      "Epoch: 6885 \tTraining Loss: 0.040843\n",
      "Epoch: 6886 \tTraining Loss: 0.040960\n",
      "Epoch: 6887 \tTraining Loss: 0.040874\n",
      "Epoch: 6888 \tTraining Loss: 0.040846\n",
      "Epoch: 6889 \tTraining Loss: 0.040867\n",
      "Epoch: 6890 \tTraining Loss: 0.040857\n",
      "Epoch: 6891 \tTraining Loss: 0.040859\n",
      "Epoch: 6892 \tTraining Loss: 0.040889\n",
      "Epoch: 6893 \tTraining Loss: 0.040823\n",
      "Epoch: 6894 \tTraining Loss: 0.040855\n",
      "Epoch: 6895 \tTraining Loss: 0.040833\n",
      "Epoch: 6896 \tTraining Loss: 0.040892\n",
      "Epoch: 6897 \tTraining Loss: 0.040822\n",
      "Epoch: 6898 \tTraining Loss: 0.040886\n",
      "Epoch: 6899 \tTraining Loss: 0.041004\n",
      "Epoch: 6900 \tTraining Loss: 0.041179\n",
      "Epoch: 6901 \tTraining Loss: 0.040920\n",
      "Epoch: 6902 \tTraining Loss: 0.040894\n",
      "Epoch: 6903 \tTraining Loss: 0.040868\n",
      "Epoch: 6904 \tTraining Loss: 0.040852\n",
      "Epoch: 6905 \tTraining Loss: 0.040855\n",
      "Epoch: 6906 \tTraining Loss: 0.040967\n",
      "Epoch: 6907 \tTraining Loss: 0.040851\n",
      "Epoch: 6908 \tTraining Loss: 0.040880\n",
      "Epoch: 6909 \tTraining Loss: 0.040840\n",
      "Epoch: 6910 \tTraining Loss: 0.040863\n",
      "Epoch: 6911 \tTraining Loss: 0.040848\n",
      "Epoch: 6912 \tTraining Loss: 0.040835\n",
      "Epoch: 6913 \tTraining Loss: 0.040844\n",
      "Epoch: 6914 \tTraining Loss: 0.040875\n",
      "Epoch: 6915 \tTraining Loss: 0.040872\n",
      "Epoch: 6916 \tTraining Loss: 0.040838\n",
      "Epoch: 6917 \tTraining Loss: 0.040843\n",
      "Epoch: 6918 \tTraining Loss: 0.040864\n",
      "Epoch: 6919 \tTraining Loss: 0.040834\n",
      "Epoch: 6920 \tTraining Loss: 0.040826\n",
      "Epoch: 6921 \tTraining Loss: 0.040808\n",
      "Epoch: 6922 \tTraining Loss: 0.040828\n",
      "Epoch: 6923 \tTraining Loss: 0.040888\n",
      "Epoch: 6924 \tTraining Loss: 0.040830\n",
      "Epoch: 6925 \tTraining Loss: 0.040816\n",
      "Epoch: 6926 \tTraining Loss: 0.040871\n",
      "Epoch: 6927 \tTraining Loss: 0.040819\n",
      "Epoch: 6928 \tTraining Loss: 0.040856\n",
      "Epoch: 6929 \tTraining Loss: 0.040848\n",
      "Epoch: 6930 \tTraining Loss: 0.040804\n",
      "Epoch: 6931 \tTraining Loss: 0.040814\n",
      "Epoch: 6932 \tTraining Loss: 0.040828\n",
      "Epoch: 6933 \tTraining Loss: 0.040808\n",
      "Epoch: 6934 \tTraining Loss: 0.040816\n",
      "Epoch: 6935 \tTraining Loss: 0.040828\n",
      "Epoch: 6936 \tTraining Loss: 0.040870\n",
      "Epoch: 6937 \tTraining Loss: 0.040845\n",
      "Epoch: 6938 \tTraining Loss: 0.040879\n",
      "Epoch: 6939 \tTraining Loss: 0.040870\n",
      "Epoch: 6940 \tTraining Loss: 0.040834\n",
      "Epoch: 6941 \tTraining Loss: 0.040906\n",
      "Epoch: 6942 \tTraining Loss: 0.040801\n",
      "Epoch: 6943 \tTraining Loss: 0.040807\n",
      "Epoch: 6944 \tTraining Loss: 0.040837\n",
      "Epoch: 6945 \tTraining Loss: 0.040808\n",
      "Epoch: 6946 \tTraining Loss: 0.040800\n",
      "Epoch: 6947 \tTraining Loss: 0.040856\n",
      "Epoch: 6948 \tTraining Loss: 0.040799\n",
      "Epoch: 6949 \tTraining Loss: 0.040804\n",
      "Epoch: 6950 \tTraining Loss: 0.040805\n",
      "Epoch: 6951 \tTraining Loss: 0.040797\n",
      "Epoch: 6952 \tTraining Loss: 0.040809\n",
      "Epoch: 6953 \tTraining Loss: 0.040831\n",
      "Epoch: 6954 \tTraining Loss: 0.040844\n",
      "Epoch: 6955 \tTraining Loss: 0.040801\n",
      "Epoch: 6956 \tTraining Loss: 0.040814\n",
      "Epoch: 6957 \tTraining Loss: 0.040801\n",
      "Epoch: 6958 \tTraining Loss: 0.040804\n",
      "Epoch: 6959 \tTraining Loss: 0.040792\n",
      "Epoch: 6960 \tTraining Loss: 0.040844\n",
      "Epoch: 6961 \tTraining Loss: 0.040812\n",
      "Epoch: 6962 \tTraining Loss: 0.040775\n",
      "Epoch: 6963 \tTraining Loss: 0.040814\n",
      "Epoch: 6964 \tTraining Loss: 0.040800\n",
      "Epoch: 6965 \tTraining Loss: 0.040807\n",
      "Epoch: 6966 \tTraining Loss: 0.040797\n",
      "Epoch: 6967 \tTraining Loss: 0.040822\n",
      "Epoch: 6968 \tTraining Loss: 0.040809\n",
      "Epoch: 6969 \tTraining Loss: 0.040811\n",
      "Epoch: 6970 \tTraining Loss: 0.040816\n",
      "Epoch: 6971 \tTraining Loss: 0.040841\n",
      "Epoch: 6972 \tTraining Loss: 0.040802\n",
      "Epoch: 6973 \tTraining Loss: 0.040785\n",
      "Epoch: 6974 \tTraining Loss: 0.040779\n",
      "Epoch: 6975 \tTraining Loss: 0.040837\n",
      "Epoch: 6976 \tTraining Loss: 0.040780\n",
      "Epoch: 6977 \tTraining Loss: 0.040839\n",
      "Epoch: 6978 \tTraining Loss: 0.040809\n",
      "Epoch: 6979 \tTraining Loss: 0.040823\n",
      "Epoch: 6980 \tTraining Loss: 0.040825\n",
      "Epoch: 6981 \tTraining Loss: 0.040818\n",
      "Epoch: 6982 \tTraining Loss: 0.040781\n",
      "Epoch: 6983 \tTraining Loss: 0.040839\n",
      "Epoch: 6984 \tTraining Loss: 0.040829\n",
      "Epoch: 6985 \tTraining Loss: 0.040873\n",
      "Epoch: 6986 \tTraining Loss: 0.040774\n",
      "Epoch: 6987 \tTraining Loss: 0.040780\n",
      "Epoch: 6988 \tTraining Loss: 0.040796\n",
      "Epoch: 6989 \tTraining Loss: 0.040784\n",
      "Epoch: 6990 \tTraining Loss: 0.040813\n",
      "Epoch: 6991 \tTraining Loss: 0.040771\n",
      "Epoch: 6992 \tTraining Loss: 0.040774\n",
      "Epoch: 6993 \tTraining Loss: 0.040777\n",
      "Epoch: 6994 \tTraining Loss: 0.040822\n",
      "Epoch: 6995 \tTraining Loss: 0.040949\n",
      "Epoch: 6996 \tTraining Loss: 0.040802\n",
      "Epoch: 6997 \tTraining Loss: 0.040784\n",
      "Epoch: 6998 \tTraining Loss: 0.040832\n",
      "Epoch: 6999 \tTraining Loss: 0.040773\n",
      "Epoch: 7000 \tTraining Loss: 0.040776\n",
      "Epoch: 7001 \tTraining Loss: 0.040767\n",
      "Epoch: 7002 \tTraining Loss: 0.040846\n",
      "Epoch: 7003 \tTraining Loss: 0.040848\n",
      "Epoch: 7004 \tTraining Loss: 0.040793\n",
      "Epoch: 7005 \tTraining Loss: 0.040815\n",
      "Epoch: 7006 \tTraining Loss: 0.040817\n",
      "Epoch: 7007 \tTraining Loss: 0.040799\n",
      "Epoch: 7008 \tTraining Loss: 0.040799\n",
      "Epoch: 7009 \tTraining Loss: 0.040844\n",
      "Epoch: 7010 \tTraining Loss: 0.040784\n",
      "Epoch: 7011 \tTraining Loss: 0.040830\n",
      "Epoch: 7012 \tTraining Loss: 0.040797\n",
      "Epoch: 7013 \tTraining Loss: 0.040799\n",
      "Epoch: 7014 \tTraining Loss: 0.040852\n",
      "Epoch: 7015 \tTraining Loss: 0.040779\n",
      "Epoch: 7016 \tTraining Loss: 0.040776\n",
      "Epoch: 7017 \tTraining Loss: 0.040751\n",
      "Epoch: 7018 \tTraining Loss: 0.040868\n",
      "Epoch: 7019 \tTraining Loss: 0.040797\n",
      "Epoch: 7020 \tTraining Loss: 0.040853\n",
      "Epoch: 7021 \tTraining Loss: 0.040782\n",
      "Epoch: 7022 \tTraining Loss: 0.040785\n",
      "Epoch: 7023 \tTraining Loss: 0.040766\n",
      "Epoch: 7024 \tTraining Loss: 0.040807\n",
      "Epoch: 7025 \tTraining Loss: 0.040772\n",
      "Epoch: 7026 \tTraining Loss: 0.040773\n",
      "Epoch: 7027 \tTraining Loss: 0.040773\n",
      "Epoch: 7028 \tTraining Loss: 0.040831\n",
      "Epoch: 7029 \tTraining Loss: 0.040771\n",
      "Epoch: 7030 \tTraining Loss: 0.040763\n",
      "Epoch: 7031 \tTraining Loss: 0.040781\n",
      "Epoch: 7032 \tTraining Loss: 0.040782\n",
      "Epoch: 7033 \tTraining Loss: 0.040800\n",
      "Epoch: 7034 \tTraining Loss: 0.040765\n",
      "Epoch: 7035 \tTraining Loss: 0.040771\n",
      "Epoch: 7036 \tTraining Loss: 0.040790\n",
      "Epoch: 7037 \tTraining Loss: 0.040806\n",
      "Epoch: 7038 \tTraining Loss: 0.116043\n",
      "Epoch: 7039 \tTraining Loss: 0.300090\n",
      "Epoch: 7040 \tTraining Loss: 0.168707\n",
      "Epoch: 7041 \tTraining Loss: 0.105620\n",
      "Epoch: 7042 \tTraining Loss: 0.121605\n",
      "Epoch: 7043 \tTraining Loss: 0.060471\n",
      "Epoch: 7044 \tTraining Loss: 0.056078\n",
      "Epoch: 7045 \tTraining Loss: 0.044272\n",
      "Epoch: 7046 \tTraining Loss: 0.041621\n",
      "Epoch: 7047 \tTraining Loss: 0.039813\n",
      "Epoch: 7048 \tTraining Loss: 0.039465\n",
      "Epoch: 7049 \tTraining Loss: 0.039277\n",
      "Epoch: 7050 \tTraining Loss: 0.039272\n",
      "Epoch: 7051 \tTraining Loss: 0.039141\n",
      "Epoch: 7052 \tTraining Loss: 0.039088\n",
      "Epoch: 7053 \tTraining Loss: 0.039061\n",
      "Epoch: 7054 \tTraining Loss: 0.039100\n",
      "Epoch: 7055 \tTraining Loss: 0.038991\n",
      "Epoch: 7056 \tTraining Loss: 0.039124\n",
      "Epoch: 7057 \tTraining Loss: 0.039020\n",
      "Epoch: 7058 \tTraining Loss: 0.038987\n",
      "Epoch: 7059 \tTraining Loss: 0.038953\n",
      "Epoch: 7060 \tTraining Loss: 0.038973\n",
      "Epoch: 7061 \tTraining Loss: 0.038982\n",
      "Epoch: 7062 \tTraining Loss: 0.038969\n",
      "Epoch: 7063 \tTraining Loss: 0.038924\n",
      "Epoch: 7064 \tTraining Loss: 0.038929\n",
      "Epoch: 7065 \tTraining Loss: 0.038932\n",
      "Epoch: 7066 \tTraining Loss: 0.038919\n",
      "Epoch: 7067 \tTraining Loss: 0.038945\n",
      "Epoch: 7068 \tTraining Loss: 0.038941\n",
      "Epoch: 7069 \tTraining Loss: 0.038934\n",
      "Epoch: 7070 \tTraining Loss: 0.038874\n",
      "Epoch: 7071 \tTraining Loss: 0.038904\n",
      "Epoch: 7072 \tTraining Loss: 0.038914\n",
      "Epoch: 7073 \tTraining Loss: 0.038991\n",
      "Epoch: 7074 \tTraining Loss: 0.038933\n",
      "Epoch: 7075 \tTraining Loss: 0.038934\n",
      "Epoch: 7076 \tTraining Loss: 0.039015\n",
      "Epoch: 7077 \tTraining Loss: 0.038911\n",
      "Epoch: 7078 \tTraining Loss: 0.038910\n",
      "Epoch: 7079 \tTraining Loss: 0.038914\n",
      "Epoch: 7080 \tTraining Loss: 0.038939\n",
      "Epoch: 7081 \tTraining Loss: 0.038961\n",
      "Epoch: 7082 \tTraining Loss: 0.038900\n",
      "Epoch: 7083 \tTraining Loss: 0.038888\n",
      "Epoch: 7084 \tTraining Loss: 0.038903\n",
      "Epoch: 7085 \tTraining Loss: 0.038899\n",
      "Epoch: 7086 \tTraining Loss: 0.038933\n",
      "Epoch: 7087 \tTraining Loss: 0.038923\n",
      "Epoch: 7088 \tTraining Loss: 0.038873\n",
      "Epoch: 7089 \tTraining Loss: 0.038927\n",
      "Epoch: 7090 \tTraining Loss: 0.038913\n",
      "Epoch: 7091 \tTraining Loss: 0.038885\n",
      "Epoch: 7092 \tTraining Loss: 0.038862\n",
      "Epoch: 7093 \tTraining Loss: 0.038929\n",
      "Epoch: 7094 \tTraining Loss: 0.038864\n",
      "Epoch: 7095 \tTraining Loss: 0.038867\n",
      "Epoch: 7096 \tTraining Loss: 0.038865\n",
      "Epoch: 7097 \tTraining Loss: 0.038889\n",
      "Epoch: 7098 \tTraining Loss: 0.038876\n",
      "Epoch: 7099 \tTraining Loss: 0.038856\n",
      "Epoch: 7100 \tTraining Loss: 0.038966\n",
      "Epoch: 7101 \tTraining Loss: 0.038855\n",
      "Epoch: 7102 \tTraining Loss: 0.038890\n",
      "Epoch: 7103 \tTraining Loss: 0.038825\n",
      "Epoch: 7104 \tTraining Loss: 0.038898\n",
      "Epoch: 7105 \tTraining Loss: 0.038861\n",
      "Epoch: 7106 \tTraining Loss: 0.038889\n",
      "Epoch: 7107 \tTraining Loss: 0.038897\n",
      "Epoch: 7108 \tTraining Loss: 0.038827\n",
      "Epoch: 7109 \tTraining Loss: 0.038844\n",
      "Epoch: 7110 \tTraining Loss: 0.038950\n",
      "Epoch: 7111 \tTraining Loss: 0.038911\n",
      "Epoch: 7112 \tTraining Loss: 0.038850\n",
      "Epoch: 7113 \tTraining Loss: 0.038885\n",
      "Epoch: 7114 \tTraining Loss: 0.038893\n",
      "Epoch: 7115 \tTraining Loss: 0.038856\n",
      "Epoch: 7116 \tTraining Loss: 0.038858\n",
      "Epoch: 7117 \tTraining Loss: 0.038846\n",
      "Epoch: 7118 \tTraining Loss: 0.038856\n",
      "Epoch: 7119 \tTraining Loss: 0.038841\n",
      "Epoch: 7120 \tTraining Loss: 0.038850\n",
      "Epoch: 7121 \tTraining Loss: 0.038852\n",
      "Epoch: 7122 \tTraining Loss: 0.038842\n",
      "Epoch: 7123 \tTraining Loss: 0.038848\n",
      "Epoch: 7124 \tTraining Loss: 0.038938\n",
      "Epoch: 7125 \tTraining Loss: 0.038832\n",
      "Epoch: 7126 \tTraining Loss: 0.038874\n",
      "Epoch: 7127 \tTraining Loss: 0.038867\n",
      "Epoch: 7128 \tTraining Loss: 0.038861\n",
      "Epoch: 7129 \tTraining Loss: 0.038889\n",
      "Epoch: 7130 \tTraining Loss: 0.038872\n",
      "Epoch: 7131 \tTraining Loss: 0.038834\n",
      "Epoch: 7132 \tTraining Loss: 0.038841\n",
      "Epoch: 7133 \tTraining Loss: 0.038854\n",
      "Epoch: 7134 \tTraining Loss: 0.038847\n",
      "Epoch: 7135 \tTraining Loss: 0.038839\n",
      "Epoch: 7136 \tTraining Loss: 0.038880\n",
      "Epoch: 7137 \tTraining Loss: 0.038830\n",
      "Epoch: 7138 \tTraining Loss: 0.038821\n",
      "Epoch: 7139 \tTraining Loss: 0.038864\n",
      "Epoch: 7140 \tTraining Loss: 0.038841\n",
      "Epoch: 7141 \tTraining Loss: 0.038856\n",
      "Epoch: 7142 \tTraining Loss: 0.038835\n",
      "Epoch: 7143 \tTraining Loss: 0.038836\n",
      "Epoch: 7144 \tTraining Loss: 0.038839\n",
      "Epoch: 7145 \tTraining Loss: 0.038836\n",
      "Epoch: 7146 \tTraining Loss: 0.038846\n",
      "Epoch: 7147 \tTraining Loss: 0.038826\n",
      "Epoch: 7148 \tTraining Loss: 0.038832\n",
      "Epoch: 7149 \tTraining Loss: 0.038840\n",
      "Epoch: 7150 \tTraining Loss: 0.038857\n",
      "Epoch: 7151 \tTraining Loss: 0.038829\n",
      "Epoch: 7152 \tTraining Loss: 0.038830\n",
      "Epoch: 7153 \tTraining Loss: 0.038837\n",
      "Epoch: 7154 \tTraining Loss: 0.038824\n",
      "Epoch: 7155 \tTraining Loss: 0.038924\n",
      "Epoch: 7156 \tTraining Loss: 0.038922\n",
      "Epoch: 7157 \tTraining Loss: 0.038813\n",
      "Epoch: 7158 \tTraining Loss: 0.038825\n",
      "Epoch: 7159 \tTraining Loss: 0.038851\n",
      "Epoch: 7160 \tTraining Loss: 0.038821\n",
      "Epoch: 7161 \tTraining Loss: 0.038847\n",
      "Epoch: 7162 \tTraining Loss: 0.038929\n",
      "Epoch: 7163 \tTraining Loss: 0.038898\n",
      "Epoch: 7164 \tTraining Loss: 0.038849\n",
      "Epoch: 7165 \tTraining Loss: 0.038836\n",
      "Epoch: 7166 \tTraining Loss: 0.038818\n",
      "Epoch: 7167 \tTraining Loss: 0.038843\n",
      "Epoch: 7168 \tTraining Loss: 0.038852\n",
      "Epoch: 7169 \tTraining Loss: 0.038820\n",
      "Epoch: 7170 \tTraining Loss: 0.038840\n",
      "Epoch: 7171 \tTraining Loss: 0.038814\n",
      "Epoch: 7172 \tTraining Loss: 0.038801\n",
      "Epoch: 7173 \tTraining Loss: 0.038882\n",
      "Epoch: 7174 \tTraining Loss: 0.038824\n",
      "Epoch: 7175 \tTraining Loss: 0.038851\n",
      "Epoch: 7176 \tTraining Loss: 0.038879\n",
      "Epoch: 7177 \tTraining Loss: 0.038838\n",
      "Epoch: 7178 \tTraining Loss: 0.039102\n",
      "Epoch: 7179 \tTraining Loss: 0.039652\n",
      "Epoch: 7180 \tTraining Loss: 0.038816\n",
      "Epoch: 7181 \tTraining Loss: 0.038822\n",
      "Epoch: 7182 \tTraining Loss: 0.038821\n",
      "Epoch: 7183 \tTraining Loss: 0.038804\n",
      "Epoch: 7184 \tTraining Loss: 0.038829\n",
      "Epoch: 7185 \tTraining Loss: 0.038804\n",
      "Epoch: 7186 \tTraining Loss: 0.038822\n",
      "Epoch: 7187 \tTraining Loss: 0.038854\n",
      "Epoch: 7188 \tTraining Loss: 0.038848\n",
      "Epoch: 7189 \tTraining Loss: 0.038831\n",
      "Epoch: 7190 \tTraining Loss: 0.038854\n",
      "Epoch: 7191 \tTraining Loss: 0.038808\n",
      "Epoch: 7192 \tTraining Loss: 0.038822\n",
      "Epoch: 7193 \tTraining Loss: 0.038827\n",
      "Epoch: 7194 \tTraining Loss: 0.038847\n",
      "Epoch: 7195 \tTraining Loss: 0.038828\n",
      "Epoch: 7196 \tTraining Loss: 0.038853\n",
      "Epoch: 7197 \tTraining Loss: 0.038805\n",
      "Epoch: 7198 \tTraining Loss: 0.038819\n",
      "Epoch: 7199 \tTraining Loss: 0.038860\n",
      "Epoch: 7200 \tTraining Loss: 0.038816\n",
      "Epoch: 7201 \tTraining Loss: 0.038845\n",
      "Epoch: 7202 \tTraining Loss: 0.038821\n",
      "Epoch: 7203 \tTraining Loss: 0.038793\n",
      "Epoch: 7204 \tTraining Loss: 0.038844\n",
      "Epoch: 7205 \tTraining Loss: 0.038843\n",
      "Epoch: 7206 \tTraining Loss: 0.038803\n",
      "Epoch: 7207 \tTraining Loss: 0.038870\n",
      "Epoch: 7208 \tTraining Loss: 0.038829\n",
      "Epoch: 7209 \tTraining Loss: 0.038827\n",
      "Epoch: 7210 \tTraining Loss: 0.038814\n",
      "Epoch: 7211 \tTraining Loss: 0.038796\n",
      "Epoch: 7212 \tTraining Loss: 0.038874\n",
      "Epoch: 7213 \tTraining Loss: 0.038812\n",
      "Epoch: 7214 \tTraining Loss: 0.038854\n",
      "Epoch: 7215 \tTraining Loss: 0.038866\n",
      "Epoch: 7216 \tTraining Loss: 0.038801\n",
      "Epoch: 7217 \tTraining Loss: 0.038839\n",
      "Epoch: 7218 \tTraining Loss: 0.038805\n",
      "Epoch: 7219 \tTraining Loss: 0.038845\n",
      "Epoch: 7220 \tTraining Loss: 0.038819\n",
      "Epoch: 7221 \tTraining Loss: 0.038842\n",
      "Epoch: 7222 \tTraining Loss: 0.038821\n",
      "Epoch: 7223 \tTraining Loss: 0.038807\n",
      "Epoch: 7224 \tTraining Loss: 0.038831\n",
      "Epoch: 7225 \tTraining Loss: 0.038801\n",
      "Epoch: 7226 \tTraining Loss: 0.038951\n",
      "Epoch: 7227 \tTraining Loss: 0.038779\n",
      "Epoch: 7228 \tTraining Loss: 0.038827\n",
      "Epoch: 7229 \tTraining Loss: 0.038893\n",
      "Epoch: 7230 \tTraining Loss: 0.038798\n",
      "Epoch: 7231 \tTraining Loss: 0.038858\n",
      "Epoch: 7232 \tTraining Loss: 0.038808\n",
      "Epoch: 7233 \tTraining Loss: 0.038851\n",
      "Epoch: 7234 \tTraining Loss: 0.038781\n",
      "Epoch: 7235 \tTraining Loss: 0.038791\n",
      "Epoch: 7236 \tTraining Loss: 0.038802\n",
      "Epoch: 7237 \tTraining Loss: 0.038814\n",
      "Epoch: 7238 \tTraining Loss: 0.038817\n",
      "Epoch: 7239 \tTraining Loss: 0.038801\n",
      "Epoch: 7240 \tTraining Loss: 0.038835\n",
      "Epoch: 7241 \tTraining Loss: 0.038784\n",
      "Epoch: 7242 \tTraining Loss: 0.038804\n",
      "Epoch: 7243 \tTraining Loss: 0.038799\n",
      "Epoch: 7244 \tTraining Loss: 0.038797\n",
      "Epoch: 7245 \tTraining Loss: 0.038785\n",
      "Epoch: 7246 \tTraining Loss: 0.038802\n",
      "Epoch: 7247 \tTraining Loss: 0.038820\n",
      "Epoch: 7248 \tTraining Loss: 0.038805\n",
      "Epoch: 7249 \tTraining Loss: 0.038780\n",
      "Epoch: 7250 \tTraining Loss: 0.038831\n",
      "Epoch: 7251 \tTraining Loss: 0.038815\n",
      "Epoch: 7252 \tTraining Loss: 0.038979\n",
      "Epoch: 7253 \tTraining Loss: 0.038821\n",
      "Epoch: 7254 \tTraining Loss: 0.038839\n",
      "Epoch: 7255 \tTraining Loss: 0.038782\n",
      "Epoch: 7256 \tTraining Loss: 0.038831\n",
      "Epoch: 7257 \tTraining Loss: 0.038835\n",
      "Epoch: 7258 \tTraining Loss: 0.038781\n",
      "Epoch: 7259 \tTraining Loss: 0.038806\n",
      "Epoch: 7260 \tTraining Loss: 0.038810\n",
      "Epoch: 7261 \tTraining Loss: 0.038866\n",
      "Epoch: 7262 \tTraining Loss: 0.038773\n",
      "Epoch: 7263 \tTraining Loss: 0.038792\n",
      "Epoch: 7264 \tTraining Loss: 0.038794\n",
      "Epoch: 7265 \tTraining Loss: 0.038778\n",
      "Epoch: 7266 \tTraining Loss: 0.038850\n",
      "Epoch: 7267 \tTraining Loss: 0.038862\n",
      "Epoch: 7268 \tTraining Loss: 0.038844\n",
      "Epoch: 7269 \tTraining Loss: 0.038823\n",
      "Epoch: 7270 \tTraining Loss: 0.038840\n",
      "Epoch: 7271 \tTraining Loss: 0.038824\n",
      "Epoch: 7272 \tTraining Loss: 0.038759\n",
      "Epoch: 7273 \tTraining Loss: 0.038788\n",
      "Epoch: 7274 \tTraining Loss: 0.038809\n",
      "Epoch: 7275 \tTraining Loss: 0.038787\n",
      "Epoch: 7276 \tTraining Loss: 0.038789\n",
      "Epoch: 7277 \tTraining Loss: 0.038807\n",
      "Epoch: 7278 \tTraining Loss: 0.038822\n",
      "Epoch: 7279 \tTraining Loss: 0.038766\n",
      "Epoch: 7280 \tTraining Loss: 0.038782\n",
      "Epoch: 7281 \tTraining Loss: 0.038814\n",
      "Epoch: 7282 \tTraining Loss: 0.038722\n",
      "Epoch: 7283 \tTraining Loss: 0.038781\n",
      "Epoch: 7284 \tTraining Loss: 0.038805\n",
      "Epoch: 7285 \tTraining Loss: 0.038781\n",
      "Epoch: 7286 \tTraining Loss: 0.038781\n",
      "Epoch: 7287 \tTraining Loss: 0.038891\n",
      "Epoch: 7288 \tTraining Loss: 0.038815\n",
      "Epoch: 7289 \tTraining Loss: 0.039057\n",
      "Epoch: 7290 \tTraining Loss: 0.047013\n",
      "Epoch: 7291 \tTraining Loss: 0.114461\n",
      "Epoch: 7292 \tTraining Loss: 0.168859\n",
      "Epoch: 7293 \tTraining Loss: 0.215361\n",
      "Epoch: 7294 \tTraining Loss: 0.178692\n",
      "Epoch: 7295 \tTraining Loss: 0.131678\n",
      "Epoch: 7296 \tTraining Loss: 0.113918\n",
      "Epoch: 7297 \tTraining Loss: 0.067108\n",
      "Epoch: 7298 \tTraining Loss: 0.064860\n",
      "Epoch: 7299 \tTraining Loss: 0.052213\n",
      "Epoch: 7300 \tTraining Loss: 0.054028\n",
      "Epoch: 7301 \tTraining Loss: 0.051968\n",
      "Epoch: 7302 \tTraining Loss: 0.050655\n",
      "Epoch: 7303 \tTraining Loss: 0.049224\n",
      "Epoch: 7304 \tTraining Loss: 0.048542\n",
      "Epoch: 7305 \tTraining Loss: 0.048091\n",
      "Epoch: 7306 \tTraining Loss: 0.047785\n",
      "Epoch: 7307 \tTraining Loss: 0.047506\n",
      "Epoch: 7308 \tTraining Loss: 0.047485\n",
      "Epoch: 7309 \tTraining Loss: 0.047479\n",
      "Epoch: 7310 \tTraining Loss: 0.047441\n",
      "Epoch: 7311 \tTraining Loss: 0.047391\n",
      "Epoch: 7312 \tTraining Loss: 0.047493\n",
      "Epoch: 7313 \tTraining Loss: 0.047378\n",
      "Epoch: 7314 \tTraining Loss: 0.047383\n",
      "Epoch: 7315 \tTraining Loss: 0.047353\n",
      "Epoch: 7316 \tTraining Loss: 0.047460\n",
      "Epoch: 7317 \tTraining Loss: 0.047571\n",
      "Epoch: 7318 \tTraining Loss: 0.047348\n",
      "Epoch: 7319 \tTraining Loss: 0.047358\n",
      "Epoch: 7320 \tTraining Loss: 0.047265\n",
      "Epoch: 7321 \tTraining Loss: 0.047282\n",
      "Epoch: 7322 \tTraining Loss: 0.047352\n",
      "Epoch: 7323 \tTraining Loss: 0.047466\n",
      "Epoch: 7324 \tTraining Loss: 0.047298\n",
      "Epoch: 7325 \tTraining Loss: 0.047323\n",
      "Epoch: 7326 \tTraining Loss: 0.047291\n",
      "Epoch: 7327 \tTraining Loss: 0.047236\n",
      "Epoch: 7328 \tTraining Loss: 0.047353\n",
      "Epoch: 7329 \tTraining Loss: 0.047466\n",
      "Epoch: 7330 \tTraining Loss: 0.047283\n",
      "Epoch: 7331 \tTraining Loss: 0.047256\n",
      "Epoch: 7332 \tTraining Loss: 0.050435\n",
      "Epoch: 7333 \tTraining Loss: 0.047263\n",
      "Epoch: 7334 \tTraining Loss: 0.046968\n",
      "Epoch: 7335 \tTraining Loss: 0.045871\n",
      "Epoch: 7336 \tTraining Loss: 0.045441\n",
      "Epoch: 7337 \tTraining Loss: 0.045453\n",
      "Epoch: 7338 \tTraining Loss: 0.045254\n",
      "Epoch: 7339 \tTraining Loss: 0.045284\n",
      "Epoch: 7340 \tTraining Loss: 0.045300\n",
      "Epoch: 7341 \tTraining Loss: 0.045244\n",
      "Epoch: 7342 \tTraining Loss: 0.045230\n",
      "Epoch: 7343 \tTraining Loss: 0.045198\n",
      "Epoch: 7344 \tTraining Loss: 0.045084\n",
      "Epoch: 7345 \tTraining Loss: 0.044228\n",
      "Epoch: 7346 \tTraining Loss: 0.044171\n",
      "Epoch: 7347 \tTraining Loss: 0.044187\n",
      "Epoch: 7348 \tTraining Loss: 0.044244\n",
      "Epoch: 7349 \tTraining Loss: 0.044161\n",
      "Epoch: 7350 \tTraining Loss: 0.044161\n",
      "Epoch: 7351 \tTraining Loss: 0.044151\n",
      "Epoch: 7352 \tTraining Loss: 0.044116\n",
      "Epoch: 7353 \tTraining Loss: 0.044216\n",
      "Epoch: 7354 \tTraining Loss: 0.044118\n",
      "Epoch: 7355 \tTraining Loss: 0.044054\n",
      "Epoch: 7356 \tTraining Loss: 0.044033\n",
      "Epoch: 7357 \tTraining Loss: 0.043951\n",
      "Epoch: 7358 \tTraining Loss: 0.044002\n",
      "Epoch: 7359 \tTraining Loss: 0.045677\n",
      "Epoch: 7360 \tTraining Loss: 0.044994\n",
      "Epoch: 7361 \tTraining Loss: 0.044133\n",
      "Epoch: 7362 \tTraining Loss: 0.044083\n",
      "Epoch: 7363 \tTraining Loss: 0.044130\n",
      "Epoch: 7364 \tTraining Loss: 0.043970\n",
      "Epoch: 7365 \tTraining Loss: 0.043933\n",
      "Epoch: 7366 \tTraining Loss: 0.043978\n",
      "Epoch: 7367 \tTraining Loss: 0.044060\n",
      "Epoch: 7368 \tTraining Loss: 0.043914\n",
      "Epoch: 7369 \tTraining Loss: 0.043909\n",
      "Epoch: 7370 \tTraining Loss: 0.043880\n",
      "Epoch: 7371 \tTraining Loss: 0.043865\n",
      "Epoch: 7372 \tTraining Loss: 0.043960\n",
      "Epoch: 7373 \tTraining Loss: 0.043858\n",
      "Epoch: 7374 \tTraining Loss: 0.043880\n",
      "Epoch: 7375 \tTraining Loss: 0.043874\n",
      "Epoch: 7376 \tTraining Loss: 0.043884\n",
      "Epoch: 7377 \tTraining Loss: 0.043930\n",
      "Epoch: 7378 \tTraining Loss: 0.043890\n",
      "Epoch: 7379 \tTraining Loss: 0.043833\n",
      "Epoch: 7380 \tTraining Loss: 0.043866\n",
      "Epoch: 7381 \tTraining Loss: 0.043841\n",
      "Epoch: 7382 \tTraining Loss: 0.043832\n",
      "Epoch: 7383 \tTraining Loss: 0.043882\n",
      "Epoch: 7384 \tTraining Loss: 0.043811\n",
      "Epoch: 7385 \tTraining Loss: 0.043838\n",
      "Epoch: 7386 \tTraining Loss: 0.043855\n",
      "Epoch: 7387 \tTraining Loss: 0.043885\n",
      "Epoch: 7388 \tTraining Loss: 0.043810\n",
      "Epoch: 7389 \tTraining Loss: 0.043829\n",
      "Epoch: 7390 \tTraining Loss: 0.043833\n",
      "Epoch: 7391 \tTraining Loss: 0.043875\n",
      "Epoch: 7392 \tTraining Loss: 0.043853\n",
      "Epoch: 7393 \tTraining Loss: 0.043801\n",
      "Epoch: 7394 \tTraining Loss: 0.043838\n",
      "Epoch: 7395 \tTraining Loss: 0.043808\n",
      "Epoch: 7396 \tTraining Loss: 0.043788\n",
      "Epoch: 7397 \tTraining Loss: 0.043800\n",
      "Epoch: 7398 \tTraining Loss: 0.043842\n",
      "Epoch: 7399 \tTraining Loss: 0.043785\n",
      "Epoch: 7400 \tTraining Loss: 0.043800\n",
      "Epoch: 7401 \tTraining Loss: 0.043865\n",
      "Epoch: 7402 \tTraining Loss: 0.043803\n",
      "Epoch: 7403 \tTraining Loss: 0.043804\n",
      "Epoch: 7404 \tTraining Loss: 0.043791\n",
      "Epoch: 7405 \tTraining Loss: 0.043787\n",
      "Epoch: 7406 \tTraining Loss: 0.043805\n",
      "Epoch: 7407 \tTraining Loss: 0.043804\n",
      "Epoch: 7408 \tTraining Loss: 0.043768\n",
      "Epoch: 7409 \tTraining Loss: 0.043809\n",
      "Epoch: 7410 \tTraining Loss: 0.043785\n",
      "Epoch: 7411 \tTraining Loss: 0.043887\n",
      "Epoch: 7412 \tTraining Loss: 0.043858\n",
      "Epoch: 7413 \tTraining Loss: 0.043816\n",
      "Epoch: 7414 \tTraining Loss: 0.043804\n",
      "Epoch: 7415 \tTraining Loss: 0.043780\n",
      "Epoch: 7416 \tTraining Loss: 0.043918\n",
      "Epoch: 7417 \tTraining Loss: 0.043781\n",
      "Epoch: 7418 \tTraining Loss: 0.043839\n",
      "Epoch: 7419 \tTraining Loss: 0.044046\n",
      "Epoch: 7420 \tTraining Loss: 0.043782\n",
      "Epoch: 7421 \tTraining Loss: 0.043813\n",
      "Epoch: 7422 \tTraining Loss: 0.043800\n",
      "Epoch: 7423 \tTraining Loss: 0.043782\n",
      "Epoch: 7424 \tTraining Loss: 0.043786\n",
      "Epoch: 7425 \tTraining Loss: 0.043769\n",
      "Epoch: 7426 \tTraining Loss: 0.043763\n",
      "Epoch: 7427 \tTraining Loss: 0.043819\n",
      "Epoch: 7428 \tTraining Loss: 0.043828\n",
      "Epoch: 7429 \tTraining Loss: 0.043809\n",
      "Epoch: 7430 \tTraining Loss: 0.043794\n",
      "Epoch: 7431 \tTraining Loss: 0.043812\n",
      "Epoch: 7432 \tTraining Loss: 0.048057\n",
      "Epoch: 7433 \tTraining Loss: 0.054718\n",
      "Epoch: 7434 \tTraining Loss: 0.087739\n",
      "Epoch: 7435 \tTraining Loss: 0.056929\n",
      "Epoch: 7436 \tTraining Loss: 0.052982\n",
      "Epoch: 7437 \tTraining Loss: 0.047569\n",
      "Epoch: 7438 \tTraining Loss: 0.047869\n",
      "Epoch: 7439 \tTraining Loss: 0.048910\n",
      "Epoch: 7440 \tTraining Loss: 0.050317\n",
      "Epoch: 7441 \tTraining Loss: 0.046046\n",
      "Epoch: 7442 \tTraining Loss: 0.047864\n",
      "Epoch: 7443 \tTraining Loss: 0.049260\n",
      "Epoch: 7444 \tTraining Loss: 0.048147\n",
      "Epoch: 7445 \tTraining Loss: 0.047761\n",
      "Epoch: 7446 \tTraining Loss: 0.047901\n",
      "Epoch: 7447 \tTraining Loss: 0.047635\n",
      "Epoch: 7448 \tTraining Loss: 0.048224\n",
      "Epoch: 7449 \tTraining Loss: 0.051519\n",
      "Epoch: 7450 \tTraining Loss: 0.050817\n",
      "Epoch: 7451 \tTraining Loss: 0.055659\n",
      "Epoch: 7452 \tTraining Loss: 0.047995\n",
      "Epoch: 7453 \tTraining Loss: 0.050926\n",
      "Epoch: 7454 \tTraining Loss: 0.054609\n",
      "Epoch: 7455 \tTraining Loss: 0.049415\n",
      "Epoch: 7456 \tTraining Loss: 0.047714\n",
      "Epoch: 7457 \tTraining Loss: 0.047822\n",
      "Epoch: 7458 \tTraining Loss: 0.047658\n",
      "Epoch: 7459 \tTraining Loss: 0.047630\n",
      "Epoch: 7460 \tTraining Loss: 0.047820\n",
      "Epoch: 7461 \tTraining Loss: 0.047662\n",
      "Epoch: 7462 \tTraining Loss: 0.047630\n",
      "Epoch: 7463 \tTraining Loss: 0.047613\n",
      "Epoch: 7464 \tTraining Loss: 0.047774\n",
      "Epoch: 7465 \tTraining Loss: 0.047644\n",
      "Epoch: 7466 \tTraining Loss: 0.047678\n",
      "Epoch: 7467 \tTraining Loss: 0.047681\n",
      "Epoch: 7468 \tTraining Loss: 0.047758\n",
      "Epoch: 7469 \tTraining Loss: 0.047595\n",
      "Epoch: 7470 \tTraining Loss: 0.047521\n",
      "Epoch: 7471 \tTraining Loss: 0.047706\n",
      "Epoch: 7472 \tTraining Loss: 0.047555\n",
      "Epoch: 7473 \tTraining Loss: 0.047567\n",
      "Epoch: 7474 \tTraining Loss: 0.047536\n",
      "Epoch: 7475 \tTraining Loss: 0.047658\n",
      "Epoch: 7476 \tTraining Loss: 0.047519\n",
      "Epoch: 7477 \tTraining Loss: 0.047527\n",
      "Epoch: 7478 \tTraining Loss: 0.047539\n",
      "Epoch: 7479 \tTraining Loss: 0.047481\n",
      "Epoch: 7480 \tTraining Loss: 0.047513\n",
      "Epoch: 7481 \tTraining Loss: 0.047561\n",
      "Epoch: 7482 \tTraining Loss: 0.053518\n",
      "Epoch: 7483 \tTraining Loss: 0.075866\n",
      "Epoch: 7484 \tTraining Loss: 0.065372\n",
      "Epoch: 7485 \tTraining Loss: 0.057849\n",
      "Epoch: 7486 \tTraining Loss: 0.045522\n",
      "Epoch: 7487 \tTraining Loss: 0.042534\n",
      "Epoch: 7488 \tTraining Loss: 0.052615\n",
      "Epoch: 7489 \tTraining Loss: 0.043765\n",
      "Epoch: 7490 \tTraining Loss: 0.042863\n",
      "Epoch: 7491 \tTraining Loss: 0.042716\n",
      "Epoch: 7492 \tTraining Loss: 0.042805\n",
      "Epoch: 7493 \tTraining Loss: 0.042726\n",
      "Epoch: 7494 \tTraining Loss: 0.042696\n",
      "Epoch: 7495 \tTraining Loss: 0.042705\n",
      "Epoch: 7496 \tTraining Loss: 0.042669\n",
      "Epoch: 7497 \tTraining Loss: 0.042711\n",
      "Epoch: 7498 \tTraining Loss: 0.042662\n",
      "Epoch: 7499 \tTraining Loss: 0.042724\n",
      "Epoch: 7500 \tTraining Loss: 0.042826\n",
      "Epoch: 7501 \tTraining Loss: 0.042671\n",
      "Epoch: 7502 \tTraining Loss: 0.042672\n",
      "Epoch: 7503 \tTraining Loss: 0.042661\n",
      "Epoch: 7504 \tTraining Loss: 0.042659\n",
      "Epoch: 7505 \tTraining Loss: 0.042660\n",
      "Epoch: 7506 \tTraining Loss: 0.042621\n",
      "Epoch: 7507 \tTraining Loss: 0.042638\n",
      "Epoch: 7508 \tTraining Loss: 0.042644\n",
      "Epoch: 7509 \tTraining Loss: 0.042660\n",
      "Epoch: 7510 \tTraining Loss: 0.042634\n",
      "Epoch: 7511 \tTraining Loss: 0.042633\n",
      "Epoch: 7512 \tTraining Loss: 0.042650\n",
      "Epoch: 7513 \tTraining Loss: 0.042619\n",
      "Epoch: 7514 \tTraining Loss: 0.042721\n",
      "Epoch: 7515 \tTraining Loss: 0.042616\n",
      "Epoch: 7516 \tTraining Loss: 0.042621\n",
      "Epoch: 7517 \tTraining Loss: 0.042642\n",
      "Epoch: 7518 \tTraining Loss: 0.042636\n",
      "Epoch: 7519 \tTraining Loss: 0.042657\n",
      "Epoch: 7520 \tTraining Loss: 0.042637\n",
      "Epoch: 7521 \tTraining Loss: 0.042604\n",
      "Epoch: 7522 \tTraining Loss: 0.042601\n",
      "Epoch: 7523 \tTraining Loss: 0.042610\n",
      "Epoch: 7524 \tTraining Loss: 0.042671\n",
      "Epoch: 7525 \tTraining Loss: 0.042583\n",
      "Epoch: 7526 \tTraining Loss: 0.042595\n",
      "Epoch: 7527 \tTraining Loss: 0.042590\n",
      "Epoch: 7528 \tTraining Loss: 0.042601\n",
      "Epoch: 7529 \tTraining Loss: 0.042625\n",
      "Epoch: 7530 \tTraining Loss: 0.042639\n",
      "Epoch: 7531 \tTraining Loss: 0.042611\n",
      "Epoch: 7532 \tTraining Loss: 0.042589\n",
      "Epoch: 7533 \tTraining Loss: 0.042638\n",
      "Epoch: 7534 \tTraining Loss: 0.042610\n",
      "Epoch: 7535 \tTraining Loss: 0.042602\n",
      "Epoch: 7536 \tTraining Loss: 0.042606\n",
      "Epoch: 7537 \tTraining Loss: 0.042603\n",
      "Epoch: 7538 \tTraining Loss: 0.042612\n",
      "Epoch: 7539 \tTraining Loss: 0.042606\n",
      "Epoch: 7540 \tTraining Loss: 0.042747\n",
      "Epoch: 7541 \tTraining Loss: 0.042564\n",
      "Epoch: 7542 \tTraining Loss: 0.042707\n",
      "Epoch: 7543 \tTraining Loss: 0.042618\n",
      "Epoch: 7544 \tTraining Loss: 0.042590\n",
      "Epoch: 7545 \tTraining Loss: 0.042660\n",
      "Epoch: 7546 \tTraining Loss: 0.042597\n",
      "Epoch: 7547 \tTraining Loss: 0.042593\n",
      "Epoch: 7548 \tTraining Loss: 0.042619\n",
      "Epoch: 7549 \tTraining Loss: 0.042574\n",
      "Epoch: 7550 \tTraining Loss: 0.042570\n",
      "Epoch: 7551 \tTraining Loss: 0.042595\n",
      "Epoch: 7552 \tTraining Loss: 0.042670\n",
      "Epoch: 7553 \tTraining Loss: 0.042575\n",
      "Epoch: 7554 \tTraining Loss: 0.042583\n",
      "Epoch: 7555 \tTraining Loss: 0.042632\n",
      "Epoch: 7556 \tTraining Loss: 0.042582\n",
      "Epoch: 7557 \tTraining Loss: 0.042745\n",
      "Epoch: 7558 \tTraining Loss: 0.042633\n",
      "Epoch: 7559 \tTraining Loss: 0.042590\n",
      "Epoch: 7560 \tTraining Loss: 0.042588\n",
      "Epoch: 7561 \tTraining Loss: 0.042818\n",
      "Epoch: 7562 \tTraining Loss: 0.042594\n",
      "Epoch: 7563 \tTraining Loss: 0.042635\n",
      "Epoch: 7564 \tTraining Loss: 0.042611\n",
      "Epoch: 7565 \tTraining Loss: 0.042590\n",
      "Epoch: 7566 \tTraining Loss: 0.042626\n",
      "Epoch: 7567 \tTraining Loss: 0.042612\n",
      "Epoch: 7568 \tTraining Loss: 0.042622\n",
      "Epoch: 7569 \tTraining Loss: 0.042622\n",
      "Epoch: 7570 \tTraining Loss: 0.042583\n",
      "Epoch: 7571 \tTraining Loss: 0.042578\n",
      "Epoch: 7572 \tTraining Loss: 0.042599\n",
      "Epoch: 7573 \tTraining Loss: 0.042573\n",
      "Epoch: 7574 \tTraining Loss: 0.042573\n",
      "Epoch: 7575 \tTraining Loss: 0.042576\n",
      "Epoch: 7576 \tTraining Loss: 0.042586\n",
      "Epoch: 7577 \tTraining Loss: 0.042597\n",
      "Epoch: 7578 \tTraining Loss: 0.042658\n",
      "Epoch: 7579 \tTraining Loss: 0.042588\n",
      "Epoch: 7580 \tTraining Loss: 0.042571\n",
      "Epoch: 7581 \tTraining Loss: 0.042657\n",
      "Epoch: 7582 \tTraining Loss: 0.042619\n",
      "Epoch: 7583 \tTraining Loss: 0.042556\n",
      "Epoch: 7584 \tTraining Loss: 0.042596\n",
      "Epoch: 7585 \tTraining Loss: 0.042574\n",
      "Epoch: 7586 \tTraining Loss: 0.042600\n",
      "Epoch: 7587 \tTraining Loss: 0.042559\n",
      "Epoch: 7588 \tTraining Loss: 0.042635\n",
      "Epoch: 7589 \tTraining Loss: 0.042571\n",
      "Epoch: 7590 \tTraining Loss: 0.042573\n",
      "Epoch: 7591 \tTraining Loss: 0.042577\n",
      "Epoch: 7592 \tTraining Loss: 0.042598\n",
      "Epoch: 7593 \tTraining Loss: 0.042578\n",
      "Epoch: 7594 \tTraining Loss: 0.042574\n",
      "Epoch: 7595 \tTraining Loss: 0.042560\n",
      "Epoch: 7596 \tTraining Loss: 0.042624\n",
      "Epoch: 7597 \tTraining Loss: 0.042594\n",
      "Epoch: 7598 \tTraining Loss: 0.042553\n",
      "Epoch: 7599 \tTraining Loss: 0.042557\n",
      "Epoch: 7600 \tTraining Loss: 0.042557\n",
      "Epoch: 7601 \tTraining Loss: 0.042625\n",
      "Epoch: 7602 \tTraining Loss: 0.042588\n",
      "Epoch: 7603 \tTraining Loss: 0.042564\n",
      "Epoch: 7604 \tTraining Loss: 0.042622\n",
      "Epoch: 7605 \tTraining Loss: 0.042593\n",
      "Epoch: 7606 \tTraining Loss: 0.042562\n",
      "Epoch: 7607 \tTraining Loss: 0.042577\n",
      "Epoch: 7608 \tTraining Loss: 0.042572\n",
      "Epoch: 7609 \tTraining Loss: 0.042570\n",
      "Epoch: 7610 \tTraining Loss: 0.042655\n",
      "Epoch: 7611 \tTraining Loss: 0.042633\n",
      "Epoch: 7612 \tTraining Loss: 0.042588\n",
      "Epoch: 7613 \tTraining Loss: 0.042581\n",
      "Epoch: 7614 \tTraining Loss: 0.042701\n",
      "Epoch: 7615 \tTraining Loss: 0.042597\n",
      "Epoch: 7616 \tTraining Loss: 0.042596\n",
      "Epoch: 7617 \tTraining Loss: 0.042566\n",
      "Epoch: 7618 \tTraining Loss: 0.042684\n",
      "Epoch: 7619 \tTraining Loss: 0.042568\n",
      "Epoch: 7620 \tTraining Loss: 0.042565\n",
      "Epoch: 7621 \tTraining Loss: 0.042578\n",
      "Epoch: 7622 \tTraining Loss: 0.042583\n",
      "Epoch: 7623 \tTraining Loss: 0.054272\n",
      "Epoch: 7624 \tTraining Loss: 0.096891\n",
      "Epoch: 7625 \tTraining Loss: 0.082660\n",
      "Epoch: 7626 \tTraining Loss: 0.088601\n",
      "Epoch: 7627 \tTraining Loss: 0.083539\n",
      "Epoch: 7628 \tTraining Loss: 0.069576\n",
      "Epoch: 7629 \tTraining Loss: 0.074722\n",
      "Epoch: 7630 \tTraining Loss: 0.059194\n",
      "Epoch: 7631 \tTraining Loss: 0.076402\n",
      "Epoch: 7632 \tTraining Loss: 0.048617\n",
      "Epoch: 7633 \tTraining Loss: 0.063446\n",
      "Epoch: 7634 \tTraining Loss: 0.087221\n",
      "Epoch: 7635 \tTraining Loss: 0.042280\n",
      "Epoch: 7636 \tTraining Loss: 0.042367\n",
      "Epoch: 7637 \tTraining Loss: 0.041076\n",
      "Epoch: 7638 \tTraining Loss: 0.040368\n",
      "Epoch: 7639 \tTraining Loss: 0.040071\n",
      "Epoch: 7640 \tTraining Loss: 0.039138\n",
      "Epoch: 7641 \tTraining Loss: 0.038340\n",
      "Epoch: 7642 \tTraining Loss: 0.038092\n",
      "Epoch: 7643 \tTraining Loss: 0.038199\n",
      "Epoch: 7644 \tTraining Loss: 0.037434\n",
      "Epoch: 7645 \tTraining Loss: 0.036789\n",
      "Epoch: 7646 \tTraining Loss: 0.036728\n",
      "Epoch: 7647 \tTraining Loss: 0.036670\n",
      "Epoch: 7648 \tTraining Loss: 0.036694\n",
      "Epoch: 7649 \tTraining Loss: 0.036660\n",
      "Epoch: 7650 \tTraining Loss: 0.036660\n",
      "Epoch: 7651 \tTraining Loss: 0.036668\n",
      "Epoch: 7652 \tTraining Loss: 0.036655\n",
      "Epoch: 7653 \tTraining Loss: 0.036644\n",
      "Epoch: 7654 \tTraining Loss: 0.036679\n",
      "Epoch: 7655 \tTraining Loss: 0.036644\n",
      "Epoch: 7656 \tTraining Loss: 0.036672\n",
      "Epoch: 7657 \tTraining Loss: 0.036637\n",
      "Epoch: 7658 \tTraining Loss: 0.036629\n",
      "Epoch: 7659 \tTraining Loss: 0.036639\n",
      "Epoch: 7660 \tTraining Loss: 0.036649\n",
      "Epoch: 7661 \tTraining Loss: 0.036644\n",
      "Epoch: 7662 \tTraining Loss: 0.036625\n",
      "Epoch: 7663 \tTraining Loss: 0.036634\n",
      "Epoch: 7664 \tTraining Loss: 0.036691\n",
      "Epoch: 7665 \tTraining Loss: 0.036669\n",
      "Epoch: 7666 \tTraining Loss: 0.036637\n",
      "Epoch: 7667 \tTraining Loss: 0.036634\n",
      "Epoch: 7668 \tTraining Loss: 0.036642\n",
      "Epoch: 7669 \tTraining Loss: 0.036636\n",
      "Epoch: 7670 \tTraining Loss: 0.036632\n",
      "Epoch: 7671 \tTraining Loss: 0.036624\n",
      "Epoch: 7672 \tTraining Loss: 0.036664\n",
      "Epoch: 7673 \tTraining Loss: 0.036700\n",
      "Epoch: 7674 \tTraining Loss: 0.036624\n",
      "Epoch: 7675 \tTraining Loss: 0.036619\n",
      "Epoch: 7676 \tTraining Loss: 0.036680\n",
      "Epoch: 7677 \tTraining Loss: 0.036619\n",
      "Epoch: 7678 \tTraining Loss: 0.036642\n",
      "Epoch: 7679 \tTraining Loss: 0.036673\n",
      "Epoch: 7680 \tTraining Loss: 0.036630\n",
      "Epoch: 7681 \tTraining Loss: 0.036655\n",
      "Epoch: 7682 \tTraining Loss: 0.036673\n",
      "Epoch: 7683 \tTraining Loss: 0.036611\n",
      "Epoch: 7684 \tTraining Loss: 0.036648\n",
      "Epoch: 7685 \tTraining Loss: 0.036638\n",
      "Epoch: 7686 \tTraining Loss: 0.036621\n",
      "Epoch: 7687 \tTraining Loss: 0.036631\n",
      "Epoch: 7688 \tTraining Loss: 0.036619\n",
      "Epoch: 7689 \tTraining Loss: 0.036639\n",
      "Epoch: 7690 \tTraining Loss: 0.036647\n",
      "Epoch: 7691 \tTraining Loss: 0.036671\n",
      "Epoch: 7692 \tTraining Loss: 0.036635\n",
      "Epoch: 7693 \tTraining Loss: 0.036762\n",
      "Epoch: 7694 \tTraining Loss: 0.036557\n",
      "Epoch: 7695 \tTraining Loss: 0.036680\n",
      "Epoch: 7696 \tTraining Loss: 0.036643\n",
      "Epoch: 7697 \tTraining Loss: 0.036629\n",
      "Epoch: 7698 \tTraining Loss: 0.036621\n",
      "Epoch: 7699 \tTraining Loss: 0.036667\n",
      "Epoch: 7700 \tTraining Loss: 0.036655\n",
      "Epoch: 7701 \tTraining Loss: 0.036625\n",
      "Epoch: 7702 \tTraining Loss: 0.036724\n",
      "Epoch: 7703 \tTraining Loss: 0.036653\n",
      "Epoch: 7704 \tTraining Loss: 0.036639\n",
      "Epoch: 7705 \tTraining Loss: 0.036617\n",
      "Epoch: 7706 \tTraining Loss: 0.036655\n",
      "Epoch: 7707 \tTraining Loss: 0.036658\n",
      "Epoch: 7708 \tTraining Loss: 0.036628\n",
      "Epoch: 7709 \tTraining Loss: 0.036635\n",
      "Epoch: 7710 \tTraining Loss: 0.036736\n",
      "Epoch: 7711 \tTraining Loss: 0.036658\n",
      "Epoch: 7712 \tTraining Loss: 0.036644\n",
      "Epoch: 7713 \tTraining Loss: 0.036640\n",
      "Epoch: 7714 \tTraining Loss: 0.036622\n",
      "Epoch: 7715 \tTraining Loss: 0.036666\n",
      "Epoch: 7716 \tTraining Loss: 0.036650\n",
      "Epoch: 7717 \tTraining Loss: 0.036623\n",
      "Epoch: 7718 \tTraining Loss: 0.036624\n",
      "Epoch: 7719 \tTraining Loss: 0.036715\n",
      "Epoch: 7720 \tTraining Loss: 0.036688\n",
      "Epoch: 7721 \tTraining Loss: 0.036676\n",
      "Epoch: 7722 \tTraining Loss: 0.036626\n",
      "Epoch: 7723 \tTraining Loss: 0.036646\n",
      "Epoch: 7724 \tTraining Loss: 0.036642\n",
      "Epoch: 7725 \tTraining Loss: 0.036630\n",
      "Epoch: 7726 \tTraining Loss: 0.036614\n",
      "Epoch: 7727 \tTraining Loss: 0.036660\n",
      "Epoch: 7728 \tTraining Loss: 0.036632\n",
      "Epoch: 7729 \tTraining Loss: 0.036618\n",
      "Epoch: 7730 \tTraining Loss: 0.036618\n",
      "Epoch: 7731 \tTraining Loss: 0.036659\n",
      "Epoch: 7732 \tTraining Loss: 0.036677\n",
      "Epoch: 7733 \tTraining Loss: 0.036621\n",
      "Epoch: 7734 \tTraining Loss: 0.036633\n",
      "Epoch: 7735 \tTraining Loss: 0.036653\n",
      "Epoch: 7736 \tTraining Loss: 0.036624\n",
      "Epoch: 7737 \tTraining Loss: 0.036622\n",
      "Epoch: 7738 \tTraining Loss: 0.036668\n",
      "Epoch: 7739 \tTraining Loss: 0.036698\n",
      "Epoch: 7740 \tTraining Loss: 0.036623\n",
      "Epoch: 7741 \tTraining Loss: 0.036593\n",
      "Epoch: 7742 \tTraining Loss: 0.036641\n",
      "Epoch: 7743 \tTraining Loss: 0.036633\n",
      "Epoch: 7744 \tTraining Loss: 0.036666\n",
      "Epoch: 7745 \tTraining Loss: 0.037187\n",
      "Epoch: 7746 \tTraining Loss: 0.037034\n",
      "Epoch: 7747 \tTraining Loss: 0.036657\n",
      "Epoch: 7748 \tTraining Loss: 0.036618\n",
      "Epoch: 7749 \tTraining Loss: 0.036631\n",
      "Epoch: 7750 \tTraining Loss: 0.036654\n",
      "Epoch: 7751 \tTraining Loss: 0.036705\n",
      "Epoch: 7752 \tTraining Loss: 0.036718\n",
      "Epoch: 7753 \tTraining Loss: 0.036671\n",
      "Epoch: 7754 \tTraining Loss: 0.036683\n",
      "Epoch: 7755 \tTraining Loss: 0.036635\n",
      "Epoch: 7756 \tTraining Loss: 0.036729\n",
      "Epoch: 7757 \tTraining Loss: 0.036621\n",
      "Epoch: 7758 \tTraining Loss: 0.036619\n",
      "Epoch: 7759 \tTraining Loss: 0.036632\n",
      "Epoch: 7760 \tTraining Loss: 0.036649\n",
      "Epoch: 7761 \tTraining Loss: 0.036635\n",
      "Epoch: 7762 \tTraining Loss: 0.036627\n",
      "Epoch: 7763 \tTraining Loss: 0.036654\n",
      "Epoch: 7764 \tTraining Loss: 0.036707\n",
      "Epoch: 7765 \tTraining Loss: 0.036657\n",
      "Epoch: 7766 \tTraining Loss: 0.036681\n",
      "Epoch: 7767 \tTraining Loss: 0.036637\n",
      "Epoch: 7768 \tTraining Loss: 0.036630\n",
      "Epoch: 7769 \tTraining Loss: 0.036651\n",
      "Epoch: 7770 \tTraining Loss: 0.036647\n",
      "Epoch: 7771 \tTraining Loss: 0.036621\n",
      "Epoch: 7772 \tTraining Loss: 0.036646\n",
      "Epoch: 7773 \tTraining Loss: 0.036677\n",
      "Epoch: 7774 \tTraining Loss: 0.036678\n",
      "Epoch: 7775 \tTraining Loss: 0.036648\n",
      "Epoch: 7776 \tTraining Loss: 0.036629\n",
      "Epoch: 7777 \tTraining Loss: 0.036675\n",
      "Epoch: 7778 \tTraining Loss: 0.036625\n",
      "Epoch: 7779 \tTraining Loss: 0.036624\n",
      "Epoch: 7780 \tTraining Loss: 0.036604\n",
      "Epoch: 7781 \tTraining Loss: 0.036684\n",
      "Epoch: 7782 \tTraining Loss: 0.036646\n",
      "Epoch: 7783 \tTraining Loss: 0.036643\n",
      "Epoch: 7784 \tTraining Loss: 0.036639\n",
      "Epoch: 7785 \tTraining Loss: 0.036630\n",
      "Epoch: 7786 \tTraining Loss: 0.036656\n",
      "Epoch: 7787 \tTraining Loss: 0.036645\n",
      "Epoch: 7788 \tTraining Loss: 0.036653\n",
      "Epoch: 7789 \tTraining Loss: 0.036761\n",
      "Epoch: 7790 \tTraining Loss: 0.036681\n",
      "Epoch: 7791 \tTraining Loss: 0.036630\n",
      "Epoch: 7792 \tTraining Loss: 0.036634\n",
      "Epoch: 7793 \tTraining Loss: 0.036660\n",
      "Epoch: 7794 \tTraining Loss: 0.036665\n",
      "Epoch: 7795 \tTraining Loss: 0.036650\n",
      "Epoch: 7796 \tTraining Loss: 0.036619\n",
      "Epoch: 7797 \tTraining Loss: 0.036637\n",
      "Epoch: 7798 \tTraining Loss: 0.036640\n",
      "Epoch: 7799 \tTraining Loss: 0.036683\n",
      "Epoch: 7800 \tTraining Loss: 0.036658\n",
      "Epoch: 7801 \tTraining Loss: 0.036641\n",
      "Epoch: 7802 \tTraining Loss: 0.036635\n",
      "Epoch: 7803 \tTraining Loss: 0.036636\n",
      "Epoch: 7804 \tTraining Loss: 0.036640\n",
      "Epoch: 7805 \tTraining Loss: 0.036618\n",
      "Epoch: 7806 \tTraining Loss: 0.036621\n",
      "Epoch: 7807 \tTraining Loss: 0.036691\n",
      "Epoch: 7808 \tTraining Loss: 0.036644\n",
      "Epoch: 7809 \tTraining Loss: 0.036739\n",
      "Epoch: 7810 \tTraining Loss: 0.036636\n",
      "Epoch: 7811 \tTraining Loss: 0.036619\n",
      "Epoch: 7812 \tTraining Loss: 0.036649\n",
      "Epoch: 7813 \tTraining Loss: 0.036623\n",
      "Epoch: 7814 \tTraining Loss: 0.036637\n",
      "Epoch: 7815 \tTraining Loss: 0.036625\n",
      "Epoch: 7816 \tTraining Loss: 0.036617\n",
      "Epoch: 7817 \tTraining Loss: 0.036626\n",
      "Epoch: 7818 \tTraining Loss: 0.036617\n",
      "Epoch: 7819 \tTraining Loss: 0.036733\n",
      "Epoch: 7820 \tTraining Loss: 0.036636\n",
      "Epoch: 7821 \tTraining Loss: 0.036617\n",
      "Epoch: 7822 \tTraining Loss: 0.036632\n",
      "Epoch: 7823 \tTraining Loss: 0.036621\n",
      "Epoch: 7824 \tTraining Loss: 0.036655\n",
      "Epoch: 7825 \tTraining Loss: 0.036616\n",
      "Epoch: 7826 \tTraining Loss: 0.036607\n",
      "Epoch: 7827 \tTraining Loss: 0.036601\n",
      "Epoch: 7828 \tTraining Loss: 0.036684\n",
      "Epoch: 7829 \tTraining Loss: 0.036693\n",
      "Epoch: 7830 \tTraining Loss: 0.036643\n",
      "Epoch: 7831 \tTraining Loss: 0.036690\n",
      "Epoch: 7832 \tTraining Loss: 0.036595\n",
      "Epoch: 7833 \tTraining Loss: 0.036607\n",
      "Epoch: 7834 \tTraining Loss: 0.036637\n",
      "Epoch: 7835 \tTraining Loss: 0.036636\n",
      "Epoch: 7836 \tTraining Loss: 0.036611\n",
      "Epoch: 7837 \tTraining Loss: 0.036683\n",
      "Epoch: 7838 \tTraining Loss: 0.036613\n",
      "Epoch: 7839 \tTraining Loss: 0.036632\n",
      "Epoch: 7840 \tTraining Loss: 0.036634\n",
      "Epoch: 7841 \tTraining Loss: 0.036804\n",
      "Epoch: 7842 \tTraining Loss: 0.036611\n",
      "Epoch: 7843 \tTraining Loss: 0.036655\n",
      "Epoch: 7844 \tTraining Loss: 0.036652\n",
      "Epoch: 7845 \tTraining Loss: 0.036625\n",
      "Epoch: 7846 \tTraining Loss: 0.036649\n",
      "Epoch: 7847 \tTraining Loss: 0.036641\n",
      "Epoch: 7848 \tTraining Loss: 0.036635\n",
      "Epoch: 7849 \tTraining Loss: 0.036609\n",
      "Epoch: 7850 \tTraining Loss: 0.036645\n",
      "Epoch: 7851 \tTraining Loss: 0.036608\n",
      "Epoch: 7852 \tTraining Loss: 0.036650\n",
      "Epoch: 7853 \tTraining Loss: 0.036636\n",
      "Epoch: 7854 \tTraining Loss: 0.036619\n",
      "Epoch: 7855 \tTraining Loss: 0.036722\n",
      "Epoch: 7856 \tTraining Loss: 0.036623\n",
      "Epoch: 7857 \tTraining Loss: 0.036685\n",
      "Epoch: 7858 \tTraining Loss: 0.036606\n",
      "Epoch: 7859 \tTraining Loss: 0.036702\n",
      "Epoch: 7860 \tTraining Loss: 0.036673\n",
      "Epoch: 7861 \tTraining Loss: 0.036615\n",
      "Epoch: 7862 \tTraining Loss: 0.036668\n",
      "Epoch: 7863 \tTraining Loss: 0.036609\n",
      "Epoch: 7864 \tTraining Loss: 0.036754\n",
      "Epoch: 7865 \tTraining Loss: 0.036618\n",
      "Epoch: 7866 \tTraining Loss: 0.036661\n",
      "Epoch: 7867 \tTraining Loss: 0.036626\n",
      "Epoch: 7868 \tTraining Loss: 0.036658\n",
      "Epoch: 7869 \tTraining Loss: 0.036619\n",
      "Epoch: 7870 \tTraining Loss: 0.036635\n",
      "Epoch: 7871 \tTraining Loss: 0.036636\n",
      "Epoch: 7872 \tTraining Loss: 0.036716\n",
      "Epoch: 7873 \tTraining Loss: 0.036604\n",
      "Epoch: 7874 \tTraining Loss: 0.036613\n",
      "Epoch: 7875 \tTraining Loss: 0.036616\n",
      "Epoch: 7876 \tTraining Loss: 0.036672\n",
      "Epoch: 7877 \tTraining Loss: 0.036646\n",
      "Epoch: 7878 \tTraining Loss: 0.036672\n",
      "Epoch: 7879 \tTraining Loss: 0.036620\n",
      "Epoch: 7880 \tTraining Loss: 0.036613\n",
      "Epoch: 7881 \tTraining Loss: 0.036667\n",
      "Epoch: 7882 \tTraining Loss: 0.036621\n",
      "Epoch: 7883 \tTraining Loss: 0.036619\n",
      "Epoch: 7884 \tTraining Loss: 0.036693\n",
      "Epoch: 7885 \tTraining Loss: 0.036611\n",
      "Epoch: 7886 \tTraining Loss: 0.036650\n",
      "Epoch: 7887 \tTraining Loss: 0.036620\n",
      "Epoch: 7888 \tTraining Loss: 0.036795\n",
      "Epoch: 7889 \tTraining Loss: 0.036630\n",
      "Epoch: 7890 \tTraining Loss: 0.036641\n",
      "Epoch: 7891 \tTraining Loss: 0.036665\n",
      "Epoch: 7892 \tTraining Loss: 0.036620\n",
      "Epoch: 7893 \tTraining Loss: 0.036641\n",
      "Epoch: 7894 \tTraining Loss: 0.036690\n",
      "Epoch: 7895 \tTraining Loss: 0.036624\n",
      "Epoch: 7896 \tTraining Loss: 0.036619\n",
      "Epoch: 7897 \tTraining Loss: 0.036622\n",
      "Epoch: 7898 \tTraining Loss: 0.036629\n",
      "Epoch: 7899 \tTraining Loss: 0.036611\n",
      "Epoch: 7900 \tTraining Loss: 0.036603\n",
      "Epoch: 7901 \tTraining Loss: 0.036617\n",
      "Epoch: 7902 \tTraining Loss: 0.036614\n",
      "Epoch: 7903 \tTraining Loss: 0.036644\n",
      "Epoch: 7904 \tTraining Loss: 0.036621\n",
      "Epoch: 7905 \tTraining Loss: 0.036624\n",
      "Epoch: 7906 \tTraining Loss: 0.036691\n",
      "Epoch: 7907 \tTraining Loss: 0.036604\n",
      "Epoch: 7908 \tTraining Loss: 0.036633\n",
      "Epoch: 7909 \tTraining Loss: 0.036611\n",
      "Epoch: 7910 \tTraining Loss: 0.036800\n",
      "Epoch: 7911 \tTraining Loss: 0.036658\n",
      "Epoch: 7912 \tTraining Loss: 0.036698\n",
      "Epoch: 7913 \tTraining Loss: 0.036629\n",
      "Epoch: 7914 \tTraining Loss: 0.036623\n",
      "Epoch: 7915 \tTraining Loss: 0.036655\n",
      "Epoch: 7916 \tTraining Loss: 0.036663\n",
      "Epoch: 7917 \tTraining Loss: 0.036608\n",
      "Epoch: 7918 \tTraining Loss: 0.036603\n",
      "Epoch: 7919 \tTraining Loss: 0.036609\n",
      "Epoch: 7920 \tTraining Loss: 0.036656\n",
      "Epoch: 7921 \tTraining Loss: 0.036662\n",
      "Epoch: 7922 \tTraining Loss: 0.036608\n",
      "Epoch: 7923 \tTraining Loss: 0.036607\n",
      "Epoch: 7924 \tTraining Loss: 0.036785\n",
      "Epoch: 7925 \tTraining Loss: 0.036622\n",
      "Epoch: 7926 \tTraining Loss: 0.036596\n",
      "Epoch: 7927 \tTraining Loss: 0.036627\n",
      "Epoch: 7928 \tTraining Loss: 0.036597\n",
      "Epoch: 7929 \tTraining Loss: 0.036612\n",
      "Epoch: 7930 \tTraining Loss: 0.036622\n",
      "Epoch: 7931 \tTraining Loss: 0.036635\n",
      "Epoch: 7932 \tTraining Loss: 0.036603\n",
      "Epoch: 7933 \tTraining Loss: 0.036634\n",
      "Epoch: 7934 \tTraining Loss: 0.036615\n",
      "Epoch: 7935 \tTraining Loss: 0.036633\n",
      "Epoch: 7936 \tTraining Loss: 0.036627\n",
      "Epoch: 7937 \tTraining Loss: 0.036645\n",
      "Epoch: 7938 \tTraining Loss: 0.036608\n",
      "Epoch: 7939 \tTraining Loss: 0.036605\n",
      "Epoch: 7940 \tTraining Loss: 0.036625\n",
      "Epoch: 7941 \tTraining Loss: 0.036662\n",
      "Epoch: 7942 \tTraining Loss: 0.036617\n",
      "Epoch: 7943 \tTraining Loss: 0.036632\n",
      "Epoch: 7944 \tTraining Loss: 0.036615\n",
      "Epoch: 7945 \tTraining Loss: 0.036685\n",
      "Epoch: 7946 \tTraining Loss: 0.036619\n",
      "Epoch: 7947 \tTraining Loss: 0.036598\n",
      "Epoch: 7948 \tTraining Loss: 0.036612\n",
      "Epoch: 7949 \tTraining Loss: 0.036618\n",
      "Epoch: 7950 \tTraining Loss: 0.036677\n",
      "Epoch: 7951 \tTraining Loss: 0.036619\n",
      "Epoch: 7952 \tTraining Loss: 0.036615\n",
      "Epoch: 7953 \tTraining Loss: 0.036657\n",
      "Epoch: 7954 \tTraining Loss: 0.036606\n",
      "Epoch: 7955 \tTraining Loss: 0.036641\n",
      "Epoch: 7956 \tTraining Loss: 0.036641\n",
      "Epoch: 7957 \tTraining Loss: 0.036631\n",
      "Epoch: 7958 \tTraining Loss: 0.036618\n",
      "Epoch: 7959 \tTraining Loss: 0.036602\n",
      "Epoch: 7960 \tTraining Loss: 0.036633\n",
      "Epoch: 7961 \tTraining Loss: 0.036650\n",
      "Epoch: 7962 \tTraining Loss: 0.036736\n",
      "Epoch: 7963 \tTraining Loss: 0.036605\n",
      "Epoch: 7964 \tTraining Loss: 0.036814\n",
      "Epoch: 7965 \tTraining Loss: 0.036604\n",
      "Epoch: 7966 \tTraining Loss: 0.036601\n",
      "Epoch: 7967 \tTraining Loss: 0.036653\n",
      "Epoch: 7968 \tTraining Loss: 0.036626\n",
      "Epoch: 7969 \tTraining Loss: 0.036594\n",
      "Epoch: 7970 \tTraining Loss: 0.036606\n",
      "Epoch: 7971 \tTraining Loss: 0.036597\n",
      "Epoch: 7972 \tTraining Loss: 0.036672\n",
      "Epoch: 7973 \tTraining Loss: 0.036621\n",
      "Epoch: 7974 \tTraining Loss: 0.036632\n",
      "Epoch: 7975 \tTraining Loss: 0.036627\n",
      "Epoch: 7976 \tTraining Loss: 0.036600\n",
      "Epoch: 7977 \tTraining Loss: 0.036620\n",
      "Epoch: 7978 \tTraining Loss: 0.036601\n",
      "Epoch: 7979 \tTraining Loss: 0.036633\n",
      "Epoch: 7980 \tTraining Loss: 0.036678\n",
      "Epoch: 7981 \tTraining Loss: 0.036658\n",
      "Epoch: 7982 \tTraining Loss: 0.036725\n",
      "Epoch: 7983 \tTraining Loss: 0.036618\n",
      "Epoch: 7984 \tTraining Loss: 0.036618\n",
      "Epoch: 7985 \tTraining Loss: 0.036595\n",
      "Epoch: 7986 \tTraining Loss: 0.036608\n",
      "Epoch: 7987 \tTraining Loss: 0.036629\n",
      "Epoch: 7988 \tTraining Loss: 0.036614\n",
      "Epoch: 7989 \tTraining Loss: 0.036658\n",
      "Epoch: 7990 \tTraining Loss: 0.036668\n",
      "Epoch: 7991 \tTraining Loss: 0.036620\n",
      "Epoch: 7992 \tTraining Loss: 0.036598\n",
      "Epoch: 7993 \tTraining Loss: 0.036607\n",
      "Epoch: 7994 \tTraining Loss: 0.062157\n",
      "Epoch: 7995 \tTraining Loss: 0.123528\n",
      "Epoch: 7996 \tTraining Loss: 0.321225\n",
      "Epoch: 7997 \tTraining Loss: 0.237798\n",
      "Epoch: 7998 \tTraining Loss: 0.137042\n",
      "Epoch: 7999 \tTraining Loss: 0.076352\n",
      "Epoch: 8000 \tTraining Loss: 0.049886\n",
      "Epoch: 8001 \tTraining Loss: 0.052241\n",
      "Epoch: 8002 \tTraining Loss: 0.047314\n",
      "Epoch: 8003 \tTraining Loss: 0.047781\n",
      "Epoch: 8004 \tTraining Loss: 0.046987\n",
      "Epoch: 8005 \tTraining Loss: 0.046615\n",
      "Epoch: 8006 \tTraining Loss: 0.046452\n",
      "Epoch: 8007 \tTraining Loss: 0.046297\n",
      "Epoch: 8008 \tTraining Loss: 0.046305\n",
      "Epoch: 8009 \tTraining Loss: 0.046223\n",
      "Epoch: 8010 \tTraining Loss: 0.046251\n",
      "Epoch: 8011 \tTraining Loss: 0.046228\n",
      "Epoch: 8012 \tTraining Loss: 0.046225\n",
      "Epoch: 8013 \tTraining Loss: 0.046412\n",
      "Epoch: 8014 \tTraining Loss: 0.046330\n",
      "Epoch: 8015 \tTraining Loss: 0.046362\n",
      "Epoch: 8016 \tTraining Loss: 0.046285\n",
      "Epoch: 8017 \tTraining Loss: 0.046273\n",
      "Epoch: 8018 \tTraining Loss: 0.046374\n",
      "Epoch: 8019 \tTraining Loss: 0.046218\n",
      "Epoch: 8020 \tTraining Loss: 0.046220\n",
      "Epoch: 8021 \tTraining Loss: 0.046171\n",
      "Epoch: 8022 \tTraining Loss: 0.046200\n",
      "Epoch: 8023 \tTraining Loss: 0.045111\n",
      "Epoch: 8024 \tTraining Loss: 0.045029\n",
      "Epoch: 8025 \tTraining Loss: 0.045037\n",
      "Epoch: 8026 \tTraining Loss: 0.045015\n",
      "Epoch: 8027 \tTraining Loss: 0.044988\n",
      "Epoch: 8028 \tTraining Loss: 0.044989\n",
      "Epoch: 8029 \tTraining Loss: 0.045003\n",
      "Epoch: 8030 \tTraining Loss: 0.044989\n",
      "Epoch: 8031 \tTraining Loss: 0.045000\n",
      "Epoch: 8032 \tTraining Loss: 0.045095\n",
      "Epoch: 8033 \tTraining Loss: 0.044949\n",
      "Epoch: 8034 \tTraining Loss: 0.044968\n",
      "Epoch: 8035 \tTraining Loss: 0.044996\n",
      "Epoch: 8036 \tTraining Loss: 0.044997\n",
      "Epoch: 8037 \tTraining Loss: 0.044948\n",
      "Epoch: 8038 \tTraining Loss: 0.045005\n",
      "Epoch: 8039 \tTraining Loss: 0.044951\n",
      "Epoch: 8040 \tTraining Loss: 0.045187\n",
      "Epoch: 8041 \tTraining Loss: 0.044984\n",
      "Epoch: 8042 \tTraining Loss: 0.044993\n",
      "Epoch: 8043 \tTraining Loss: 0.044958\n",
      "Epoch: 8044 \tTraining Loss: 0.044955\n",
      "Epoch: 8045 \tTraining Loss: 0.044945\n",
      "Epoch: 8046 \tTraining Loss: 0.044982\n",
      "Epoch: 8047 \tTraining Loss: 0.044947\n",
      "Epoch: 8048 \tTraining Loss: 0.044938\n",
      "Epoch: 8049 \tTraining Loss: 0.044986\n",
      "Epoch: 8050 \tTraining Loss: 0.044965\n",
      "Epoch: 8051 \tTraining Loss: 0.044961\n",
      "Epoch: 8052 \tTraining Loss: 0.044994\n",
      "Epoch: 8053 \tTraining Loss: 0.044942\n",
      "Epoch: 8054 \tTraining Loss: 0.044965\n",
      "Epoch: 8055 \tTraining Loss: 0.044966\n",
      "Epoch: 8056 \tTraining Loss: 0.044942\n",
      "Epoch: 8057 \tTraining Loss: 0.044961\n",
      "Epoch: 8058 \tTraining Loss: 0.044992\n",
      "Epoch: 8059 \tTraining Loss: 0.044979\n",
      "Epoch: 8060 \tTraining Loss: 0.044918\n",
      "Epoch: 8061 \tTraining Loss: 0.044978\n",
      "Epoch: 8062 \tTraining Loss: 0.044927\n",
      "Epoch: 8063 \tTraining Loss: 0.044931\n",
      "Epoch: 8064 \tTraining Loss: 0.044944\n",
      "Epoch: 8065 \tTraining Loss: 0.044972\n",
      "Epoch: 8066 \tTraining Loss: 0.044904\n",
      "Epoch: 8067 \tTraining Loss: 0.044950\n",
      "Epoch: 8068 \tTraining Loss: 0.044935\n",
      "Epoch: 8069 \tTraining Loss: 0.044938\n",
      "Epoch: 8070 \tTraining Loss: 0.045016\n",
      "Epoch: 8071 \tTraining Loss: 0.044966\n",
      "Epoch: 8072 \tTraining Loss: 0.044950\n",
      "Epoch: 8073 \tTraining Loss: 0.044933\n",
      "Epoch: 8074 \tTraining Loss: 0.044924\n",
      "Epoch: 8075 \tTraining Loss: 0.044950\n",
      "Epoch: 8076 \tTraining Loss: 0.044933\n",
      "Epoch: 8077 \tTraining Loss: 0.044963\n",
      "Epoch: 8078 \tTraining Loss: 0.044937\n",
      "Epoch: 8079 \tTraining Loss: 0.044962\n",
      "Epoch: 8080 \tTraining Loss: 0.044932\n",
      "Epoch: 8081 \tTraining Loss: 0.044944\n",
      "Epoch: 8082 \tTraining Loss: 0.044944\n",
      "Epoch: 8083 \tTraining Loss: 0.044963\n",
      "Epoch: 8084 \tTraining Loss: 0.044971\n",
      "Epoch: 8085 \tTraining Loss: 0.044960\n",
      "Epoch: 8086 \tTraining Loss: 0.044963\n",
      "Epoch: 8087 \tTraining Loss: 0.044974\n",
      "Epoch: 8088 \tTraining Loss: 0.044954\n",
      "Epoch: 8089 \tTraining Loss: 0.045057\n",
      "Epoch: 8090 \tTraining Loss: 0.044984\n",
      "Epoch: 8091 \tTraining Loss: 0.044950\n",
      "Epoch: 8092 \tTraining Loss: 0.044930\n",
      "Epoch: 8093 \tTraining Loss: 0.044966\n",
      "Epoch: 8094 \tTraining Loss: 0.044941\n",
      "Epoch: 8095 \tTraining Loss: 0.044928\n",
      "Epoch: 8096 \tTraining Loss: 0.044925\n",
      "Epoch: 8097 \tTraining Loss: 0.044934\n",
      "Epoch: 8098 \tTraining Loss: 0.044919\n",
      "Epoch: 8099 \tTraining Loss: 0.044929\n",
      "Epoch: 8100 \tTraining Loss: 0.044969\n",
      "Epoch: 8101 \tTraining Loss: 0.044935\n",
      "Epoch: 8102 \tTraining Loss: 0.045012\n",
      "Epoch: 8103 \tTraining Loss: 0.044935\n",
      "Epoch: 8104 \tTraining Loss: 0.044980\n",
      "Epoch: 8105 \tTraining Loss: 0.044937\n",
      "Epoch: 8106 \tTraining Loss: 0.044915\n",
      "Epoch: 8107 \tTraining Loss: 0.044943\n",
      "Epoch: 8108 \tTraining Loss: 0.044950\n",
      "Epoch: 8109 \tTraining Loss: 0.044928\n",
      "Epoch: 8110 \tTraining Loss: 0.044948\n",
      "Epoch: 8111 \tTraining Loss: 0.044961\n",
      "Epoch: 8112 \tTraining Loss: 0.044957\n",
      "Epoch: 8113 \tTraining Loss: 0.045115\n",
      "Epoch: 8114 \tTraining Loss: 0.044934\n",
      "Epoch: 8115 \tTraining Loss: 0.044071\n",
      "Epoch: 8116 \tTraining Loss: 0.044136\n",
      "Epoch: 8117 \tTraining Loss: 0.043960\n",
      "Epoch: 8118 \tTraining Loss: 0.043873\n",
      "Epoch: 8119 \tTraining Loss: 0.043832\n",
      "Epoch: 8120 \tTraining Loss: 0.043827\n",
      "Epoch: 8121 \tTraining Loss: 0.043800\n",
      "Epoch: 8122 \tTraining Loss: 0.043791\n",
      "Epoch: 8123 \tTraining Loss: 0.043815\n",
      "Epoch: 8124 \tTraining Loss: 0.043771\n",
      "Epoch: 8125 \tTraining Loss: 0.043744\n",
      "Epoch: 8126 \tTraining Loss: 0.043792\n",
      "Epoch: 8127 \tTraining Loss: 0.043754\n",
      "Epoch: 8128 \tTraining Loss: 0.043766\n",
      "Epoch: 8129 \tTraining Loss: 0.043723\n",
      "Epoch: 8130 \tTraining Loss: 0.043860\n",
      "Epoch: 8131 \tTraining Loss: 0.043787\n",
      "Epoch: 8132 \tTraining Loss: 0.043819\n",
      "Epoch: 8133 \tTraining Loss: 0.043748\n",
      "Epoch: 8134 \tTraining Loss: 0.043768\n",
      "Epoch: 8135 \tTraining Loss: 0.043761\n",
      "Epoch: 8136 \tTraining Loss: 0.043803\n",
      "Epoch: 8137 \tTraining Loss: 0.043744\n",
      "Epoch: 8138 \tTraining Loss: 0.043768\n",
      "Epoch: 8139 \tTraining Loss: 0.043777\n",
      "Epoch: 8140 \tTraining Loss: 0.043802\n",
      "Epoch: 8141 \tTraining Loss: 0.043756\n",
      "Epoch: 8142 \tTraining Loss: 0.043735\n",
      "Epoch: 8143 \tTraining Loss: 0.043871\n",
      "Epoch: 8144 \tTraining Loss: 0.043727\n",
      "Epoch: 8145 \tTraining Loss: 0.043761\n",
      "Epoch: 8146 \tTraining Loss: 0.043735\n",
      "Epoch: 8147 \tTraining Loss: 0.043745\n",
      "Epoch: 8148 \tTraining Loss: 0.043746\n",
      "Epoch: 8149 \tTraining Loss: 0.043720\n",
      "Epoch: 8150 \tTraining Loss: 0.043777\n",
      "Epoch: 8151 \tTraining Loss: 0.043739\n",
      "Epoch: 8152 \tTraining Loss: 0.043770\n",
      "Epoch: 8153 \tTraining Loss: 0.043719\n",
      "Epoch: 8154 \tTraining Loss: 0.043734\n",
      "Epoch: 8155 \tTraining Loss: 0.043744\n",
      "Epoch: 8156 \tTraining Loss: 0.043756\n",
      "Epoch: 8157 \tTraining Loss: 0.043720\n",
      "Epoch: 8158 \tTraining Loss: 0.043731\n",
      "Epoch: 8159 \tTraining Loss: 0.043759\n",
      "Epoch: 8160 \tTraining Loss: 0.043769\n",
      "Epoch: 8161 \tTraining Loss: 0.043722\n",
      "Epoch: 8162 \tTraining Loss: 0.043750\n",
      "Epoch: 8163 \tTraining Loss: 0.043757\n",
      "Epoch: 8164 \tTraining Loss: 0.043838\n",
      "Epoch: 8165 \tTraining Loss: 0.043716\n",
      "Epoch: 8166 \tTraining Loss: 0.043743\n",
      "Epoch: 8167 \tTraining Loss: 0.043772\n",
      "Epoch: 8168 \tTraining Loss: 0.043750\n",
      "Epoch: 8169 \tTraining Loss: 0.043723\n",
      "Epoch: 8170 \tTraining Loss: 0.043737\n",
      "Epoch: 8171 \tTraining Loss: 0.043739\n",
      "Epoch: 8172 \tTraining Loss: 0.043728\n",
      "Epoch: 8173 \tTraining Loss: 0.043724\n",
      "Epoch: 8174 \tTraining Loss: 0.043768\n",
      "Epoch: 8175 \tTraining Loss: 0.043675\n",
      "Epoch: 8176 \tTraining Loss: 0.043729\n",
      "Epoch: 8177 \tTraining Loss: 0.043745\n",
      "Epoch: 8178 \tTraining Loss: 0.043735\n",
      "Epoch: 8179 \tTraining Loss: 0.043848\n",
      "Epoch: 8180 \tTraining Loss: 0.043733\n",
      "Epoch: 8181 \tTraining Loss: 0.043727\n",
      "Epoch: 8182 \tTraining Loss: 0.043757\n",
      "Epoch: 8183 \tTraining Loss: 0.043694\n",
      "Epoch: 8184 \tTraining Loss: 0.043711\n",
      "Epoch: 8185 \tTraining Loss: 0.043713\n",
      "Epoch: 8186 \tTraining Loss: 0.043737\n",
      "Epoch: 8187 \tTraining Loss: 0.043735\n",
      "Epoch: 8188 \tTraining Loss: 0.043707\n",
      "Epoch: 8189 \tTraining Loss: 0.043723\n",
      "Epoch: 8190 \tTraining Loss: 0.043752\n",
      "Epoch: 8191 \tTraining Loss: 0.043740\n",
      "Epoch: 8192 \tTraining Loss: 0.043713\n",
      "Epoch: 8193 \tTraining Loss: 0.043716\n",
      "Epoch: 8194 \tTraining Loss: 0.043707\n",
      "Epoch: 8195 \tTraining Loss: 0.043706\n",
      "Epoch: 8196 \tTraining Loss: 0.043727\n",
      "Epoch: 8197 \tTraining Loss: 0.043736\n",
      "Epoch: 8198 \tTraining Loss: 0.043711\n",
      "Epoch: 8199 \tTraining Loss: 0.043755\n",
      "Epoch: 8200 \tTraining Loss: 0.043726\n",
      "Epoch: 8201 \tTraining Loss: 0.043737\n",
      "Epoch: 8202 \tTraining Loss: 0.043730\n",
      "Epoch: 8203 \tTraining Loss: 0.043721\n",
      "Epoch: 8204 \tTraining Loss: 0.043723\n",
      "Epoch: 8205 \tTraining Loss: 0.043706\n",
      "Epoch: 8206 \tTraining Loss: 0.043762\n",
      "Epoch: 8207 \tTraining Loss: 0.043758\n",
      "Epoch: 8208 \tTraining Loss: 0.043690\n",
      "Epoch: 8209 \tTraining Loss: 0.043733\n",
      "Epoch: 8210 \tTraining Loss: 0.043720\n",
      "Epoch: 8211 \tTraining Loss: 0.043834\n",
      "Epoch: 8212 \tTraining Loss: 0.043711\n",
      "Epoch: 8213 \tTraining Loss: 0.043713\n",
      "Epoch: 8214 \tTraining Loss: 0.043781\n",
      "Epoch: 8215 \tTraining Loss: 0.043742\n",
      "Epoch: 8216 \tTraining Loss: 0.043758\n",
      "Epoch: 8217 \tTraining Loss: 0.043720\n",
      "Epoch: 8218 \tTraining Loss: 0.043730\n",
      "Epoch: 8219 \tTraining Loss: 0.043789\n",
      "Epoch: 8220 \tTraining Loss: 0.043708\n",
      "Epoch: 8221 \tTraining Loss: 0.043856\n",
      "Epoch: 8222 \tTraining Loss: 0.043710\n",
      "Epoch: 8223 \tTraining Loss: 0.043708\n",
      "Epoch: 8224 \tTraining Loss: 0.043718\n",
      "Epoch: 8225 \tTraining Loss: 0.043696\n",
      "Epoch: 8226 \tTraining Loss: 0.043722\n",
      "Epoch: 8227 \tTraining Loss: 0.043727\n",
      "Epoch: 8228 \tTraining Loss: 0.043699\n",
      "Epoch: 8229 \tTraining Loss: 0.043727\n",
      "Epoch: 8230 \tTraining Loss: 0.043724\n",
      "Epoch: 8231 \tTraining Loss: 0.043709\n",
      "Epoch: 8232 \tTraining Loss: 0.043720\n",
      "Epoch: 8233 \tTraining Loss: 0.043798\n",
      "Epoch: 8234 \tTraining Loss: 0.043708\n",
      "Epoch: 8235 \tTraining Loss: 0.043778\n",
      "Epoch: 8236 \tTraining Loss: 0.043716\n",
      "Epoch: 8237 \tTraining Loss: 0.043751\n",
      "Epoch: 8238 \tTraining Loss: 0.043694\n",
      "Epoch: 8239 \tTraining Loss: 0.043701\n",
      "Epoch: 8240 \tTraining Loss: 0.043712\n",
      "Epoch: 8241 \tTraining Loss: 0.043695\n",
      "Epoch: 8242 \tTraining Loss: 0.043708\n",
      "Epoch: 8243 \tTraining Loss: 0.043692\n",
      "Epoch: 8244 \tTraining Loss: 0.043720\n",
      "Epoch: 8245 \tTraining Loss: 0.043750\n",
      "Epoch: 8246 \tTraining Loss: 0.043733\n",
      "Epoch: 8247 \tTraining Loss: 0.043700\n",
      "Epoch: 8248 \tTraining Loss: 0.043832\n",
      "Epoch: 8249 \tTraining Loss: 0.043690\n",
      "Epoch: 8250 \tTraining Loss: 0.043705\n",
      "Epoch: 8251 \tTraining Loss: 0.043720\n",
      "Epoch: 8252 \tTraining Loss: 0.043728\n",
      "Epoch: 8253 \tTraining Loss: 0.043771\n",
      "Epoch: 8254 \tTraining Loss: 0.043735\n",
      "Epoch: 8255 \tTraining Loss: 0.043693\n",
      "Epoch: 8256 \tTraining Loss: 0.043709\n",
      "Epoch: 8257 \tTraining Loss: 0.043706\n",
      "Epoch: 8258 \tTraining Loss: 0.043694\n",
      "Epoch: 8259 \tTraining Loss: 0.043710\n",
      "Epoch: 8260 \tTraining Loss: 0.043727\n",
      "Epoch: 8261 \tTraining Loss: 0.043699\n",
      "Epoch: 8262 \tTraining Loss: 0.043710\n",
      "Epoch: 8263 \tTraining Loss: 0.043717\n",
      "Epoch: 8264 \tTraining Loss: 0.048219\n",
      "Epoch: 8265 \tTraining Loss: 0.104929\n",
      "Epoch: 8266 \tTraining Loss: 0.184549\n",
      "Epoch: 8267 \tTraining Loss: 0.178247\n",
      "Epoch: 8268 \tTraining Loss: 0.100030\n",
      "Epoch: 8269 \tTraining Loss: 0.124871\n",
      "Epoch: 8270 \tTraining Loss: 0.087221\n",
      "Epoch: 8271 \tTraining Loss: 0.081846\n",
      "Epoch: 8272 \tTraining Loss: 0.078013\n",
      "Epoch: 8273 \tTraining Loss: 0.056287\n",
      "Epoch: 8274 \tTraining Loss: 0.053318\n",
      "Epoch: 8275 \tTraining Loss: 0.052932\n",
      "Epoch: 8276 \tTraining Loss: 0.052831\n",
      "Epoch: 8277 \tTraining Loss: 0.052670\n",
      "Epoch: 8278 \tTraining Loss: 0.052637\n",
      "Epoch: 8279 \tTraining Loss: 0.052543\n",
      "Epoch: 8280 \tTraining Loss: 0.052533\n",
      "Epoch: 8281 \tTraining Loss: 0.052584\n",
      "Epoch: 8282 \tTraining Loss: 0.052504\n",
      "Epoch: 8283 \tTraining Loss: 0.052494\n",
      "Epoch: 8284 \tTraining Loss: 0.052593\n",
      "Epoch: 8285 \tTraining Loss: 0.052471\n",
      "Epoch: 8286 \tTraining Loss: 0.052478\n",
      "Epoch: 8287 \tTraining Loss: 0.052418\n",
      "Epoch: 8288 \tTraining Loss: 0.052413\n",
      "Epoch: 8289 \tTraining Loss: 0.052387\n",
      "Epoch: 8290 \tTraining Loss: 0.052409\n",
      "Epoch: 8291 \tTraining Loss: 0.052391\n",
      "Epoch: 8292 \tTraining Loss: 0.052427\n",
      "Epoch: 8293 \tTraining Loss: 0.052462\n",
      "Epoch: 8294 \tTraining Loss: 0.052365\n",
      "Epoch: 8295 \tTraining Loss: 0.052473\n",
      "Epoch: 8296 \tTraining Loss: 0.052407\n",
      "Epoch: 8297 \tTraining Loss: 0.052404\n",
      "Epoch: 8298 \tTraining Loss: 0.052364\n",
      "Epoch: 8299 \tTraining Loss: 0.052399\n",
      "Epoch: 8300 \tTraining Loss: 0.052362\n",
      "Epoch: 8301 \tTraining Loss: 0.052358\n",
      "Epoch: 8302 \tTraining Loss: 0.052343\n",
      "Epoch: 8303 \tTraining Loss: 0.052355\n",
      "Epoch: 8304 \tTraining Loss: 0.052359\n",
      "Epoch: 8305 \tTraining Loss: 0.052328\n",
      "Epoch: 8306 \tTraining Loss: 0.052329\n",
      "Epoch: 8307 \tTraining Loss: 0.052405\n",
      "Epoch: 8308 \tTraining Loss: 0.052394\n",
      "Epoch: 8309 \tTraining Loss: 0.052328\n",
      "Epoch: 8310 \tTraining Loss: 0.052349\n",
      "Epoch: 8311 \tTraining Loss: 0.052304\n",
      "Epoch: 8312 \tTraining Loss: 0.052323\n",
      "Epoch: 8313 \tTraining Loss: 0.052325\n",
      "Epoch: 8314 \tTraining Loss: 0.052359\n",
      "Epoch: 8315 \tTraining Loss: 0.052324\n",
      "Epoch: 8316 \tTraining Loss: 0.052326\n",
      "Epoch: 8317 \tTraining Loss: 0.052314\n",
      "Epoch: 8318 \tTraining Loss: 0.052329\n",
      "Epoch: 8319 \tTraining Loss: 0.052337\n",
      "Epoch: 8320 \tTraining Loss: 0.052316\n",
      "Epoch: 8321 \tTraining Loss: 0.052310\n",
      "Epoch: 8322 \tTraining Loss: 0.052337\n",
      "Epoch: 8323 \tTraining Loss: 0.052405\n",
      "Epoch: 8324 \tTraining Loss: 0.052484\n",
      "Epoch: 8325 \tTraining Loss: 0.052313\n",
      "Epoch: 8326 \tTraining Loss: 0.052304\n",
      "Epoch: 8327 \tTraining Loss: 0.052358\n",
      "Epoch: 8328 \tTraining Loss: 0.052323\n",
      "Epoch: 8329 \tTraining Loss: 0.052318\n",
      "Epoch: 8330 \tTraining Loss: 0.052459\n",
      "Epoch: 8331 \tTraining Loss: 0.047736\n",
      "Epoch: 8332 \tTraining Loss: 0.047587\n",
      "Epoch: 8333 \tTraining Loss: 0.047541\n",
      "Epoch: 8334 \tTraining Loss: 0.047407\n",
      "Epoch: 8335 \tTraining Loss: 0.047432\n",
      "Epoch: 8336 \tTraining Loss: 0.047307\n",
      "Epoch: 8337 \tTraining Loss: 0.047403\n",
      "Epoch: 8338 \tTraining Loss: 0.047289\n",
      "Epoch: 8339 \tTraining Loss: 0.047263\n",
      "Epoch: 8340 \tTraining Loss: 0.047276\n",
      "Epoch: 8341 \tTraining Loss: 0.047257\n",
      "Epoch: 8342 \tTraining Loss: 0.047298\n",
      "Epoch: 8343 \tTraining Loss: 0.047314\n",
      "Epoch: 8344 \tTraining Loss: 0.047249\n",
      "Epoch: 8345 \tTraining Loss: 0.047252\n",
      "Epoch: 8346 \tTraining Loss: 0.046160\n",
      "Epoch: 8347 \tTraining Loss: 0.045166\n",
      "Epoch: 8348 \tTraining Loss: 0.045119\n",
      "Epoch: 8349 \tTraining Loss: 0.045101\n",
      "Epoch: 8350 \tTraining Loss: 0.045039\n",
      "Epoch: 8351 \tTraining Loss: 0.045019\n",
      "Epoch: 8352 \tTraining Loss: 0.045001\n",
      "Epoch: 8353 \tTraining Loss: 0.045035\n",
      "Epoch: 8354 \tTraining Loss: 0.045006\n",
      "Epoch: 8355 \tTraining Loss: 0.044952\n",
      "Epoch: 8356 \tTraining Loss: 0.044972\n",
      "Epoch: 8357 \tTraining Loss: 0.044963\n",
      "Epoch: 8358 \tTraining Loss: 0.044951\n",
      "Epoch: 8359 \tTraining Loss: 0.045051\n",
      "Epoch: 8360 \tTraining Loss: 0.044993\n",
      "Epoch: 8361 \tTraining Loss: 0.045098\n",
      "Epoch: 8362 \tTraining Loss: 0.044953\n",
      "Epoch: 8363 \tTraining Loss: 0.045119\n",
      "Epoch: 8364 \tTraining Loss: 0.044944\n",
      "Epoch: 8365 \tTraining Loss: 0.044908\n",
      "Epoch: 8366 \tTraining Loss: 0.044955\n",
      "Epoch: 8367 \tTraining Loss: 0.044950\n",
      "Epoch: 8368 \tTraining Loss: 0.044947\n",
      "Epoch: 8369 \tTraining Loss: 0.044961\n",
      "Epoch: 8370 \tTraining Loss: 0.045028\n",
      "Epoch: 8371 \tTraining Loss: 0.044987\n",
      "Epoch: 8372 \tTraining Loss: 0.044937\n",
      "Epoch: 8373 \tTraining Loss: 0.044994\n",
      "Epoch: 8374 \tTraining Loss: 0.045037\n",
      "Epoch: 8375 \tTraining Loss: 0.045014\n",
      "Epoch: 8376 \tTraining Loss: 0.045061\n",
      "Epoch: 8377 \tTraining Loss: 0.044984\n",
      "Epoch: 8378 \tTraining Loss: 0.044928\n",
      "Epoch: 8379 \tTraining Loss: 0.044960\n",
      "Epoch: 8380 \tTraining Loss: 0.044921\n",
      "Epoch: 8381 \tTraining Loss: 0.044955\n",
      "Epoch: 8382 \tTraining Loss: 0.045030\n",
      "Epoch: 8383 \tTraining Loss: 0.044919\n",
      "Epoch: 8384 \tTraining Loss: 0.044919\n",
      "Epoch: 8385 \tTraining Loss: 0.044997\n",
      "Epoch: 8386 \tTraining Loss: 0.045011\n",
      "Epoch: 8387 \tTraining Loss: 0.044969\n",
      "Epoch: 8388 \tTraining Loss: 0.044945\n",
      "Epoch: 8389 \tTraining Loss: 0.044919\n",
      "Epoch: 8390 \tTraining Loss: 0.044909\n",
      "Epoch: 8391 \tTraining Loss: 0.044929\n",
      "Epoch: 8392 \tTraining Loss: 0.044948\n",
      "Epoch: 8393 \tTraining Loss: 0.044983\n",
      "Epoch: 8394 \tTraining Loss: 0.044919\n",
      "Epoch: 8395 \tTraining Loss: 0.044951\n",
      "Epoch: 8396 \tTraining Loss: 0.044945\n",
      "Epoch: 8397 \tTraining Loss: 0.044908\n",
      "Epoch: 8398 \tTraining Loss: 0.044921\n",
      "Epoch: 8399 \tTraining Loss: 0.044984\n",
      "Epoch: 8400 \tTraining Loss: 0.044993\n",
      "Epoch: 8401 \tTraining Loss: 0.044929\n",
      "Epoch: 8402 \tTraining Loss: 0.044917\n",
      "Epoch: 8403 \tTraining Loss: 0.044972\n",
      "Epoch: 8404 \tTraining Loss: 0.044923\n",
      "Epoch: 8405 \tTraining Loss: 0.044938\n",
      "Epoch: 8406 \tTraining Loss: 0.044940\n",
      "Epoch: 8407 \tTraining Loss: 0.044928\n",
      "Epoch: 8408 \tTraining Loss: 0.044912\n",
      "Epoch: 8409 \tTraining Loss: 0.044926\n",
      "Epoch: 8410 \tTraining Loss: 0.044938\n",
      "Epoch: 8411 \tTraining Loss: 0.044939\n",
      "Epoch: 8412 \tTraining Loss: 0.044926\n",
      "Epoch: 8413 \tTraining Loss: 0.044936\n",
      "Epoch: 8414 \tTraining Loss: 0.044917\n",
      "Epoch: 8415 \tTraining Loss: 0.044937\n",
      "Epoch: 8416 \tTraining Loss: 0.044909\n",
      "Epoch: 8417 \tTraining Loss: 0.044931\n",
      "Epoch: 8418 \tTraining Loss: 0.044940\n",
      "Epoch: 8419 \tTraining Loss: 0.044984\n",
      "Epoch: 8420 \tTraining Loss: 0.044888\n",
      "Epoch: 8421 \tTraining Loss: 0.044916\n",
      "Epoch: 8422 \tTraining Loss: 0.044914\n",
      "Epoch: 8423 \tTraining Loss: 0.044987\n",
      "Epoch: 8424 \tTraining Loss: 0.044915\n",
      "Epoch: 8425 \tTraining Loss: 0.044926\n",
      "Epoch: 8426 \tTraining Loss: 0.044942\n",
      "Epoch: 8427 \tTraining Loss: 0.044919\n",
      "Epoch: 8428 \tTraining Loss: 0.044981\n",
      "Epoch: 8429 \tTraining Loss: 0.044976\n",
      "Epoch: 8430 \tTraining Loss: 0.045394\n",
      "Epoch: 8431 \tTraining Loss: 0.044391\n",
      "Epoch: 8432 \tTraining Loss: 0.054518\n",
      "Epoch: 8433 \tTraining Loss: 0.048809\n",
      "Epoch: 8434 \tTraining Loss: 0.046226\n",
      "Epoch: 8435 \tTraining Loss: 0.046332\n",
      "Epoch: 8436 \tTraining Loss: 0.046244\n",
      "Epoch: 8437 \tTraining Loss: 0.046230\n",
      "Epoch: 8438 \tTraining Loss: 0.046164\n",
      "Epoch: 8439 \tTraining Loss: 0.046170\n",
      "Epoch: 8440 \tTraining Loss: 0.046167\n",
      "Epoch: 8441 \tTraining Loss: 0.046145\n",
      "Epoch: 8442 \tTraining Loss: 0.046192\n",
      "Epoch: 8443 \tTraining Loss: 0.046141\n",
      "Epoch: 8444 \tTraining Loss: 0.046186\n",
      "Epoch: 8445 \tTraining Loss: 0.046133\n",
      "Epoch: 8446 \tTraining Loss: 0.046131\n",
      "Epoch: 8447 \tTraining Loss: 0.046120\n",
      "Epoch: 8448 \tTraining Loss: 0.046137\n",
      "Epoch: 8449 \tTraining Loss: 0.046182\n",
      "Epoch: 8450 \tTraining Loss: 0.046119\n",
      "Epoch: 8451 \tTraining Loss: 0.046157\n",
      "Epoch: 8452 \tTraining Loss: 0.046134\n",
      "Epoch: 8453 \tTraining Loss: 0.046101\n",
      "Epoch: 8454 \tTraining Loss: 0.046164\n",
      "Epoch: 8455 \tTraining Loss: 0.046128\n",
      "Epoch: 8456 \tTraining Loss: 0.046180\n",
      "Epoch: 8457 \tTraining Loss: 0.046116\n",
      "Epoch: 8458 \tTraining Loss: 0.046095\n",
      "Epoch: 8459 \tTraining Loss: 0.046142\n",
      "Epoch: 8460 \tTraining Loss: 0.046105\n",
      "Epoch: 8461 \tTraining Loss: 0.046191\n",
      "Epoch: 8462 \tTraining Loss: 0.046184\n",
      "Epoch: 8463 \tTraining Loss: 0.046149\n",
      "Epoch: 8464 \tTraining Loss: 0.046154\n",
      "Epoch: 8465 \tTraining Loss: 0.046144\n",
      "Epoch: 8466 \tTraining Loss: 0.046148\n",
      "Epoch: 8467 \tTraining Loss: 0.046152\n",
      "Epoch: 8468 \tTraining Loss: 0.048557\n",
      "Epoch: 8469 \tTraining Loss: 0.106879\n",
      "Epoch: 8470 \tTraining Loss: 0.066705\n",
      "Epoch: 8471 \tTraining Loss: 0.092736\n",
      "Epoch: 8472 \tTraining Loss: 0.097999\n",
      "Epoch: 8473 \tTraining Loss: 0.055862\n",
      "Epoch: 8474 \tTraining Loss: 0.066561\n",
      "Epoch: 8475 \tTraining Loss: 0.064559\n",
      "Epoch: 8476 \tTraining Loss: 0.049666\n",
      "Epoch: 8477 \tTraining Loss: 0.047341\n",
      "Epoch: 8478 \tTraining Loss: 0.045143\n",
      "Epoch: 8479 \tTraining Loss: 0.046440\n",
      "Epoch: 8480 \tTraining Loss: 0.044715\n",
      "Epoch: 8481 \tTraining Loss: 0.042622\n",
      "Epoch: 8482 \tTraining Loss: 0.042634\n",
      "Epoch: 8483 \tTraining Loss: 0.042624\n",
      "Epoch: 8484 \tTraining Loss: 0.042598\n",
      "Epoch: 8485 \tTraining Loss: 0.042823\n",
      "Epoch: 8486 \tTraining Loss: 0.042501\n",
      "Epoch: 8487 \tTraining Loss: 0.042554\n",
      "Epoch: 8488 \tTraining Loss: 0.042533\n",
      "Epoch: 8489 \tTraining Loss: 0.042553\n",
      "Epoch: 8490 \tTraining Loss: 0.042664\n",
      "Epoch: 8491 \tTraining Loss: 0.042529\n",
      "Epoch: 8492 \tTraining Loss: 0.042562\n",
      "Epoch: 8493 \tTraining Loss: 0.042564\n",
      "Epoch: 8494 \tTraining Loss: 0.042501\n",
      "Epoch: 8495 \tTraining Loss: 0.042510\n",
      "Epoch: 8496 \tTraining Loss: 0.042522\n",
      "Epoch: 8497 \tTraining Loss: 0.042642\n",
      "Epoch: 8498 \tTraining Loss: 0.042499\n",
      "Epoch: 8499 \tTraining Loss: 0.042538\n",
      "Epoch: 8500 \tTraining Loss: 0.042555\n",
      "Epoch: 8501 \tTraining Loss: 0.042519\n",
      "Epoch: 8502 \tTraining Loss: 0.042497\n",
      "Epoch: 8503 \tTraining Loss: 0.042524\n",
      "Epoch: 8504 \tTraining Loss: 0.042530\n",
      "Epoch: 8505 \tTraining Loss: 0.042490\n",
      "Epoch: 8506 \tTraining Loss: 0.042519\n",
      "Epoch: 8507 \tTraining Loss: 0.042565\n",
      "Epoch: 8508 \tTraining Loss: 0.042512\n",
      "Epoch: 8509 \tTraining Loss: 0.042514\n",
      "Epoch: 8510 \tTraining Loss: 0.042514\n",
      "Epoch: 8511 \tTraining Loss: 0.042492\n",
      "Epoch: 8512 \tTraining Loss: 0.042510\n",
      "Epoch: 8513 \tTraining Loss: 0.042506\n",
      "Epoch: 8514 \tTraining Loss: 0.042489\n",
      "Epoch: 8515 \tTraining Loss: 0.042550\n",
      "Epoch: 8516 \tTraining Loss: 0.042518\n",
      "Epoch: 8517 \tTraining Loss: 0.042479\n",
      "Epoch: 8518 \tTraining Loss: 0.042526\n",
      "Epoch: 8519 \tTraining Loss: 0.042528\n",
      "Epoch: 8520 \tTraining Loss: 0.042486\n",
      "Epoch: 8521 \tTraining Loss: 0.042501\n",
      "Epoch: 8522 \tTraining Loss: 0.042508\n",
      "Epoch: 8523 \tTraining Loss: 0.042575\n",
      "Epoch: 8524 \tTraining Loss: 0.042498\n",
      "Epoch: 8525 \tTraining Loss: 0.042480\n",
      "Epoch: 8526 \tTraining Loss: 0.042494\n",
      "Epoch: 8527 \tTraining Loss: 0.042525\n",
      "Epoch: 8528 \tTraining Loss: 0.042491\n",
      "Epoch: 8529 \tTraining Loss: 0.042606\n",
      "Epoch: 8530 \tTraining Loss: 0.042538\n",
      "Epoch: 8531 \tTraining Loss: 0.042475\n",
      "Epoch: 8532 \tTraining Loss: 0.042499\n",
      "Epoch: 8533 \tTraining Loss: 0.042572\n",
      "Epoch: 8534 \tTraining Loss: 0.042464\n",
      "Epoch: 8535 \tTraining Loss: 0.042479\n",
      "Epoch: 8536 \tTraining Loss: 0.042487\n",
      "Epoch: 8537 \tTraining Loss: 0.042484\n",
      "Epoch: 8538 \tTraining Loss: 0.042513\n",
      "Epoch: 8539 \tTraining Loss: 0.042483\n",
      "Epoch: 8540 \tTraining Loss: 0.042493\n",
      "Epoch: 8541 \tTraining Loss: 0.042501\n",
      "Epoch: 8542 \tTraining Loss: 0.042542\n",
      "Epoch: 8543 \tTraining Loss: 0.042470\n",
      "Epoch: 8544 \tTraining Loss: 0.042485\n",
      "Epoch: 8545 \tTraining Loss: 0.042475\n",
      "Epoch: 8546 \tTraining Loss: 0.042483\n",
      "Epoch: 8547 \tTraining Loss: 0.042506\n",
      "Epoch: 8548 \tTraining Loss: 0.042489\n",
      "Epoch: 8549 \tTraining Loss: 0.042485\n",
      "Epoch: 8550 \tTraining Loss: 0.042491\n",
      "Epoch: 8551 \tTraining Loss: 0.042498\n",
      "Epoch: 8552 \tTraining Loss: 0.042515\n",
      "Epoch: 8553 \tTraining Loss: 0.042630\n",
      "Epoch: 8554 \tTraining Loss: 0.042667\n",
      "Epoch: 8555 \tTraining Loss: 0.042484\n",
      "Epoch: 8556 \tTraining Loss: 0.042537\n",
      "Epoch: 8557 \tTraining Loss: 0.042494\n",
      "Epoch: 8558 \tTraining Loss: 0.042531\n",
      "Epoch: 8559 \tTraining Loss: 0.042471\n",
      "Epoch: 8560 \tTraining Loss: 0.042546\n",
      "Epoch: 8561 \tTraining Loss: 0.042469\n",
      "Epoch: 8562 \tTraining Loss: 0.042554\n",
      "Epoch: 8563 \tTraining Loss: 0.042454\n",
      "Epoch: 8564 \tTraining Loss: 0.042483\n",
      "Epoch: 8565 \tTraining Loss: 0.042519\n",
      "Epoch: 8566 \tTraining Loss: 0.042489\n",
      "Epoch: 8567 \tTraining Loss: 0.042507\n",
      "Epoch: 8568 \tTraining Loss: 0.042551\n",
      "Epoch: 8569 \tTraining Loss: 0.042501\n",
      "Epoch: 8570 \tTraining Loss: 0.042459\n",
      "Epoch: 8571 \tTraining Loss: 0.042460\n",
      "Epoch: 8572 \tTraining Loss: 0.042487\n",
      "Epoch: 8573 \tTraining Loss: 0.042483\n",
      "Epoch: 8574 \tTraining Loss: 0.042474\n",
      "Epoch: 8575 \tTraining Loss: 0.042488\n",
      "Epoch: 8576 \tTraining Loss: 0.042511\n",
      "Epoch: 8577 \tTraining Loss: 0.042480\n",
      "Epoch: 8578 \tTraining Loss: 0.042454\n",
      "Epoch: 8579 \tTraining Loss: 0.042489\n",
      "Epoch: 8580 \tTraining Loss: 0.042468\n",
      "Epoch: 8581 \tTraining Loss: 0.042454\n",
      "Epoch: 8582 \tTraining Loss: 0.042508\n",
      "Epoch: 8583 \tTraining Loss: 0.042468\n",
      "Epoch: 8584 \tTraining Loss: 0.042472\n",
      "Epoch: 8585 \tTraining Loss: 0.042500\n",
      "Epoch: 8586 \tTraining Loss: 0.042560\n",
      "Epoch: 8587 \tTraining Loss: 0.042447\n",
      "Epoch: 8588 \tTraining Loss: 0.042456\n",
      "Epoch: 8589 \tTraining Loss: 0.042480\n",
      "Epoch: 8590 \tTraining Loss: 0.042463\n",
      "Epoch: 8591 \tTraining Loss: 0.042490\n",
      "Epoch: 8592 \tTraining Loss: 0.042461\n",
      "Epoch: 8593 \tTraining Loss: 0.042463\n",
      "Epoch: 8594 \tTraining Loss: 0.042469\n",
      "Epoch: 8595 \tTraining Loss: 0.042509\n",
      "Epoch: 8596 \tTraining Loss: 0.042453\n",
      "Epoch: 8597 \tTraining Loss: 0.042490\n",
      "Epoch: 8598 \tTraining Loss: 0.042471\n",
      "Epoch: 8599 \tTraining Loss: 0.042463\n",
      "Epoch: 8600 \tTraining Loss: 0.042554\n",
      "Epoch: 8601 \tTraining Loss: 0.042476\n",
      "Epoch: 8602 \tTraining Loss: 0.042465\n",
      "Epoch: 8603 \tTraining Loss: 0.042489\n",
      "Epoch: 8604 \tTraining Loss: 0.042463\n",
      "Epoch: 8605 \tTraining Loss: 0.042466\n",
      "Epoch: 8606 \tTraining Loss: 0.042470\n",
      "Epoch: 8607 \tTraining Loss: 0.042449\n",
      "Epoch: 8608 \tTraining Loss: 0.042465\n",
      "Epoch: 8609 \tTraining Loss: 0.042537\n",
      "Epoch: 8610 \tTraining Loss: 0.042517\n",
      "Epoch: 8611 \tTraining Loss: 0.042456\n",
      "Epoch: 8612 \tTraining Loss: 0.042447\n",
      "Epoch: 8613 \tTraining Loss: 0.042462\n",
      "Epoch: 8614 \tTraining Loss: 0.057036\n",
      "Epoch: 8615 \tTraining Loss: 0.181112\n",
      "Epoch: 8616 \tTraining Loss: 0.089775\n",
      "Epoch: 8617 \tTraining Loss: 0.143767\n",
      "Epoch: 8618 \tTraining Loss: 0.078121\n",
      "Epoch: 8619 \tTraining Loss: 0.091841\n",
      "Epoch: 8620 \tTraining Loss: 0.063851\n",
      "Epoch: 8621 \tTraining Loss: 0.088831\n",
      "Epoch: 8622 \tTraining Loss: 0.046263\n",
      "Epoch: 8623 \tTraining Loss: 0.045091\n",
      "Epoch: 8624 \tTraining Loss: 0.041913\n",
      "Epoch: 8625 \tTraining Loss: 0.040865\n",
      "Epoch: 8626 \tTraining Loss: 0.042403\n",
      "Epoch: 8627 \tTraining Loss: 0.041922\n",
      "Epoch: 8628 \tTraining Loss: 0.040402\n",
      "Epoch: 8629 \tTraining Loss: 0.040347\n",
      "Epoch: 8630 \tTraining Loss: 0.040355\n",
      "Epoch: 8631 \tTraining Loss: 0.040329\n",
      "Epoch: 8632 \tTraining Loss: 0.040294\n",
      "Epoch: 8633 \tTraining Loss: 0.040294\n",
      "Epoch: 8634 \tTraining Loss: 0.040276\n",
      "Epoch: 8635 \tTraining Loss: 0.040312\n",
      "Epoch: 8636 \tTraining Loss: 0.040310\n",
      "Epoch: 8637 \tTraining Loss: 0.040280\n",
      "Epoch: 8638 \tTraining Loss: 0.040290\n",
      "Epoch: 8639 \tTraining Loss: 0.040252\n",
      "Epoch: 8640 \tTraining Loss: 0.040266\n",
      "Epoch: 8641 \tTraining Loss: 0.040326\n",
      "Epoch: 8642 \tTraining Loss: 0.040263\n",
      "Epoch: 8643 \tTraining Loss: 0.040332\n",
      "Epoch: 8644 \tTraining Loss: 0.040267\n",
      "Epoch: 8645 \tTraining Loss: 0.040280\n",
      "Epoch: 8646 \tTraining Loss: 0.040275\n",
      "Epoch: 8647 \tTraining Loss: 0.040271\n",
      "Epoch: 8648 \tTraining Loss: 0.040273\n",
      "Epoch: 8649 \tTraining Loss: 0.040346\n",
      "Epoch: 8650 \tTraining Loss: 0.040277\n",
      "Epoch: 8651 \tTraining Loss: 0.040256\n",
      "Epoch: 8652 \tTraining Loss: 0.040238\n",
      "Epoch: 8653 \tTraining Loss: 0.040244\n",
      "Epoch: 8654 \tTraining Loss: 0.040299\n",
      "Epoch: 8655 \tTraining Loss: 0.040269\n",
      "Epoch: 8656 \tTraining Loss: 0.040234\n",
      "Epoch: 8657 \tTraining Loss: 0.040266\n",
      "Epoch: 8658 \tTraining Loss: 0.040386\n",
      "Epoch: 8659 \tTraining Loss: 0.040249\n",
      "Epoch: 8660 \tTraining Loss: 0.040252\n",
      "Epoch: 8661 \tTraining Loss: 0.040245\n",
      "Epoch: 8662 \tTraining Loss: 0.040241\n",
      "Epoch: 8663 \tTraining Loss: 0.040271\n",
      "Epoch: 8664 \tTraining Loss: 0.040290\n",
      "Epoch: 8665 \tTraining Loss: 0.040232\n",
      "Epoch: 8666 \tTraining Loss: 0.040247\n",
      "Epoch: 8667 \tTraining Loss: 0.040230\n",
      "Epoch: 8668 \tTraining Loss: 0.040273\n",
      "Epoch: 8669 \tTraining Loss: 0.040260\n",
      "Epoch: 8670 \tTraining Loss: 0.040225\n",
      "Epoch: 8671 \tTraining Loss: 0.040226\n",
      "Epoch: 8672 \tTraining Loss: 0.040230\n",
      "Epoch: 8673 \tTraining Loss: 0.040242\n",
      "Epoch: 8674 \tTraining Loss: 0.040270\n",
      "Epoch: 8675 \tTraining Loss: 0.040231\n",
      "Epoch: 8676 \tTraining Loss: 0.040237\n",
      "Epoch: 8677 \tTraining Loss: 0.040300\n",
      "Epoch: 8678 \tTraining Loss: 0.040221\n",
      "Epoch: 8679 \tTraining Loss: 0.040241\n",
      "Epoch: 8680 \tTraining Loss: 0.040246\n",
      "Epoch: 8681 \tTraining Loss: 0.040298\n",
      "Epoch: 8682 \tTraining Loss: 0.040254\n",
      "Epoch: 8683 \tTraining Loss: 0.040233\n",
      "Epoch: 8684 \tTraining Loss: 0.040234\n",
      "Epoch: 8685 \tTraining Loss: 0.040337\n",
      "Epoch: 8686 \tTraining Loss: 0.040242\n",
      "Epoch: 8687 \tTraining Loss: 0.040223\n",
      "Epoch: 8688 \tTraining Loss: 0.040223\n",
      "Epoch: 8689 \tTraining Loss: 0.040221\n",
      "Epoch: 8690 \tTraining Loss: 0.040222\n",
      "Epoch: 8691 \tTraining Loss: 0.040245\n",
      "Epoch: 8692 \tTraining Loss: 0.040241\n",
      "Epoch: 8693 \tTraining Loss: 0.040235\n",
      "Epoch: 8694 \tTraining Loss: 0.040262\n",
      "Epoch: 8695 \tTraining Loss: 0.040237\n",
      "Epoch: 8696 \tTraining Loss: 0.040235\n",
      "Epoch: 8697 \tTraining Loss: 0.040796\n",
      "Epoch: 8698 \tTraining Loss: 0.039577\n",
      "Epoch: 8699 \tTraining Loss: 0.040751\n",
      "Epoch: 8700 \tTraining Loss: 0.039234\n",
      "Epoch: 8701 \tTraining Loss: 0.039673\n",
      "Epoch: 8702 \tTraining Loss: 0.040487\n",
      "Epoch: 8703 \tTraining Loss: 0.040345\n",
      "Epoch: 8704 \tTraining Loss: 0.040279\n",
      "Epoch: 8705 \tTraining Loss: 0.040283\n",
      "Epoch: 8706 \tTraining Loss: 0.040265\n",
      "Epoch: 8707 \tTraining Loss: 0.040259\n",
      "Epoch: 8708 \tTraining Loss: 0.040249\n",
      "Epoch: 8709 \tTraining Loss: 0.040257\n",
      "Epoch: 8710 \tTraining Loss: 0.040255\n",
      "Epoch: 8711 \tTraining Loss: 0.040356\n",
      "Epoch: 8712 \tTraining Loss: 0.040232\n",
      "Epoch: 8713 \tTraining Loss: 0.040241\n",
      "Epoch: 8714 \tTraining Loss: 0.040214\n",
      "Epoch: 8715 \tTraining Loss: 0.040231\n",
      "Epoch: 8716 \tTraining Loss: 0.040209\n",
      "Epoch: 8717 \tTraining Loss: 0.040298\n",
      "Epoch: 8718 \tTraining Loss: 0.040246\n",
      "Epoch: 8719 \tTraining Loss: 0.040222\n",
      "Epoch: 8720 \tTraining Loss: 0.040223\n",
      "Epoch: 8721 \tTraining Loss: 0.040211\n",
      "Epoch: 8722 \tTraining Loss: 0.040233\n",
      "Epoch: 8723 \tTraining Loss: 0.040228\n",
      "Epoch: 8724 \tTraining Loss: 0.040261\n",
      "Epoch: 8725 \tTraining Loss: 0.040213\n",
      "Epoch: 8726 \tTraining Loss: 0.040243\n",
      "Epoch: 8727 \tTraining Loss: 0.040236\n",
      "Epoch: 8728 \tTraining Loss: 0.040258\n",
      "Epoch: 8729 \tTraining Loss: 0.040229\n",
      "Epoch: 8730 \tTraining Loss: 0.040298\n",
      "Epoch: 8731 \tTraining Loss: 0.040207\n",
      "Epoch: 8732 \tTraining Loss: 0.040220\n",
      "Epoch: 8733 \tTraining Loss: 0.040302\n",
      "Epoch: 8734 \tTraining Loss: 0.040340\n",
      "Epoch: 8735 \tTraining Loss: 0.040238\n",
      "Epoch: 8736 \tTraining Loss: 0.040305\n",
      "Epoch: 8737 \tTraining Loss: 0.040226\n",
      "Epoch: 8738 \tTraining Loss: 0.040233\n",
      "Epoch: 8739 \tTraining Loss: 0.040237\n",
      "Epoch: 8740 \tTraining Loss: 0.040208\n",
      "Epoch: 8741 \tTraining Loss: 0.040212\n",
      "Epoch: 8742 \tTraining Loss: 0.040273\n",
      "Epoch: 8743 \tTraining Loss: 0.040242\n",
      "Epoch: 8744 \tTraining Loss: 0.040205\n",
      "Epoch: 8745 \tTraining Loss: 0.040377\n",
      "Epoch: 8746 \tTraining Loss: 0.040208\n",
      "Epoch: 8747 \tTraining Loss: 0.040303\n",
      "Epoch: 8748 \tTraining Loss: 0.040216\n",
      "Epoch: 8749 \tTraining Loss: 0.040239\n",
      "Epoch: 8750 \tTraining Loss: 0.040281\n",
      "Epoch: 8751 \tTraining Loss: 0.040218\n",
      "Epoch: 8752 \tTraining Loss: 0.040194\n",
      "Epoch: 8753 \tTraining Loss: 0.040207\n",
      "Epoch: 8754 \tTraining Loss: 0.040224\n",
      "Epoch: 8755 \tTraining Loss: 0.040222\n",
      "Epoch: 8756 \tTraining Loss: 0.040235\n",
      "Epoch: 8757 \tTraining Loss: 0.040199\n",
      "Epoch: 8758 \tTraining Loss: 0.040227\n",
      "Epoch: 8759 \tTraining Loss: 0.040220\n",
      "Epoch: 8760 \tTraining Loss: 0.040205\n",
      "Epoch: 8761 \tTraining Loss: 0.040214\n",
      "Epoch: 8762 \tTraining Loss: 0.040233\n",
      "Epoch: 8763 \tTraining Loss: 0.040250\n",
      "Epoch: 8764 \tTraining Loss: 0.040270\n",
      "Epoch: 8765 \tTraining Loss: 0.040257\n",
      "Epoch: 8766 \tTraining Loss: 0.040226\n",
      "Epoch: 8767 \tTraining Loss: 0.040243\n",
      "Epoch: 8768 \tTraining Loss: 0.040211\n",
      "Epoch: 8769 \tTraining Loss: 0.041000\n",
      "Epoch: 8770 \tTraining Loss: 0.042030\n",
      "Epoch: 8771 \tTraining Loss: 0.040708\n",
      "Epoch: 8772 \tTraining Loss: 0.055610\n",
      "Epoch: 8773 \tTraining Loss: 0.050665\n",
      "Epoch: 8774 \tTraining Loss: 0.073504\n",
      "Epoch: 8775 \tTraining Loss: 0.049233\n",
      "Epoch: 8776 \tTraining Loss: 0.045683\n",
      "Epoch: 8777 \tTraining Loss: 0.043070\n",
      "Epoch: 8778 \tTraining Loss: 0.042781\n",
      "Epoch: 8779 \tTraining Loss: 0.042646\n",
      "Epoch: 8780 \tTraining Loss: 0.042695\n",
      "Epoch: 8781 \tTraining Loss: 0.042599\n",
      "Epoch: 8782 \tTraining Loss: 0.042601\n",
      "Epoch: 8783 \tTraining Loss: 0.042661\n",
      "Epoch: 8784 \tTraining Loss: 0.042583\n",
      "Epoch: 8785 \tTraining Loss: 0.042570\n",
      "Epoch: 8786 \tTraining Loss: 0.042595\n",
      "Epoch: 8787 \tTraining Loss: 0.042612\n",
      "Epoch: 8788 \tTraining Loss: 0.042536\n",
      "Epoch: 8789 \tTraining Loss: 0.042552\n",
      "Epoch: 8790 \tTraining Loss: 0.042568\n",
      "Epoch: 8791 \tTraining Loss: 0.042604\n",
      "Epoch: 8792 \tTraining Loss: 0.042540\n",
      "Epoch: 8793 \tTraining Loss: 0.042526\n",
      "Epoch: 8794 \tTraining Loss: 0.042531\n",
      "Epoch: 8795 \tTraining Loss: 0.042495\n",
      "Epoch: 8796 \tTraining Loss: 0.042499\n",
      "Epoch: 8797 \tTraining Loss: 0.042521\n",
      "Epoch: 8798 \tTraining Loss: 0.042536\n",
      "Epoch: 8799 \tTraining Loss: 0.042538\n",
      "Epoch: 8800 \tTraining Loss: 0.042508\n",
      "Epoch: 8801 \tTraining Loss: 0.042515\n",
      "Epoch: 8802 \tTraining Loss: 0.042521\n",
      "Epoch: 8803 \tTraining Loss: 0.042497\n",
      "Epoch: 8804 \tTraining Loss: 0.042531\n",
      "Epoch: 8805 \tTraining Loss: 0.042535\n",
      "Epoch: 8806 \tTraining Loss: 0.042533\n",
      "Epoch: 8807 \tTraining Loss: 0.042514\n",
      "Epoch: 8808 \tTraining Loss: 0.042510\n",
      "Epoch: 8809 \tTraining Loss: 0.042518\n",
      "Epoch: 8810 \tTraining Loss: 0.042492\n",
      "Epoch: 8811 \tTraining Loss: 0.042513\n",
      "Epoch: 8812 \tTraining Loss: 0.042484\n",
      "Epoch: 8813 \tTraining Loss: 0.042495\n",
      "Epoch: 8814 \tTraining Loss: 0.042489\n",
      "Epoch: 8815 \tTraining Loss: 0.042483\n",
      "Epoch: 8816 \tTraining Loss: 0.042499\n",
      "Epoch: 8817 \tTraining Loss: 0.042538\n",
      "Epoch: 8818 \tTraining Loss: 0.042571\n",
      "Epoch: 8819 \tTraining Loss: 0.042524\n",
      "Epoch: 8820 \tTraining Loss: 0.042610\n",
      "Epoch: 8821 \tTraining Loss: 0.042573\n",
      "Epoch: 8822 \tTraining Loss: 0.042530\n",
      "Epoch: 8823 \tTraining Loss: 0.042533\n",
      "Epoch: 8824 \tTraining Loss: 0.042487\n",
      "Epoch: 8825 \tTraining Loss: 0.042522\n",
      "Epoch: 8826 \tTraining Loss: 0.042489\n",
      "Epoch: 8827 \tTraining Loss: 0.042508\n",
      "Epoch: 8828 \tTraining Loss: 0.042481\n",
      "Epoch: 8829 \tTraining Loss: 0.042477\n",
      "Epoch: 8830 \tTraining Loss: 0.042494\n",
      "Epoch: 8831 \tTraining Loss: 0.042489\n",
      "Epoch: 8832 \tTraining Loss: 0.042478\n",
      "Epoch: 8833 \tTraining Loss: 0.042471\n",
      "Epoch: 8834 \tTraining Loss: 0.042473\n",
      "Epoch: 8835 \tTraining Loss: 0.042506\n",
      "Epoch: 8836 \tTraining Loss: 0.042497\n",
      "Epoch: 8837 \tTraining Loss: 0.042513\n",
      "Epoch: 8838 \tTraining Loss: 0.042486\n",
      "Epoch: 8839 \tTraining Loss: 0.042560\n",
      "Epoch: 8840 \tTraining Loss: 0.042471\n",
      "Epoch: 8841 \tTraining Loss: 0.042533\n",
      "Epoch: 8842 \tTraining Loss: 0.042471\n",
      "Epoch: 8843 \tTraining Loss: 0.042565\n",
      "Epoch: 8844 \tTraining Loss: 0.042528\n",
      "Epoch: 8845 \tTraining Loss: 0.042534\n",
      "Epoch: 8846 \tTraining Loss: 0.042477\n",
      "Epoch: 8847 \tTraining Loss: 0.042493\n",
      "Epoch: 8848 \tTraining Loss: 0.042503\n",
      "Epoch: 8849 \tTraining Loss: 0.042478\n",
      "Epoch: 8850 \tTraining Loss: 0.042478\n",
      "Epoch: 8851 \tTraining Loss: 0.042508\n",
      "Epoch: 8852 \tTraining Loss: 0.042532\n",
      "Epoch: 8853 \tTraining Loss: 0.042617\n",
      "Epoch: 8854 \tTraining Loss: 0.042486\n",
      "Epoch: 8855 \tTraining Loss: 0.042488\n",
      "Epoch: 8856 \tTraining Loss: 0.042465\n",
      "Epoch: 8857 \tTraining Loss: 0.042486\n",
      "Epoch: 8858 \tTraining Loss: 0.042521\n",
      "Epoch: 8859 \tTraining Loss: 0.042462\n",
      "Epoch: 8860 \tTraining Loss: 0.042501\n",
      "Epoch: 8861 \tTraining Loss: 0.042471\n",
      "Epoch: 8862 \tTraining Loss: 0.042463\n",
      "Epoch: 8863 \tTraining Loss: 0.042554\n",
      "Epoch: 8864 \tTraining Loss: 0.042492\n",
      "Epoch: 8865 \tTraining Loss: 0.042481\n",
      "Epoch: 8866 \tTraining Loss: 0.042455\n",
      "Epoch: 8867 \tTraining Loss: 0.042529\n",
      "Epoch: 8868 \tTraining Loss: 0.042663\n",
      "Epoch: 8869 \tTraining Loss: 0.042469\n",
      "Epoch: 8870 \tTraining Loss: 0.042521\n",
      "Epoch: 8871 \tTraining Loss: 0.042483\n",
      "Epoch: 8872 \tTraining Loss: 0.042495\n",
      "Epoch: 8873 \tTraining Loss: 0.042549\n",
      "Epoch: 8874 \tTraining Loss: 0.042582\n",
      "Epoch: 8875 \tTraining Loss: 0.042525\n",
      "Epoch: 8876 \tTraining Loss: 0.042499\n",
      "Epoch: 8877 \tTraining Loss: 0.042557\n",
      "Epoch: 8878 \tTraining Loss: 0.042500\n",
      "Epoch: 8879 \tTraining Loss: 0.042479\n",
      "Epoch: 8880 \tTraining Loss: 0.042507\n",
      "Epoch: 8881 \tTraining Loss: 0.042470\n",
      "Epoch: 8882 \tTraining Loss: 0.042543\n",
      "Epoch: 8883 \tTraining Loss: 0.042507\n",
      "Epoch: 8884 \tTraining Loss: 0.042471\n",
      "Epoch: 8885 \tTraining Loss: 0.042487\n",
      "Epoch: 8886 \tTraining Loss: 0.042467\n",
      "Epoch: 8887 \tTraining Loss: 0.042481\n",
      "Epoch: 8888 \tTraining Loss: 0.042465\n",
      "Epoch: 8889 \tTraining Loss: 0.042488\n",
      "Epoch: 8890 \tTraining Loss: 0.042456\n",
      "Epoch: 8891 \tTraining Loss: 0.042457\n",
      "Epoch: 8892 \tTraining Loss: 0.042459\n",
      "Epoch: 8893 \tTraining Loss: 0.042467\n",
      "Epoch: 8894 \tTraining Loss: 0.042512\n",
      "Epoch: 8895 \tTraining Loss: 0.042457\n",
      "Epoch: 8896 \tTraining Loss: 0.042482\n",
      "Epoch: 8897 \tTraining Loss: 0.042472\n",
      "Epoch: 8898 \tTraining Loss: 0.042458\n",
      "Epoch: 8899 \tTraining Loss: 0.042479\n",
      "Epoch: 8900 \tTraining Loss: 0.042469\n",
      "Epoch: 8901 \tTraining Loss: 0.042492\n",
      "Epoch: 8902 \tTraining Loss: 0.042520\n",
      "Epoch: 8903 \tTraining Loss: 0.042449\n",
      "Epoch: 8904 \tTraining Loss: 0.042479\n",
      "Epoch: 8905 \tTraining Loss: 0.042452\n",
      "Epoch: 8906 \tTraining Loss: 0.042449\n",
      "Epoch: 8907 \tTraining Loss: 0.042490\n",
      "Epoch: 8908 \tTraining Loss: 0.042529\n",
      "Epoch: 8909 \tTraining Loss: 0.042461\n",
      "Epoch: 8910 \tTraining Loss: 0.042488\n",
      "Epoch: 8911 \tTraining Loss: 0.042450\n",
      "Epoch: 8912 \tTraining Loss: 0.042518\n",
      "Epoch: 8913 \tTraining Loss: 0.042461\n",
      "Epoch: 8914 \tTraining Loss: 0.042468\n",
      "Epoch: 8915 \tTraining Loss: 0.042490\n",
      "Epoch: 8916 \tTraining Loss: 0.042537\n",
      "Epoch: 8917 \tTraining Loss: 0.042583\n",
      "Epoch: 8918 \tTraining Loss: 0.042387\n",
      "Epoch: 8919 \tTraining Loss: 0.042490\n",
      "Epoch: 8920 \tTraining Loss: 0.042461\n",
      "Epoch: 8921 \tTraining Loss: 0.042500\n",
      "Epoch: 8922 \tTraining Loss: 0.042482\n",
      "Epoch: 8923 \tTraining Loss: 0.042490\n",
      "Epoch: 8924 \tTraining Loss: 0.042467\n",
      "Epoch: 8925 \tTraining Loss: 0.042472\n",
      "Epoch: 8926 \tTraining Loss: 0.042455\n",
      "Epoch: 8927 \tTraining Loss: 0.042458\n",
      "Epoch: 8928 \tTraining Loss: 0.042479\n",
      "Epoch: 8929 \tTraining Loss: 0.042489\n",
      "Epoch: 8930 \tTraining Loss: 0.042548\n",
      "Epoch: 8931 \tTraining Loss: 0.042452\n",
      "Epoch: 8932 \tTraining Loss: 0.042476\n",
      "Epoch: 8933 \tTraining Loss: 0.042461\n",
      "Epoch: 8934 \tTraining Loss: 0.042621\n",
      "Epoch: 8935 \tTraining Loss: 0.042451\n",
      "Epoch: 8936 \tTraining Loss: 0.042545\n",
      "Epoch: 8937 \tTraining Loss: 0.042497\n",
      "Epoch: 8938 \tTraining Loss: 0.050898\n",
      "Epoch: 8939 \tTraining Loss: 0.178377\n",
      "Epoch: 8940 \tTraining Loss: 0.180558\n",
      "Epoch: 8941 \tTraining Loss: 0.129807\n",
      "Epoch: 8942 \tTraining Loss: 0.058652\n",
      "Epoch: 8943 \tTraining Loss: 0.042646\n",
      "Epoch: 8944 \tTraining Loss: 0.040479\n",
      "Epoch: 8945 \tTraining Loss: 0.038449\n",
      "Epoch: 8946 \tTraining Loss: 0.038605\n",
      "Epoch: 8947 \tTraining Loss: 0.038237\n",
      "Epoch: 8948 \tTraining Loss: 0.038031\n",
      "Epoch: 8949 \tTraining Loss: 0.037981\n",
      "Epoch: 8950 \tTraining Loss: 0.037903\n",
      "Epoch: 8951 \tTraining Loss: 0.037916\n",
      "Epoch: 8952 \tTraining Loss: 0.037991\n",
      "Epoch: 8953 \tTraining Loss: 0.037843\n",
      "Epoch: 8954 \tTraining Loss: 0.037870\n",
      "Epoch: 8955 \tTraining Loss: 0.037865\n",
      "Epoch: 8956 \tTraining Loss: 0.037898\n",
      "Epoch: 8957 \tTraining Loss: 0.037794\n",
      "Epoch: 8958 \tTraining Loss: 0.037832\n",
      "Epoch: 8959 \tTraining Loss: 0.037806\n",
      "Epoch: 8960 \tTraining Loss: 0.037835\n",
      "Epoch: 8961 \tTraining Loss: 0.037815\n",
      "Epoch: 8962 \tTraining Loss: 0.037785\n",
      "Epoch: 8963 \tTraining Loss: 0.037836\n",
      "Epoch: 8964 \tTraining Loss: 0.037808\n",
      "Epoch: 8965 \tTraining Loss: 0.037779\n",
      "Epoch: 8966 \tTraining Loss: 0.037812\n",
      "Epoch: 8967 \tTraining Loss: 0.037783\n",
      "Epoch: 8968 \tTraining Loss: 0.037741\n",
      "Epoch: 8969 \tTraining Loss: 0.037773\n",
      "Epoch: 8970 \tTraining Loss: 0.037759\n",
      "Epoch: 8971 \tTraining Loss: 0.037750\n",
      "Epoch: 8972 \tTraining Loss: 0.037816\n",
      "Epoch: 8973 \tTraining Loss: 0.037750\n",
      "Epoch: 8974 \tTraining Loss: 0.037741\n",
      "Epoch: 8975 \tTraining Loss: 0.037775\n",
      "Epoch: 8976 \tTraining Loss: 0.037785\n",
      "Epoch: 8977 \tTraining Loss: 0.037742\n",
      "Epoch: 8978 \tTraining Loss: 0.037749\n",
      "Epoch: 8979 \tTraining Loss: 0.037764\n",
      "Epoch: 8980 \tTraining Loss: 0.037764\n",
      "Epoch: 8981 \tTraining Loss: 0.037780\n",
      "Epoch: 8982 \tTraining Loss: 0.037753\n",
      "Epoch: 8983 \tTraining Loss: 0.037786\n",
      "Epoch: 8984 \tTraining Loss: 0.037743\n",
      "Epoch: 8985 \tTraining Loss: 0.037748\n",
      "Epoch: 8986 \tTraining Loss: 0.037762\n",
      "Epoch: 8987 \tTraining Loss: 0.037775\n",
      "Epoch: 8988 \tTraining Loss: 0.037830\n",
      "Epoch: 8989 \tTraining Loss: 0.037757\n",
      "Epoch: 8990 \tTraining Loss: 0.037759\n",
      "Epoch: 8991 \tTraining Loss: 0.037784\n",
      "Epoch: 8992 \tTraining Loss: 0.037754\n",
      "Epoch: 8993 \tTraining Loss: 0.037755\n",
      "Epoch: 8994 \tTraining Loss: 0.037755\n",
      "Epoch: 8995 \tTraining Loss: 0.037790\n",
      "Epoch: 8996 \tTraining Loss: 0.037775\n",
      "Epoch: 8997 \tTraining Loss: 0.037799\n",
      "Epoch: 8998 \tTraining Loss: 0.037802\n",
      "Epoch: 8999 \tTraining Loss: 0.037780\n",
      "Epoch: 9000 \tTraining Loss: 0.037749\n",
      "Epoch: 9001 \tTraining Loss: 0.037737\n",
      "Epoch: 9002 \tTraining Loss: 0.037767\n",
      "Epoch: 9003 \tTraining Loss: 0.037764\n",
      "Epoch: 9004 \tTraining Loss: 0.037838\n",
      "Epoch: 9005 \tTraining Loss: 0.037747\n",
      "Epoch: 9006 \tTraining Loss: 0.037847\n",
      "Epoch: 9007 \tTraining Loss: 0.037746\n",
      "Epoch: 9008 \tTraining Loss: 0.037737\n",
      "Epoch: 9009 \tTraining Loss: 0.037731\n",
      "Epoch: 9010 \tTraining Loss: 0.037726\n",
      "Epoch: 9011 \tTraining Loss: 0.037746\n",
      "Epoch: 9012 \tTraining Loss: 0.037868\n",
      "Epoch: 9013 \tTraining Loss: 0.037770\n",
      "Epoch: 9014 \tTraining Loss: 0.037780\n",
      "Epoch: 9015 \tTraining Loss: 0.037733\n",
      "Epoch: 9016 \tTraining Loss: 0.037742\n",
      "Epoch: 9017 \tTraining Loss: 0.037737\n",
      "Epoch: 9018 \tTraining Loss: 0.037796\n",
      "Epoch: 9019 \tTraining Loss: 0.037776\n",
      "Epoch: 9020 \tTraining Loss: 0.037741\n",
      "Epoch: 9021 \tTraining Loss: 0.037726\n",
      "Epoch: 9022 \tTraining Loss: 0.037772\n",
      "Epoch: 9023 \tTraining Loss: 0.037771\n",
      "Epoch: 9024 \tTraining Loss: 0.037747\n",
      "Epoch: 9025 \tTraining Loss: 0.037754\n",
      "Epoch: 9026 \tTraining Loss: 0.037759\n",
      "Epoch: 9027 \tTraining Loss: 0.037738\n",
      "Epoch: 9028 \tTraining Loss: 0.037743\n",
      "Epoch: 9029 \tTraining Loss: 0.037776\n",
      "Epoch: 9030 \tTraining Loss: 0.037734\n",
      "Epoch: 9031 \tTraining Loss: 0.037742\n",
      "Epoch: 9032 \tTraining Loss: 0.037761\n",
      "Epoch: 9033 \tTraining Loss: 0.037823\n",
      "Epoch: 9034 \tTraining Loss: 0.037770\n",
      "Epoch: 9035 \tTraining Loss: 0.037800\n",
      "Epoch: 9036 \tTraining Loss: 0.037743\n",
      "Epoch: 9037 \tTraining Loss: 0.037774\n",
      "Epoch: 9038 \tTraining Loss: 0.037788\n",
      "Epoch: 9039 \tTraining Loss: 0.037742\n",
      "Epoch: 9040 \tTraining Loss: 0.037770\n",
      "Epoch: 9041 \tTraining Loss: 0.037770\n",
      "Epoch: 9042 \tTraining Loss: 0.037756\n",
      "Epoch: 9043 \tTraining Loss: 0.037744\n",
      "Epoch: 9044 \tTraining Loss: 0.037736\n",
      "Epoch: 9045 \tTraining Loss: 0.037728\n",
      "Epoch: 9046 \tTraining Loss: 0.037813\n",
      "Epoch: 9047 \tTraining Loss: 0.037811\n",
      "Epoch: 9048 \tTraining Loss: 0.037753\n",
      "Epoch: 9049 \tTraining Loss: 0.037730\n",
      "Epoch: 9050 \tTraining Loss: 0.037799\n",
      "Epoch: 9051 \tTraining Loss: 0.037755\n",
      "Epoch: 9052 \tTraining Loss: 0.037748\n",
      "Epoch: 9053 \tTraining Loss: 0.037753\n",
      "Epoch: 9054 \tTraining Loss: 0.037727\n",
      "Epoch: 9055 \tTraining Loss: 0.037729\n",
      "Epoch: 9056 \tTraining Loss: 0.037747\n",
      "Epoch: 9057 \tTraining Loss: 0.037735\n",
      "Epoch: 9058 \tTraining Loss: 0.037723\n",
      "Epoch: 9059 \tTraining Loss: 0.037744\n",
      "Epoch: 9060 \tTraining Loss: 0.037783\n",
      "Epoch: 9061 \tTraining Loss: 0.037736\n",
      "Epoch: 9062 \tTraining Loss: 0.037721\n",
      "Epoch: 9063 \tTraining Loss: 0.037895\n",
      "Epoch: 9064 \tTraining Loss: 0.037698\n",
      "Epoch: 9065 \tTraining Loss: 0.037706\n",
      "Epoch: 9066 \tTraining Loss: 0.037792\n",
      "Epoch: 9067 \tTraining Loss: 0.037848\n",
      "Epoch: 9068 \tTraining Loss: 0.037749\n",
      "Epoch: 9069 \tTraining Loss: 0.037749\n",
      "Epoch: 9070 \tTraining Loss: 0.037777\n",
      "Epoch: 9071 \tTraining Loss: 0.037757\n",
      "Epoch: 9072 \tTraining Loss: 0.037789\n",
      "Epoch: 9073 \tTraining Loss: 0.037742\n",
      "Epoch: 9074 \tTraining Loss: 0.037789\n",
      "Epoch: 9075 \tTraining Loss: 0.037726\n",
      "Epoch: 9076 \tTraining Loss: 0.037782\n",
      "Epoch: 9077 \tTraining Loss: 0.037737\n",
      "Epoch: 9078 \tTraining Loss: 0.037733\n",
      "Epoch: 9079 \tTraining Loss: 0.037756\n",
      "Epoch: 9080 \tTraining Loss: 0.037745\n",
      "Epoch: 9081 \tTraining Loss: 0.037751\n",
      "Epoch: 9082 \tTraining Loss: 0.037737\n",
      "Epoch: 9083 \tTraining Loss: 0.037756\n",
      "Epoch: 9084 \tTraining Loss: 0.037778\n",
      "Epoch: 9085 \tTraining Loss: 0.037809\n",
      "Epoch: 9086 \tTraining Loss: 0.037727\n",
      "Epoch: 9087 \tTraining Loss: 0.037720\n",
      "Epoch: 9088 \tTraining Loss: 0.037722\n",
      "Epoch: 9089 \tTraining Loss: 0.037750\n",
      "Epoch: 9090 \tTraining Loss: 0.037885\n",
      "Epoch: 9091 \tTraining Loss: 0.037726\n",
      "Epoch: 9092 \tTraining Loss: 0.037736\n",
      "Epoch: 9093 \tTraining Loss: 0.037726\n",
      "Epoch: 9094 \tTraining Loss: 0.037742\n",
      "Epoch: 9095 \tTraining Loss: 0.037758\n",
      "Epoch: 9096 \tTraining Loss: 0.037748\n",
      "Epoch: 9097 \tTraining Loss: 0.037736\n",
      "Epoch: 9098 \tTraining Loss: 0.037783\n",
      "Epoch: 9099 \tTraining Loss: 0.037745\n",
      "Epoch: 9100 \tTraining Loss: 0.037720\n",
      "Epoch: 9101 \tTraining Loss: 0.037719\n",
      "Epoch: 9102 \tTraining Loss: 0.037725\n",
      "Epoch: 9103 \tTraining Loss: 0.037731\n",
      "Epoch: 9104 \tTraining Loss: 0.037713\n",
      "Epoch: 9105 \tTraining Loss: 0.037735\n",
      "Epoch: 9106 \tTraining Loss: 0.037720\n",
      "Epoch: 9107 \tTraining Loss: 0.037700\n",
      "Epoch: 9108 \tTraining Loss: 0.037760\n",
      "Epoch: 9109 \tTraining Loss: 0.037809\n",
      "Epoch: 9110 \tTraining Loss: 0.037765\n",
      "Epoch: 9111 \tTraining Loss: 0.037795\n",
      "Epoch: 9112 \tTraining Loss: 0.037729\n",
      "Epoch: 9113 \tTraining Loss: 0.037734\n",
      "Epoch: 9114 \tTraining Loss: 0.037747\n",
      "Epoch: 9115 \tTraining Loss: 0.037717\n",
      "Epoch: 9116 \tTraining Loss: 0.037777\n",
      "Epoch: 9117 \tTraining Loss: 0.037756\n",
      "Epoch: 9118 \tTraining Loss: 0.037746\n",
      "Epoch: 9119 \tTraining Loss: 0.037807\n",
      "Epoch: 9120 \tTraining Loss: 0.037731\n",
      "Epoch: 9121 \tTraining Loss: 0.037728\n",
      "Epoch: 9122 \tTraining Loss: 0.037777\n",
      "Epoch: 9123 \tTraining Loss: 0.037845\n",
      "Epoch: 9124 \tTraining Loss: 0.037766\n",
      "Epoch: 9125 \tTraining Loss: 0.037757\n",
      "Epoch: 9126 \tTraining Loss: 0.037791\n",
      "Epoch: 9127 \tTraining Loss: 0.037785\n",
      "Epoch: 9128 \tTraining Loss: 0.037812\n",
      "Epoch: 9129 \tTraining Loss: 0.037760\n",
      "Epoch: 9130 \tTraining Loss: 0.037756\n",
      "Epoch: 9131 \tTraining Loss: 0.037755\n",
      "Epoch: 9132 \tTraining Loss: 0.037776\n",
      "Epoch: 9133 \tTraining Loss: 0.037721\n",
      "Epoch: 9134 \tTraining Loss: 0.037747\n",
      "Epoch: 9135 \tTraining Loss: 0.037741\n",
      "Epoch: 9136 \tTraining Loss: 0.037743\n",
      "Epoch: 9137 \tTraining Loss: 0.037802\n",
      "Epoch: 9138 \tTraining Loss: 0.037720\n",
      "Epoch: 9139 \tTraining Loss: 0.037716\n",
      "Epoch: 9140 \tTraining Loss: 0.037738\n",
      "Epoch: 9141 \tTraining Loss: 0.037746\n",
      "Epoch: 9142 \tTraining Loss: 0.037737\n",
      "Epoch: 9143 \tTraining Loss: 0.037755\n",
      "Epoch: 9144 \tTraining Loss: 0.037739\n",
      "Epoch: 9145 \tTraining Loss: 0.037772\n",
      "Epoch: 9146 \tTraining Loss: 0.037724\n",
      "Epoch: 9147 \tTraining Loss: 0.037752\n",
      "Epoch: 9148 \tTraining Loss: 0.037753\n",
      "Epoch: 9149 \tTraining Loss: 0.037780\n",
      "Epoch: 9150 \tTraining Loss: 0.037874\n",
      "Epoch: 9151 \tTraining Loss: 0.037741\n",
      "Epoch: 9152 \tTraining Loss: 0.037717\n",
      "Epoch: 9153 \tTraining Loss: 0.037791\n",
      "Epoch: 9154 \tTraining Loss: 0.037765\n",
      "Epoch: 9155 \tTraining Loss: 0.037761\n",
      "Epoch: 9156 \tTraining Loss: 0.037756\n",
      "Epoch: 9157 \tTraining Loss: 0.037810\n",
      "Epoch: 9158 \tTraining Loss: 0.037715\n",
      "Epoch: 9159 \tTraining Loss: 0.037724\n",
      "Epoch: 9160 \tTraining Loss: 0.037810\n",
      "Epoch: 9161 \tTraining Loss: 0.037720\n",
      "Epoch: 9162 \tTraining Loss: 0.039921\n",
      "Epoch: 9163 \tTraining Loss: 0.038316\n",
      "Epoch: 9164 \tTraining Loss: 0.099800\n",
      "Epoch: 9165 \tTraining Loss: 0.351224\n",
      "Epoch: 9166 \tTraining Loss: 0.228889\n",
      "Epoch: 9167 \tTraining Loss: 0.082157\n",
      "Epoch: 9168 \tTraining Loss: 0.098346\n",
      "Epoch: 9169 \tTraining Loss: 0.065656\n",
      "Epoch: 9170 \tTraining Loss: 0.043686\n",
      "Epoch: 9171 \tTraining Loss: 0.042943\n",
      "Epoch: 9172 \tTraining Loss: 0.042396\n",
      "Epoch: 9173 \tTraining Loss: 0.042406\n",
      "Epoch: 9174 \tTraining Loss: 0.042436\n",
      "Epoch: 9175 \tTraining Loss: 0.042394\n",
      "Epoch: 9176 \tTraining Loss: 0.042297\n",
      "Epoch: 9177 \tTraining Loss: 0.042281\n",
      "Epoch: 9178 \tTraining Loss: 0.042265\n",
      "Epoch: 9179 \tTraining Loss: 0.042265\n",
      "Epoch: 9180 \tTraining Loss: 0.042319\n",
      "Epoch: 9181 \tTraining Loss: 0.042257\n",
      "Epoch: 9182 \tTraining Loss: 0.042270\n",
      "Epoch: 9183 \tTraining Loss: 0.042262\n",
      "Epoch: 9184 \tTraining Loss: 0.042259\n",
      "Epoch: 9185 \tTraining Loss: 0.042246\n",
      "Epoch: 9186 \tTraining Loss: 0.042232\n",
      "Epoch: 9187 \tTraining Loss: 0.042361\n",
      "Epoch: 9188 \tTraining Loss: 0.042309\n",
      "Epoch: 9189 \tTraining Loss: 0.042294\n",
      "Epoch: 9190 \tTraining Loss: 0.042243\n",
      "Epoch: 9191 \tTraining Loss: 0.042247\n",
      "Epoch: 9192 \tTraining Loss: 0.042252\n",
      "Epoch: 9193 \tTraining Loss: 0.042239\n",
      "Epoch: 9194 \tTraining Loss: 0.042244\n",
      "Epoch: 9195 \tTraining Loss: 0.042253\n",
      "Epoch: 9196 \tTraining Loss: 0.042202\n",
      "Epoch: 9197 \tTraining Loss: 0.042261\n",
      "Epoch: 9198 \tTraining Loss: 0.042200\n",
      "Epoch: 9199 \tTraining Loss: 0.042214\n",
      "Epoch: 9200 \tTraining Loss: 0.042224\n",
      "Epoch: 9201 \tTraining Loss: 0.042248\n",
      "Epoch: 9202 \tTraining Loss: 0.042194\n",
      "Epoch: 9203 \tTraining Loss: 0.042215\n",
      "Epoch: 9204 \tTraining Loss: 0.042212\n",
      "Epoch: 9205 \tTraining Loss: 0.042194\n",
      "Epoch: 9206 \tTraining Loss: 0.042233\n",
      "Epoch: 9207 \tTraining Loss: 0.042213\n",
      "Epoch: 9208 \tTraining Loss: 0.042206\n",
      "Epoch: 9209 \tTraining Loss: 0.042198\n",
      "Epoch: 9210 \tTraining Loss: 0.042208\n",
      "Epoch: 9211 \tTraining Loss: 0.042222\n",
      "Epoch: 9212 \tTraining Loss: 0.042278\n",
      "Epoch: 9213 \tTraining Loss: 0.042194\n",
      "Epoch: 9214 \tTraining Loss: 0.042190\n",
      "Epoch: 9215 \tTraining Loss: 0.042197\n",
      "Epoch: 9216 \tTraining Loss: 0.042211\n",
      "Epoch: 9217 \tTraining Loss: 0.042227\n",
      "Epoch: 9218 \tTraining Loss: 0.042239\n",
      "Epoch: 9219 \tTraining Loss: 0.042276\n",
      "Epoch: 9220 \tTraining Loss: 0.042261\n",
      "Epoch: 9221 \tTraining Loss: 0.042233\n",
      "Epoch: 9222 \tTraining Loss: 0.042188\n",
      "Epoch: 9223 \tTraining Loss: 0.042186\n",
      "Epoch: 9224 \tTraining Loss: 0.042201\n",
      "Epoch: 9225 \tTraining Loss: 0.042304\n",
      "Epoch: 9226 \tTraining Loss: 0.042185\n",
      "Epoch: 9227 \tTraining Loss: 0.042287\n",
      "Epoch: 9228 \tTraining Loss: 0.042194\n",
      "Epoch: 9229 \tTraining Loss: 0.042214\n",
      "Epoch: 9230 \tTraining Loss: 0.042365\n",
      "Epoch: 9231 \tTraining Loss: 0.042215\n",
      "Epoch: 9232 \tTraining Loss: 0.042180\n",
      "Epoch: 9233 \tTraining Loss: 0.042186\n",
      "Epoch: 9234 \tTraining Loss: 0.042185\n",
      "Epoch: 9235 \tTraining Loss: 0.042194\n",
      "Epoch: 9236 \tTraining Loss: 0.042172\n",
      "Epoch: 9237 \tTraining Loss: 0.042195\n",
      "Epoch: 9238 \tTraining Loss: 0.042178\n",
      "Epoch: 9239 \tTraining Loss: 0.042184\n",
      "Epoch: 9240 \tTraining Loss: 0.042198\n",
      "Epoch: 9241 \tTraining Loss: 0.042187\n",
      "Epoch: 9242 \tTraining Loss: 0.042188\n",
      "Epoch: 9243 \tTraining Loss: 0.042200\n",
      "Epoch: 9244 \tTraining Loss: 0.042181\n",
      "Epoch: 9245 \tTraining Loss: 0.042191\n",
      "Epoch: 9246 \tTraining Loss: 0.042170\n",
      "Epoch: 9247 \tTraining Loss: 0.042218\n",
      "Epoch: 9248 \tTraining Loss: 0.042247\n",
      "Epoch: 9249 \tTraining Loss: 0.042186\n",
      "Epoch: 9250 \tTraining Loss: 0.042195\n",
      "Epoch: 9251 \tTraining Loss: 0.042229\n",
      "Epoch: 9252 \tTraining Loss: 0.042225\n",
      "Epoch: 9253 \tTraining Loss: 0.042196\n",
      "Epoch: 9254 \tTraining Loss: 0.042195\n",
      "Epoch: 9255 \tTraining Loss: 0.042183\n",
      "Epoch: 9256 \tTraining Loss: 0.042200\n",
      "Epoch: 9257 \tTraining Loss: 0.042198\n",
      "Epoch: 9258 \tTraining Loss: 0.042202\n",
      "Epoch: 9259 \tTraining Loss: 0.042203\n",
      "Epoch: 9260 \tTraining Loss: 0.042209\n",
      "Epoch: 9261 \tTraining Loss: 0.042204\n",
      "Epoch: 9262 \tTraining Loss: 0.042155\n",
      "Epoch: 9263 \tTraining Loss: 0.042230\n",
      "Epoch: 9264 \tTraining Loss: 0.042245\n",
      "Epoch: 9265 \tTraining Loss: 0.042170\n",
      "Epoch: 9266 \tTraining Loss: 0.042188\n",
      "Epoch: 9267 \tTraining Loss: 0.042192\n",
      "Epoch: 9268 \tTraining Loss: 0.042167\n",
      "Epoch: 9269 \tTraining Loss: 0.042194\n",
      "Epoch: 9270 \tTraining Loss: 0.042190\n",
      "Epoch: 9271 \tTraining Loss: 0.042231\n",
      "Epoch: 9272 \tTraining Loss: 0.042180\n",
      "Epoch: 9273 \tTraining Loss: 0.042194\n",
      "Epoch: 9274 \tTraining Loss: 0.042202\n",
      "Epoch: 9275 \tTraining Loss: 0.042180\n",
      "Epoch: 9276 \tTraining Loss: 0.042183\n",
      "Epoch: 9277 \tTraining Loss: 0.042218\n",
      "Epoch: 9278 \tTraining Loss: 0.042167\n",
      "Epoch: 9279 \tTraining Loss: 0.042198\n",
      "Epoch: 9280 \tTraining Loss: 0.042183\n",
      "Epoch: 9281 \tTraining Loss: 0.042314\n",
      "Epoch: 9282 \tTraining Loss: 0.042145\n",
      "Epoch: 9283 \tTraining Loss: 0.042182\n",
      "Epoch: 9284 \tTraining Loss: 0.042230\n",
      "Epoch: 9285 \tTraining Loss: 0.042185\n",
      "Epoch: 9286 \tTraining Loss: 0.042337\n",
      "Epoch: 9287 \tTraining Loss: 0.042194\n",
      "Epoch: 9288 \tTraining Loss: 0.042174\n",
      "Epoch: 9289 \tTraining Loss: 0.042279\n",
      "Epoch: 9290 \tTraining Loss: 0.042161\n",
      "Epoch: 9291 \tTraining Loss: 0.042206\n",
      "Epoch: 9292 \tTraining Loss: 0.042214\n",
      "Epoch: 9293 \tTraining Loss: 0.042205\n",
      "Epoch: 9294 \tTraining Loss: 0.057201\n",
      "Epoch: 9295 \tTraining Loss: 0.077360\n",
      "Epoch: 9296 \tTraining Loss: 0.156234\n",
      "Epoch: 9297 \tTraining Loss: 0.107287\n",
      "Epoch: 9298 \tTraining Loss: 0.053344\n",
      "Epoch: 9299 \tTraining Loss: 0.057154\n",
      "Epoch: 9300 \tTraining Loss: 0.051323\n",
      "Epoch: 9301 \tTraining Loss: 0.045892\n",
      "Epoch: 9302 \tTraining Loss: 0.050549\n",
      "Epoch: 9303 \tTraining Loss: 0.046212\n",
      "Epoch: 9304 \tTraining Loss: 0.043032\n",
      "Epoch: 9305 \tTraining Loss: 0.042717\n",
      "Epoch: 9306 \tTraining Loss: 0.042686\n",
      "Epoch: 9307 \tTraining Loss: 0.042659\n",
      "Epoch: 9308 \tTraining Loss: 0.042714\n",
      "Epoch: 9309 \tTraining Loss: 0.042667\n",
      "Epoch: 9310 \tTraining Loss: 0.042624\n",
      "Epoch: 9311 \tTraining Loss: 0.042636\n",
      "Epoch: 9312 \tTraining Loss: 0.042683\n",
      "Epoch: 9313 \tTraining Loss: 0.042673\n",
      "Epoch: 9314 \tTraining Loss: 0.042877\n",
      "Epoch: 9315 \tTraining Loss: 0.042577\n",
      "Epoch: 9316 \tTraining Loss: 0.042628\n",
      "Epoch: 9317 \tTraining Loss: 0.042690\n",
      "Epoch: 9318 \tTraining Loss: 0.042627\n",
      "Epoch: 9319 \tTraining Loss: 0.042682\n",
      "Epoch: 9320 \tTraining Loss: 0.042645\n",
      "Epoch: 9321 \tTraining Loss: 0.042655\n",
      "Epoch: 9322 \tTraining Loss: 0.042634\n",
      "Epoch: 9323 \tTraining Loss: 0.042621\n",
      "Epoch: 9324 \tTraining Loss: 0.042684\n",
      "Epoch: 9325 \tTraining Loss: 0.042629\n",
      "Epoch: 9326 \tTraining Loss: 0.042665\n",
      "Epoch: 9327 \tTraining Loss: 0.042616\n",
      "Epoch: 9328 \tTraining Loss: 0.042611\n",
      "Epoch: 9329 \tTraining Loss: 0.042605\n",
      "Epoch: 9330 \tTraining Loss: 0.042607\n",
      "Epoch: 9331 \tTraining Loss: 0.042693\n",
      "Epoch: 9332 \tTraining Loss: 0.042601\n",
      "Epoch: 9333 \tTraining Loss: 0.042609\n",
      "Epoch: 9334 \tTraining Loss: 0.042594\n",
      "Epoch: 9335 \tTraining Loss: 0.042619\n",
      "Epoch: 9336 \tTraining Loss: 0.042620\n",
      "Epoch: 9337 \tTraining Loss: 0.042607\n",
      "Epoch: 9338 \tTraining Loss: 0.042645\n",
      "Epoch: 9339 \tTraining Loss: 0.042597\n",
      "Epoch: 9340 \tTraining Loss: 0.042636\n",
      "Epoch: 9341 \tTraining Loss: 0.042612\n",
      "Epoch: 9342 \tTraining Loss: 0.042609\n",
      "Epoch: 9343 \tTraining Loss: 0.042601\n",
      "Epoch: 9344 \tTraining Loss: 0.042648\n",
      "Epoch: 9345 \tTraining Loss: 0.042593\n",
      "Epoch: 9346 \tTraining Loss: 0.042671\n",
      "Epoch: 9347 \tTraining Loss: 0.042624\n",
      "Epoch: 9348 \tTraining Loss: 0.042648\n",
      "Epoch: 9349 \tTraining Loss: 0.042655\n",
      "Epoch: 9350 \tTraining Loss: 0.042616\n",
      "Epoch: 9351 \tTraining Loss: 0.042613\n",
      "Epoch: 9352 \tTraining Loss: 0.042594\n",
      "Epoch: 9353 \tTraining Loss: 0.042612\n",
      "Epoch: 9354 \tTraining Loss: 0.042706\n",
      "Epoch: 9355 \tTraining Loss: 0.042591\n",
      "Epoch: 9356 \tTraining Loss: 0.042593\n",
      "Epoch: 9357 \tTraining Loss: 0.042631\n",
      "Epoch: 9358 \tTraining Loss: 0.042662\n",
      "Epoch: 9359 \tTraining Loss: 0.042591\n",
      "Epoch: 9360 \tTraining Loss: 0.042617\n",
      "Epoch: 9361 \tTraining Loss: 0.042619\n",
      "Epoch: 9362 \tTraining Loss: 0.042581\n",
      "Epoch: 9363 \tTraining Loss: 0.042597\n",
      "Epoch: 9364 \tTraining Loss: 0.042635\n",
      "Epoch: 9365 \tTraining Loss: 0.042738\n",
      "Epoch: 9366 \tTraining Loss: 0.043042\n",
      "Epoch: 9367 \tTraining Loss: 0.039610\n",
      "Epoch: 9368 \tTraining Loss: 0.040835\n",
      "Epoch: 9369 \tTraining Loss: 0.040097\n",
      "Epoch: 9370 \tTraining Loss: 0.038593\n",
      "Epoch: 9371 \tTraining Loss: 0.037869\n",
      "Epoch: 9372 \tTraining Loss: 0.037854\n",
      "Epoch: 9373 \tTraining Loss: 0.037774\n",
      "Epoch: 9374 \tTraining Loss: 0.037838\n",
      "Epoch: 9375 \tTraining Loss: 0.037756\n",
      "Epoch: 9376 \tTraining Loss: 0.037775\n",
      "Epoch: 9377 \tTraining Loss: 0.038465\n",
      "Epoch: 9378 \tTraining Loss: 0.037807\n",
      "Epoch: 9379 \tTraining Loss: 0.037770\n",
      "Epoch: 9380 \tTraining Loss: 0.037741\n",
      "Epoch: 9381 \tTraining Loss: 0.037695\n",
      "Epoch: 9382 \tTraining Loss: 0.037814\n",
      "Epoch: 9383 \tTraining Loss: 0.037740\n",
      "Epoch: 9384 \tTraining Loss: 0.037689\n",
      "Epoch: 9385 \tTraining Loss: 0.037680\n",
      "Epoch: 9386 \tTraining Loss: 0.037651\n",
      "Epoch: 9387 \tTraining Loss: 0.037731\n",
      "Epoch: 9388 \tTraining Loss: 0.037648\n",
      "Epoch: 9389 \tTraining Loss: 0.037670\n",
      "Epoch: 9390 \tTraining Loss: 0.037666\n",
      "Epoch: 9391 \tTraining Loss: 0.037665\n",
      "Epoch: 9392 \tTraining Loss: 0.037665\n",
      "Epoch: 9393 \tTraining Loss: 0.037654\n",
      "Epoch: 9394 \tTraining Loss: 0.037662\n",
      "Epoch: 9395 \tTraining Loss: 0.037702\n",
      "Epoch: 9396 \tTraining Loss: 0.037719\n",
      "Epoch: 9397 \tTraining Loss: 0.036790\n",
      "Epoch: 9398 \tTraining Loss: 0.036487\n",
      "Epoch: 9399 \tTraining Loss: 0.036170\n",
      "Epoch: 9400 \tTraining Loss: 0.036210\n",
      "Epoch: 9401 \tTraining Loss: 0.036188\n",
      "Epoch: 9402 \tTraining Loss: 0.036136\n",
      "Epoch: 9403 \tTraining Loss: 0.036145\n",
      "Epoch: 9404 \tTraining Loss: 0.036140\n",
      "Epoch: 9405 \tTraining Loss: 0.036193\n",
      "Epoch: 9406 \tTraining Loss: 0.036143\n",
      "Epoch: 9407 \tTraining Loss: 0.036146\n",
      "Epoch: 9408 \tTraining Loss: 0.036122\n",
      "Epoch: 9409 \tTraining Loss: 0.036153\n",
      "Epoch: 9410 \tTraining Loss: 0.036125\n",
      "Epoch: 9411 \tTraining Loss: 0.036153\n",
      "Epoch: 9412 \tTraining Loss: 0.036209\n",
      "Epoch: 9413 \tTraining Loss: 0.036151\n",
      "Epoch: 9414 \tTraining Loss: 0.036147\n",
      "Epoch: 9415 \tTraining Loss: 0.036116\n",
      "Epoch: 9416 \tTraining Loss: 0.036129\n",
      "Epoch: 9417 \tTraining Loss: 0.036223\n",
      "Epoch: 9418 \tTraining Loss: 0.036162\n",
      "Epoch: 9419 \tTraining Loss: 0.036124\n",
      "Epoch: 9420 \tTraining Loss: 0.036090\n",
      "Epoch: 9421 \tTraining Loss: 0.036151\n",
      "Epoch: 9422 \tTraining Loss: 0.036105\n",
      "Epoch: 9423 \tTraining Loss: 0.036113\n",
      "Epoch: 9424 \tTraining Loss: 0.036097\n",
      "Epoch: 9425 \tTraining Loss: 0.036142\n",
      "Epoch: 9426 \tTraining Loss: 0.036119\n",
      "Epoch: 9427 \tTraining Loss: 0.036112\n",
      "Epoch: 9428 \tTraining Loss: 0.036143\n",
      "Epoch: 9429 \tTraining Loss: 0.036130\n",
      "Epoch: 9430 \tTraining Loss: 0.036155\n",
      "Epoch: 9431 \tTraining Loss: 0.036091\n",
      "Epoch: 9432 \tTraining Loss: 0.036095\n",
      "Epoch: 9433 \tTraining Loss: 0.036173\n",
      "Epoch: 9434 \tTraining Loss: 0.036114\n",
      "Epoch: 9435 \tTraining Loss: 0.036114\n",
      "Epoch: 9436 \tTraining Loss: 0.036129\n",
      "Epoch: 9437 \tTraining Loss: 0.036138\n",
      "Epoch: 9438 \tTraining Loss: 0.036123\n",
      "Epoch: 9439 \tTraining Loss: 0.036252\n",
      "Epoch: 9440 \tTraining Loss: 0.036080\n",
      "Epoch: 9441 \tTraining Loss: 0.036134\n",
      "Epoch: 9442 \tTraining Loss: 0.036102\n",
      "Epoch: 9443 \tTraining Loss: 0.036111\n",
      "Epoch: 9444 \tTraining Loss: 0.036118\n",
      "Epoch: 9445 \tTraining Loss: 0.036101\n",
      "Epoch: 9446 \tTraining Loss: 0.036103\n",
      "Epoch: 9447 \tTraining Loss: 0.036121\n",
      "Epoch: 9448 \tTraining Loss: 0.036116\n",
      "Epoch: 9449 \tTraining Loss: 0.036134\n",
      "Epoch: 9450 \tTraining Loss: 0.036099\n",
      "Epoch: 9451 \tTraining Loss: 0.036135\n",
      "Epoch: 9452 \tTraining Loss: 0.036113\n",
      "Epoch: 9453 \tTraining Loss: 0.036150\n",
      "Epoch: 9454 \tTraining Loss: 0.036097\n",
      "Epoch: 9455 \tTraining Loss: 0.036115\n",
      "Epoch: 9456 \tTraining Loss: 0.036104\n",
      "Epoch: 9457 \tTraining Loss: 0.036103\n",
      "Epoch: 9458 \tTraining Loss: 0.036089\n",
      "Epoch: 9459 \tTraining Loss: 0.036111\n",
      "Epoch: 9460 \tTraining Loss: 0.036125\n",
      "Epoch: 9461 \tTraining Loss: 0.036086\n",
      "Epoch: 9462 \tTraining Loss: 0.036122\n",
      "Epoch: 9463 \tTraining Loss: 0.036127\n",
      "Epoch: 9464 \tTraining Loss: 0.036087\n",
      "Epoch: 9465 \tTraining Loss: 0.036152\n",
      "Epoch: 9466 \tTraining Loss: 0.036201\n",
      "Epoch: 9467 \tTraining Loss: 0.036109\n",
      "Epoch: 9468 \tTraining Loss: 0.036082\n",
      "Epoch: 9469 \tTraining Loss: 0.036123\n",
      "Epoch: 9470 \tTraining Loss: 0.036107\n",
      "Epoch: 9471 \tTraining Loss: 0.036096\n",
      "Epoch: 9472 \tTraining Loss: 0.036107\n",
      "Epoch: 9473 \tTraining Loss: 0.036130\n",
      "Epoch: 9474 \tTraining Loss: 0.036119\n",
      "Epoch: 9475 \tTraining Loss: 0.036118\n",
      "Epoch: 9476 \tTraining Loss: 0.036104\n",
      "Epoch: 9477 \tTraining Loss: 0.036106\n",
      "Epoch: 9478 \tTraining Loss: 0.036132\n",
      "Epoch: 9479 \tTraining Loss: 0.036202\n",
      "Epoch: 9480 \tTraining Loss: 0.036189\n",
      "Epoch: 9481 \tTraining Loss: 0.036117\n",
      "Epoch: 9482 \tTraining Loss: 0.036100\n",
      "Epoch: 9483 \tTraining Loss: 0.036149\n",
      "Epoch: 9484 \tTraining Loss: 0.036101\n",
      "Epoch: 9485 \tTraining Loss: 0.036140\n",
      "Epoch: 9486 \tTraining Loss: 0.036095\n",
      "Epoch: 9487 \tTraining Loss: 0.036127\n",
      "Epoch: 9488 \tTraining Loss: 0.036095\n",
      "Epoch: 9489 \tTraining Loss: 0.036198\n",
      "Epoch: 9490 \tTraining Loss: 0.036079\n",
      "Epoch: 9491 \tTraining Loss: 0.036210\n",
      "Epoch: 9492 \tTraining Loss: 0.036101\n",
      "Epoch: 9493 \tTraining Loss: 0.036099\n",
      "Epoch: 9494 \tTraining Loss: 0.036089\n",
      "Epoch: 9495 \tTraining Loss: 0.036088\n",
      "Epoch: 9496 \tTraining Loss: 0.036101\n",
      "Epoch: 9497 \tTraining Loss: 0.036150\n",
      "Epoch: 9498 \tTraining Loss: 0.036146\n",
      "Epoch: 9499 \tTraining Loss: 0.036093\n",
      "Epoch: 9500 \tTraining Loss: 0.036100\n",
      "Epoch: 9501 \tTraining Loss: 0.036107\n",
      "Epoch: 9502 \tTraining Loss: 0.036091\n",
      "Epoch: 9503 \tTraining Loss: 0.036105\n",
      "Epoch: 9504 \tTraining Loss: 0.036113\n",
      "Epoch: 9505 \tTraining Loss: 0.036103\n",
      "Epoch: 9506 \tTraining Loss: 0.036095\n",
      "Epoch: 9507 \tTraining Loss: 0.036134\n",
      "Epoch: 9508 \tTraining Loss: 0.036129\n",
      "Epoch: 9509 \tTraining Loss: 0.036092\n",
      "Epoch: 9510 \tTraining Loss: 0.036139\n",
      "Epoch: 9511 \tTraining Loss: 0.036172\n",
      "Epoch: 9512 \tTraining Loss: 0.036150\n",
      "Epoch: 9513 \tTraining Loss: 0.036098\n",
      "Epoch: 9514 \tTraining Loss: 0.036126\n",
      "Epoch: 9515 \tTraining Loss: 0.036112\n",
      "Epoch: 9516 \tTraining Loss: 0.036086\n",
      "Epoch: 9517 \tTraining Loss: 0.036195\n",
      "Epoch: 9518 \tTraining Loss: 0.036089\n",
      "Epoch: 9519 \tTraining Loss: 0.036091\n",
      "Epoch: 9520 \tTraining Loss: 0.036103\n",
      "Epoch: 9521 \tTraining Loss: 0.036092\n",
      "Epoch: 9522 \tTraining Loss: 0.036089\n",
      "Epoch: 9523 \tTraining Loss: 0.036126\n",
      "Epoch: 9524 \tTraining Loss: 0.036089\n",
      "Epoch: 9525 \tTraining Loss: 0.036089\n",
      "Epoch: 9526 \tTraining Loss: 0.036086\n",
      "Epoch: 9527 \tTraining Loss: 0.036128\n",
      "Epoch: 9528 \tTraining Loss: 0.036125\n",
      "Epoch: 9529 \tTraining Loss: 0.036216\n",
      "Epoch: 9530 \tTraining Loss: 0.036205\n",
      "Epoch: 9531 \tTraining Loss: 0.036140\n",
      "Epoch: 9532 \tTraining Loss: 0.036099\n",
      "Epoch: 9533 \tTraining Loss: 0.036092\n",
      "Epoch: 9534 \tTraining Loss: 0.036095\n",
      "Epoch: 9535 \tTraining Loss: 0.036085\n",
      "Epoch: 9536 \tTraining Loss: 0.036126\n",
      "Epoch: 9537 \tTraining Loss: 0.036184\n",
      "Epoch: 9538 \tTraining Loss: 0.036128\n",
      "Epoch: 9539 \tTraining Loss: 0.036090\n",
      "Epoch: 9540 \tTraining Loss: 0.036102\n",
      "Epoch: 9541 \tTraining Loss: 0.036106\n",
      "Epoch: 9542 \tTraining Loss: 0.036104\n",
      "Epoch: 9543 \tTraining Loss: 0.036112\n",
      "Epoch: 9544 \tTraining Loss: 0.036172\n",
      "Epoch: 9545 \tTraining Loss: 0.036079\n",
      "Epoch: 9546 \tTraining Loss: 0.036110\n",
      "Epoch: 9547 \tTraining Loss: 0.036224\n",
      "Epoch: 9548 \tTraining Loss: 0.036190\n",
      "Epoch: 9549 \tTraining Loss: 0.036083\n",
      "Epoch: 9550 \tTraining Loss: 0.036104\n",
      "Epoch: 9551 \tTraining Loss: 0.036105\n",
      "Epoch: 9552 \tTraining Loss: 0.036097\n",
      "Epoch: 9553 \tTraining Loss: 0.036110\n",
      "Epoch: 9554 \tTraining Loss: 0.036102\n",
      "Epoch: 9555 \tTraining Loss: 0.036083\n",
      "Epoch: 9556 \tTraining Loss: 0.036119\n",
      "Epoch: 9557 \tTraining Loss: 0.036149\n",
      "Epoch: 9558 \tTraining Loss: 0.036092\n",
      "Epoch: 9559 \tTraining Loss: 0.036138\n",
      "Epoch: 9560 \tTraining Loss: 0.036127\n",
      "Epoch: 9561 \tTraining Loss: 0.036084\n",
      "Epoch: 9562 \tTraining Loss: 0.036098\n",
      "Epoch: 9563 \tTraining Loss: 0.036103\n",
      "Epoch: 9564 \tTraining Loss: 0.036157\n",
      "Epoch: 9565 \tTraining Loss: 0.036110\n",
      "Epoch: 9566 \tTraining Loss: 0.036219\n",
      "Epoch: 9567 \tTraining Loss: 0.036093\n",
      "Epoch: 9568 \tTraining Loss: 0.036088\n",
      "Epoch: 9569 \tTraining Loss: 0.036129\n",
      "Epoch: 9570 \tTraining Loss: 0.036107\n",
      "Epoch: 9571 \tTraining Loss: 0.036077\n",
      "Epoch: 9572 \tTraining Loss: 0.036117\n",
      "Epoch: 9573 \tTraining Loss: 0.036095\n",
      "Epoch: 9574 \tTraining Loss: 0.036089\n",
      "Epoch: 9575 \tTraining Loss: 0.036162\n",
      "Epoch: 9576 \tTraining Loss: 0.036128\n",
      "Epoch: 9577 \tTraining Loss: 0.036087\n",
      "Epoch: 9578 \tTraining Loss: 0.036086\n",
      "Epoch: 9579 \tTraining Loss: 0.036120\n",
      "Epoch: 9580 \tTraining Loss: 0.036101\n",
      "Epoch: 9581 \tTraining Loss: 0.036115\n",
      "Epoch: 9582 \tTraining Loss: 0.036105\n",
      "Epoch: 9583 \tTraining Loss: 0.036141\n",
      "Epoch: 9584 \tTraining Loss: 0.036135\n",
      "Epoch: 9585 \tTraining Loss: 0.036133\n",
      "Epoch: 9586 \tTraining Loss: 0.036103\n",
      "Epoch: 9587 \tTraining Loss: 0.036146\n",
      "Epoch: 9588 \tTraining Loss: 0.036125\n",
      "Epoch: 9589 \tTraining Loss: 0.036107\n",
      "Epoch: 9590 \tTraining Loss: 0.036133\n",
      "Epoch: 9591 \tTraining Loss: 0.036249\n",
      "Epoch: 9592 \tTraining Loss: 0.036086\n",
      "Epoch: 9593 \tTraining Loss: 0.036088\n",
      "Epoch: 9594 \tTraining Loss: 0.036099\n",
      "Epoch: 9595 \tTraining Loss: 0.036201\n",
      "Epoch: 9596 \tTraining Loss: 0.036091\n",
      "Epoch: 9597 \tTraining Loss: 0.036198\n",
      "Epoch: 9598 \tTraining Loss: 0.036080\n",
      "Epoch: 9599 \tTraining Loss: 0.036119\n",
      "Epoch: 9600 \tTraining Loss: 0.036082\n",
      "Epoch: 9601 \tTraining Loss: 0.036107\n",
      "Epoch: 9602 \tTraining Loss: 0.036088\n",
      "Epoch: 9603 \tTraining Loss: 0.036088\n",
      "Epoch: 9604 \tTraining Loss: 0.036090\n",
      "Epoch: 9605 \tTraining Loss: 0.036087\n",
      "Epoch: 9606 \tTraining Loss: 0.036079\n",
      "Epoch: 9607 \tTraining Loss: 0.036172\n",
      "Epoch: 9608 \tTraining Loss: 0.036105\n",
      "Epoch: 9609 \tTraining Loss: 0.036218\n",
      "Epoch: 9610 \tTraining Loss: 0.036083\n",
      "Epoch: 9611 \tTraining Loss: 0.036195\n",
      "Epoch: 9612 \tTraining Loss: 0.036148\n",
      "Epoch: 9613 \tTraining Loss: 0.036086\n",
      "Epoch: 9614 \tTraining Loss: 0.036127\n",
      "Epoch: 9615 \tTraining Loss: 0.036098\n",
      "Epoch: 9616 \tTraining Loss: 0.036217\n",
      "Epoch: 9617 \tTraining Loss: 0.036101\n",
      "Epoch: 9618 \tTraining Loss: 0.036122\n",
      "Epoch: 9619 \tTraining Loss: 0.036128\n",
      "Epoch: 9620 \tTraining Loss: 0.036078\n",
      "Epoch: 9621 \tTraining Loss: 0.036092\n",
      "Epoch: 9622 \tTraining Loss: 0.036079\n",
      "Epoch: 9623 \tTraining Loss: 0.036109\n",
      "Epoch: 9624 \tTraining Loss: 0.036083\n",
      "Epoch: 9625 \tTraining Loss: 0.036085\n",
      "Epoch: 9626 \tTraining Loss: 0.036159\n",
      "Epoch: 9627 \tTraining Loss: 0.036156\n",
      "Epoch: 9628 \tTraining Loss: 0.036102\n",
      "Epoch: 9629 \tTraining Loss: 0.036139\n",
      "Epoch: 9630 \tTraining Loss: 0.036091\n",
      "Epoch: 9631 \tTraining Loss: 0.036093\n",
      "Epoch: 9632 \tTraining Loss: 0.036267\n",
      "Epoch: 9633 \tTraining Loss: 0.036166\n",
      "Epoch: 9634 \tTraining Loss: 0.036099\n",
      "Epoch: 9635 \tTraining Loss: 0.036113\n",
      "Epoch: 9636 \tTraining Loss: 0.036133\n",
      "Epoch: 9637 \tTraining Loss: 0.036094\n",
      "Epoch: 9638 \tTraining Loss: 0.036100\n",
      "Epoch: 9639 \tTraining Loss: 0.036131\n",
      "Epoch: 9640 \tTraining Loss: 0.036093\n",
      "Epoch: 9641 \tTraining Loss: 0.036084\n",
      "Epoch: 9642 \tTraining Loss: 0.036179\n",
      "Epoch: 9643 \tTraining Loss: 0.036153\n",
      "Epoch: 9644 \tTraining Loss: 0.036099\n",
      "Epoch: 9645 \tTraining Loss: 0.036108\n",
      "Epoch: 9646 \tTraining Loss: 0.036105\n",
      "Epoch: 9647 \tTraining Loss: 0.036088\n",
      "Epoch: 9648 \tTraining Loss: 0.036123\n",
      "Epoch: 9649 \tTraining Loss: 0.036094\n",
      "Epoch: 9650 \tTraining Loss: 0.036088\n",
      "Epoch: 9651 \tTraining Loss: 0.036096\n",
      "Epoch: 9652 \tTraining Loss: 0.036076\n",
      "Epoch: 9653 \tTraining Loss: 0.036104\n",
      "Epoch: 9654 \tTraining Loss: 0.036096\n",
      "Epoch: 9655 \tTraining Loss: 0.036196\n",
      "Epoch: 9656 \tTraining Loss: 0.036074\n",
      "Epoch: 9657 \tTraining Loss: 0.036094\n",
      "Epoch: 9658 \tTraining Loss: 0.036123\n",
      "Epoch: 9659 \tTraining Loss: 0.036123\n",
      "Epoch: 9660 \tTraining Loss: 0.036236\n",
      "Epoch: 9661 \tTraining Loss: 0.036088\n",
      "Epoch: 9662 \tTraining Loss: 0.036135\n",
      "Epoch: 9663 \tTraining Loss: 0.036143\n",
      "Epoch: 9664 \tTraining Loss: 0.036106\n",
      "Epoch: 9665 \tTraining Loss: 0.036075\n",
      "Epoch: 9666 \tTraining Loss: 0.036084\n",
      "Epoch: 9667 \tTraining Loss: 0.036110\n",
      "Epoch: 9668 \tTraining Loss: 0.036139\n",
      "Epoch: 9669 \tTraining Loss: 0.036101\n",
      "Epoch: 9670 \tTraining Loss: 0.036149\n",
      "Epoch: 9671 \tTraining Loss: 0.036095\n",
      "Epoch: 9672 \tTraining Loss: 0.036139\n",
      "Epoch: 9673 \tTraining Loss: 0.036127\n",
      "Epoch: 9674 \tTraining Loss: 0.036123\n",
      "Epoch: 9675 \tTraining Loss: 0.036156\n",
      "Epoch: 9676 \tTraining Loss: 0.036097\n",
      "Epoch: 9677 \tTraining Loss: 0.036101\n",
      "Epoch: 9678 \tTraining Loss: 0.036083\n",
      "Epoch: 9679 \tTraining Loss: 0.036125\n",
      "Epoch: 9680 \tTraining Loss: 0.036150\n",
      "Epoch: 9681 \tTraining Loss: 0.036112\n",
      "Epoch: 9682 \tTraining Loss: 0.036128\n",
      "Epoch: 9683 \tTraining Loss: 0.036067\n",
      "Epoch: 9684 \tTraining Loss: 0.036093\n",
      "Epoch: 9685 \tTraining Loss: 0.036139\n",
      "Epoch: 9686 \tTraining Loss: 0.036082\n",
      "Epoch: 9687 \tTraining Loss: 0.036120\n",
      "Epoch: 9688 \tTraining Loss: 0.036081\n",
      "Epoch: 9689 \tTraining Loss: 0.036151\n",
      "Epoch: 9690 \tTraining Loss: 0.036086\n",
      "Epoch: 9691 \tTraining Loss: 0.036091\n",
      "Epoch: 9692 \tTraining Loss: 0.036086\n",
      "Epoch: 9693 \tTraining Loss: 0.036098\n",
      "Epoch: 9694 \tTraining Loss: 0.036091\n",
      "Epoch: 9695 \tTraining Loss: 0.036086\n",
      "Epoch: 9696 \tTraining Loss: 0.036106\n",
      "Epoch: 9697 \tTraining Loss: 0.036137\n",
      "Epoch: 9698 \tTraining Loss: 0.036101\n",
      "Epoch: 9699 \tTraining Loss: 0.036090\n",
      "Epoch: 9700 \tTraining Loss: 0.036089\n",
      "Epoch: 9701 \tTraining Loss: 0.036095\n",
      "Epoch: 9702 \tTraining Loss: 0.036098\n",
      "Epoch: 9703 \tTraining Loss: 0.036085\n",
      "Epoch: 9704 \tTraining Loss: 0.036110\n",
      "Epoch: 9705 \tTraining Loss: 0.036096\n",
      "Epoch: 9706 \tTraining Loss: 0.036190\n",
      "Epoch: 9707 \tTraining Loss: 0.036178\n",
      "Epoch: 9708 \tTraining Loss: 0.036090\n",
      "Epoch: 9709 \tTraining Loss: 0.036097\n",
      "Epoch: 9710 \tTraining Loss: 0.036085\n",
      "Epoch: 9711 \tTraining Loss: 0.036103\n",
      "Epoch: 9712 \tTraining Loss: 0.036160\n",
      "Epoch: 9713 \tTraining Loss: 0.066688\n",
      "Epoch: 9714 \tTraining Loss: 0.206338\n",
      "Epoch: 9715 \tTraining Loss: 0.248175\n",
      "Epoch: 9716 \tTraining Loss: 0.207340\n",
      "Epoch: 9717 \tTraining Loss: 0.079463\n",
      "Epoch: 9718 \tTraining Loss: 0.089159\n",
      "Epoch: 9719 \tTraining Loss: 0.058599\n",
      "Epoch: 9720 \tTraining Loss: 0.056746\n",
      "Epoch: 9721 \tTraining Loss: 0.054500\n",
      "Epoch: 9722 \tTraining Loss: 0.050635\n",
      "Epoch: 9723 \tTraining Loss: 0.049728\n",
      "Epoch: 9724 \tTraining Loss: 0.049231\n",
      "Epoch: 9725 \tTraining Loss: 0.047643\n",
      "Epoch: 9726 \tTraining Loss: 0.047476\n",
      "Epoch: 9727 \tTraining Loss: 0.047325\n",
      "Epoch: 9728 \tTraining Loss: 0.047245\n",
      "Epoch: 9729 \tTraining Loss: 0.047227\n",
      "Epoch: 9730 \tTraining Loss: 0.047193\n",
      "Epoch: 9731 \tTraining Loss: 0.047144\n",
      "Epoch: 9732 \tTraining Loss: 0.047634\n",
      "Epoch: 9733 \tTraining Loss: 0.046091\n",
      "Epoch: 9734 \tTraining Loss: 0.046016\n",
      "Epoch: 9735 \tTraining Loss: 0.045704\n",
      "Epoch: 9736 \tTraining Loss: 0.045678\n",
      "Epoch: 9737 \tTraining Loss: 0.045650\n",
      "Epoch: 9738 \tTraining Loss: 0.045669\n",
      "Epoch: 9739 \tTraining Loss: 0.045642\n",
      "Epoch: 9740 \tTraining Loss: 0.045676\n",
      "Epoch: 9741 \tTraining Loss: 0.045677\n",
      "Epoch: 9742 \tTraining Loss: 0.045645\n",
      "Epoch: 9743 \tTraining Loss: 0.045663\n",
      "Epoch: 9744 \tTraining Loss: 0.045626\n",
      "Epoch: 9745 \tTraining Loss: 0.045629\n",
      "Epoch: 9746 \tTraining Loss: 0.045664\n",
      "Epoch: 9747 \tTraining Loss: 0.045650\n",
      "Epoch: 9748 \tTraining Loss: 0.045630\n",
      "Epoch: 9749 \tTraining Loss: 0.045632\n",
      "Epoch: 9750 \tTraining Loss: 0.045636\n",
      "Epoch: 9751 \tTraining Loss: 0.045617\n",
      "Epoch: 9752 \tTraining Loss: 0.045646\n",
      "Epoch: 9753 \tTraining Loss: 0.045583\n",
      "Epoch: 9754 \tTraining Loss: 0.045623\n",
      "Epoch: 9755 \tTraining Loss: 0.045611\n",
      "Epoch: 9756 \tTraining Loss: 0.045593\n",
      "Epoch: 9757 \tTraining Loss: 0.045600\n",
      "Epoch: 9758 \tTraining Loss: 0.045670\n",
      "Epoch: 9759 \tTraining Loss: 0.045636\n",
      "Epoch: 9760 \tTraining Loss: 0.045629\n",
      "Epoch: 9761 \tTraining Loss: 0.045610\n",
      "Epoch: 9762 \tTraining Loss: 0.045630\n",
      "Epoch: 9763 \tTraining Loss: 0.045621\n",
      "Epoch: 9764 \tTraining Loss: 0.045610\n",
      "Epoch: 9765 \tTraining Loss: 0.045608\n",
      "Epoch: 9766 \tTraining Loss: 0.045588\n",
      "Epoch: 9767 \tTraining Loss: 0.045713\n",
      "Epoch: 9768 \tTraining Loss: 0.045679\n",
      "Epoch: 9769 \tTraining Loss: 0.045605\n",
      "Epoch: 9770 \tTraining Loss: 0.045619\n",
      "Epoch: 9771 \tTraining Loss: 0.045588\n",
      "Epoch: 9772 \tTraining Loss: 0.045582\n",
      "Epoch: 9773 \tTraining Loss: 0.045584\n",
      "Epoch: 9774 \tTraining Loss: 0.045590\n",
      "Epoch: 9775 \tTraining Loss: 0.045580\n",
      "Epoch: 9776 \tTraining Loss: 0.045586\n",
      "Epoch: 9777 \tTraining Loss: 0.045570\n",
      "Epoch: 9778 \tTraining Loss: 0.045756\n",
      "Epoch: 9779 \tTraining Loss: 0.045603\n",
      "Epoch: 9780 \tTraining Loss: 0.045602\n",
      "Epoch: 9781 \tTraining Loss: 0.045614\n",
      "Epoch: 9782 \tTraining Loss: 0.045691\n",
      "Epoch: 9783 \tTraining Loss: 0.045577\n",
      "Epoch: 9784 \tTraining Loss: 0.045674\n",
      "Epoch: 9785 \tTraining Loss: 0.045535\n",
      "Epoch: 9786 \tTraining Loss: 0.045583\n",
      "Epoch: 9787 \tTraining Loss: 0.045611\n",
      "Epoch: 9788 \tTraining Loss: 0.045624\n",
      "Epoch: 9789 \tTraining Loss: 0.045628\n",
      "Epoch: 9790 \tTraining Loss: 0.045581\n",
      "Epoch: 9791 \tTraining Loss: 0.045596\n",
      "Epoch: 9792 \tTraining Loss: 0.045594\n",
      "Epoch: 9793 \tTraining Loss: 0.045565\n",
      "Epoch: 9794 \tTraining Loss: 0.045589\n",
      "Epoch: 9795 \tTraining Loss: 0.045602\n",
      "Epoch: 9796 \tTraining Loss: 0.045590\n",
      "Epoch: 9797 \tTraining Loss: 0.045597\n",
      "Epoch: 9798 \tTraining Loss: 0.045583\n",
      "Epoch: 9799 \tTraining Loss: 0.045602\n",
      "Epoch: 9800 \tTraining Loss: 0.045687\n",
      "Epoch: 9801 \tTraining Loss: 0.045583\n",
      "Epoch: 9802 \tTraining Loss: 0.045616\n",
      "Epoch: 9803 \tTraining Loss: 0.045611\n",
      "Epoch: 9804 \tTraining Loss: 0.045598\n",
      "Epoch: 9805 \tTraining Loss: 0.045559\n",
      "Epoch: 9806 \tTraining Loss: 0.045569\n",
      "Epoch: 9807 \tTraining Loss: 0.045601\n",
      "Epoch: 9808 \tTraining Loss: 0.046542\n",
      "Epoch: 9809 \tTraining Loss: 0.053909\n",
      "Epoch: 9810 \tTraining Loss: 0.049089\n",
      "Epoch: 9811 \tTraining Loss: 0.053601\n",
      "Epoch: 9812 \tTraining Loss: 0.047191\n",
      "Epoch: 9813 \tTraining Loss: 0.047402\n",
      "Epoch: 9814 \tTraining Loss: 0.047109\n",
      "Epoch: 9815 \tTraining Loss: 0.045804\n",
      "Epoch: 9816 \tTraining Loss: 0.046117\n",
      "Epoch: 9817 \tTraining Loss: 0.045688\n",
      "Epoch: 9818 \tTraining Loss: 0.044255\n",
      "Epoch: 9819 \tTraining Loss: 0.044194\n",
      "Epoch: 9820 \tTraining Loss: 0.044223\n",
      "Epoch: 9821 \tTraining Loss: 0.044164\n",
      "Epoch: 9822 \tTraining Loss: 0.044154\n",
      "Epoch: 9823 \tTraining Loss: 0.044133\n",
      "Epoch: 9824 \tTraining Loss: 0.044115\n",
      "Epoch: 9825 \tTraining Loss: 0.044233\n",
      "Epoch: 9826 \tTraining Loss: 0.044163\n",
      "Epoch: 9827 \tTraining Loss: 0.044129\n",
      "Epoch: 9828 \tTraining Loss: 0.044130\n",
      "Epoch: 9829 \tTraining Loss: 0.044166\n",
      "Epoch: 9830 \tTraining Loss: 0.044128\n",
      "Epoch: 9831 \tTraining Loss: 0.044125\n",
      "Epoch: 9832 \tTraining Loss: 0.044122\n",
      "Epoch: 9833 \tTraining Loss: 0.044114\n",
      "Epoch: 9834 \tTraining Loss: 0.044225\n",
      "Epoch: 9835 \tTraining Loss: 0.044153\n",
      "Epoch: 9836 \tTraining Loss: 0.044101\n",
      "Epoch: 9837 \tTraining Loss: 0.044104\n",
      "Epoch: 9838 \tTraining Loss: 0.044108\n",
      "Epoch: 9839 \tTraining Loss: 0.044105\n",
      "Epoch: 9840 \tTraining Loss: 0.044105\n",
      "Epoch: 9841 \tTraining Loss: 0.044181\n",
      "Epoch: 9842 \tTraining Loss: 0.044176\n",
      "Epoch: 9843 \tTraining Loss: 0.044119\n",
      "Epoch: 9844 \tTraining Loss: 0.044222\n",
      "Epoch: 9845 \tTraining Loss: 0.044122\n",
      "Epoch: 9846 \tTraining Loss: 0.044127\n",
      "Epoch: 9847 \tTraining Loss: 0.044133\n",
      "Epoch: 9848 \tTraining Loss: 0.044126\n",
      "Epoch: 9849 \tTraining Loss: 0.044113\n",
      "Epoch: 9850 \tTraining Loss: 0.044178\n",
      "Epoch: 9851 \tTraining Loss: 0.044106\n",
      "Epoch: 9852 \tTraining Loss: 0.044112\n",
      "Epoch: 9853 \tTraining Loss: 0.044159\n",
      "Epoch: 9854 \tTraining Loss: 0.044111\n",
      "Epoch: 9855 \tTraining Loss: 0.044137\n",
      "Epoch: 9856 \tTraining Loss: 0.044130\n",
      "Epoch: 9857 \tTraining Loss: 0.044094\n",
      "Epoch: 9858 \tTraining Loss: 0.044096\n",
      "Epoch: 9859 \tTraining Loss: 0.044093\n",
      "Epoch: 9860 \tTraining Loss: 0.044120\n",
      "Epoch: 9861 \tTraining Loss: 0.044147\n",
      "Epoch: 9862 \tTraining Loss: 0.044172\n",
      "Epoch: 9863 \tTraining Loss: 0.044063\n",
      "Epoch: 9864 \tTraining Loss: 0.044136\n",
      "Epoch: 9865 \tTraining Loss: 0.044113\n",
      "Epoch: 9866 \tTraining Loss: 0.044170\n",
      "Epoch: 9867 \tTraining Loss: 0.044112\n",
      "Epoch: 9868 \tTraining Loss: 0.044107\n",
      "Epoch: 9869 \tTraining Loss: 0.044185\n",
      "Epoch: 9870 \tTraining Loss: 0.044105\n",
      "Epoch: 9871 \tTraining Loss: 0.044135\n",
      "Epoch: 9872 \tTraining Loss: 0.044129\n",
      "Epoch: 9873 \tTraining Loss: 0.044188\n",
      "Epoch: 9874 \tTraining Loss: 0.044129\n",
      "Epoch: 9875 \tTraining Loss: 0.044163\n",
      "Epoch: 9876 \tTraining Loss: 0.044146\n",
      "Epoch: 9877 \tTraining Loss: 0.044116\n",
      "Epoch: 9878 \tTraining Loss: 0.044104\n",
      "Epoch: 9879 \tTraining Loss: 0.044103\n",
      "Epoch: 9880 \tTraining Loss: 0.044163\n",
      "Epoch: 9881 \tTraining Loss: 0.044304\n",
      "Epoch: 9882 \tTraining Loss: 0.044177\n",
      "Epoch: 9883 \tTraining Loss: 0.044109\n",
      "Epoch: 9884 \tTraining Loss: 0.044169\n",
      "Epoch: 9885 \tTraining Loss: 0.044094\n",
      "Epoch: 9886 \tTraining Loss: 0.044091\n",
      "Epoch: 9887 \tTraining Loss: 0.044113\n",
      "Epoch: 9888 \tTraining Loss: 0.044116\n",
      "Epoch: 9889 \tTraining Loss: 0.044127\n",
      "Epoch: 9890 \tTraining Loss: 0.044120\n",
      "Epoch: 9891 \tTraining Loss: 0.044145\n",
      "Epoch: 9892 \tTraining Loss: 0.044144\n",
      "Epoch: 9893 \tTraining Loss: 0.044191\n",
      "Epoch: 9894 \tTraining Loss: 0.044147\n",
      "Epoch: 9895 \tTraining Loss: 0.044082\n",
      "Epoch: 9896 \tTraining Loss: 0.044099\n",
      "Epoch: 9897 \tTraining Loss: 0.044177\n",
      "Epoch: 9898 \tTraining Loss: 0.044133\n",
      "Epoch: 9899 \tTraining Loss: 0.044177\n",
      "Epoch: 9900 \tTraining Loss: 0.044119\n",
      "Epoch: 9901 \tTraining Loss: 0.044114\n",
      "Epoch: 9902 \tTraining Loss: 0.044095\n",
      "Epoch: 9903 \tTraining Loss: 0.044093\n",
      "Epoch: 9904 \tTraining Loss: 0.044102\n",
      "Epoch: 9905 \tTraining Loss: 0.044210\n",
      "Epoch: 9906 \tTraining Loss: 0.044096\n",
      "Epoch: 9907 \tTraining Loss: 0.044143\n",
      "Epoch: 9908 \tTraining Loss: 0.044101\n",
      "Epoch: 9909 \tTraining Loss: 0.044102\n",
      "Epoch: 9910 \tTraining Loss: 0.044133\n",
      "Epoch: 9911 \tTraining Loss: 0.044107\n",
      "Epoch: 9912 \tTraining Loss: 0.044210\n",
      "Epoch: 9913 \tTraining Loss: 0.044096\n",
      "Epoch: 9914 \tTraining Loss: 0.044110\n",
      "Epoch: 9915 \tTraining Loss: 0.044116\n",
      "Epoch: 9916 \tTraining Loss: 0.044096\n",
      "Epoch: 9917 \tTraining Loss: 0.044122\n",
      "Epoch: 9918 \tTraining Loss: 0.044155\n",
      "Epoch: 9919 \tTraining Loss: 0.044097\n",
      "Epoch: 9920 \tTraining Loss: 0.044085\n",
      "Epoch: 9921 \tTraining Loss: 0.044105\n",
      "Epoch: 9922 \tTraining Loss: 0.044146\n",
      "Epoch: 9923 \tTraining Loss: 0.044116\n",
      "Epoch: 9924 \tTraining Loss: 0.044114\n",
      "Epoch: 9925 \tTraining Loss: 0.044121\n",
      "Epoch: 9926 \tTraining Loss: 0.044089\n",
      "Epoch: 9927 \tTraining Loss: 0.044116\n",
      "Epoch: 9928 \tTraining Loss: 0.044096\n",
      "Epoch: 9929 \tTraining Loss: 0.044120\n",
      "Epoch: 9930 \tTraining Loss: 0.044136\n",
      "Epoch: 9931 \tTraining Loss: 0.044128\n",
      "Epoch: 9932 \tTraining Loss: 0.044105\n",
      "Epoch: 9933 \tTraining Loss: 0.044125\n",
      "Epoch: 9934 \tTraining Loss: 0.047792\n",
      "Epoch: 9935 \tTraining Loss: 0.068822\n",
      "Epoch: 9936 \tTraining Loss: 0.064910\n",
      "Epoch: 9937 \tTraining Loss: 0.051455\n",
      "Epoch: 9938 \tTraining Loss: 0.057330\n",
      "Epoch: 9939 \tTraining Loss: 0.068831\n",
      "Epoch: 9940 \tTraining Loss: 0.112643\n",
      "Epoch: 9941 \tTraining Loss: 0.159855\n",
      "Epoch: 9942 \tTraining Loss: 0.122227\n",
      "Epoch: 9943 \tTraining Loss: 0.076891\n",
      "Epoch: 9944 \tTraining Loss: 0.046580\n",
      "Epoch: 9945 \tTraining Loss: 0.046300\n",
      "Epoch: 9946 \tTraining Loss: 0.042525\n",
      "Epoch: 9947 \tTraining Loss: 0.042341\n",
      "Epoch: 9948 \tTraining Loss: 0.042274\n",
      "Epoch: 9949 \tTraining Loss: 0.042270\n",
      "Epoch: 9950 \tTraining Loss: 0.042364\n",
      "Epoch: 9951 \tTraining Loss: 0.042227\n",
      "Epoch: 9952 \tTraining Loss: 0.042237\n",
      "Epoch: 9953 \tTraining Loss: 0.042211\n",
      "Epoch: 9954 \tTraining Loss: 0.042213\n",
      "Epoch: 9955 \tTraining Loss: 0.042213\n",
      "Epoch: 9956 \tTraining Loss: 0.042208\n",
      "Epoch: 9957 \tTraining Loss: 0.042239\n",
      "Epoch: 9958 \tTraining Loss: 0.042292\n",
      "Epoch: 9959 \tTraining Loss: 0.042223\n",
      "Epoch: 9960 \tTraining Loss: 0.042204\n",
      "Epoch: 9961 \tTraining Loss: 0.042188\n",
      "Epoch: 9962 \tTraining Loss: 0.042193\n",
      "Epoch: 9963 \tTraining Loss: 0.042216\n",
      "Epoch: 9964 \tTraining Loss: 0.042203\n",
      "Epoch: 9965 \tTraining Loss: 0.042264\n",
      "Epoch: 9966 \tTraining Loss: 0.042230\n",
      "Epoch: 9967 \tTraining Loss: 0.042190\n",
      "Epoch: 9968 \tTraining Loss: 0.042288\n",
      "Epoch: 9969 \tTraining Loss: 0.042221\n",
      "Epoch: 9970 \tTraining Loss: 0.042215\n",
      "Epoch: 9971 \tTraining Loss: 0.042283\n",
      "Epoch: 9972 \tTraining Loss: 0.042218\n",
      "Epoch: 9973 \tTraining Loss: 0.042165\n",
      "Epoch: 9974 \tTraining Loss: 0.042210\n",
      "Epoch: 9975 \tTraining Loss: 0.042193\n",
      "Epoch: 9976 \tTraining Loss: 0.042192\n",
      "Epoch: 9977 \tTraining Loss: 0.042214\n",
      "Epoch: 9978 \tTraining Loss: 0.042211\n",
      "Epoch: 9979 \tTraining Loss: 0.042243\n",
      "Epoch: 9980 \tTraining Loss: 0.042193\n",
      "Epoch: 9981 \tTraining Loss: 0.042179\n",
      "Epoch: 9982 \tTraining Loss: 0.042251\n",
      "Epoch: 9983 \tTraining Loss: 0.042156\n",
      "Epoch: 9984 \tTraining Loss: 0.042261\n",
      "Epoch: 9985 \tTraining Loss: 0.042346\n",
      "Epoch: 9986 \tTraining Loss: 0.042267\n",
      "Epoch: 9987 \tTraining Loss: 0.042217\n",
      "Epoch: 9988 \tTraining Loss: 0.042235\n",
      "Epoch: 9989 \tTraining Loss: 0.042188\n",
      "Epoch: 9990 \tTraining Loss: 0.042183\n",
      "Epoch: 9991 \tTraining Loss: 0.042230\n",
      "Epoch: 9992 \tTraining Loss: 0.042217\n",
      "Epoch: 9993 \tTraining Loss: 0.042195\n",
      "Epoch: 9994 \tTraining Loss: 0.042167\n",
      "Epoch: 9995 \tTraining Loss: 0.042234\n",
      "Epoch: 9996 \tTraining Loss: 0.042170\n",
      "Epoch: 9997 \tTraining Loss: 0.042206\n",
      "Epoch: 9998 \tTraining Loss: 0.042266\n",
      "Epoch: 9999 \tTraining Loss: 0.042179\n",
      "Epoch: 10000 \tTraining Loss: 0.042277\n"
     ]
    }
   ],
   "source": [
    "model = MyModel()\n",
    "train_model(model, train_loader, test_loader, device='cpu', epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "preds = make_predictions(model, X_test).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.8589743589743589 balanced accuracy:  0.8492401779095626\n"
     ]
    }
   ],
   "source": [
    "f1Score = f1_score(y_test, preds)\n",
    "bAccuracy = balanced_accuracy_score(y_test, preds)\n",
    "print('f1 score: ', f1Score, 'balanced accuracy: ', bAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
